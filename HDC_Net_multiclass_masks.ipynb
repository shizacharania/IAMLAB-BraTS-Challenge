{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OXG09_X81jA7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import scipy.ndimage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HDC_Block(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    # my assumption was that if you wanted a convolution with 3x3x1, you couldn't have it be 3d and specify the kernel size like that\n",
        "    # however, looking at the code implementation, you can do that\n",
        "    self.one_one_one1 = nn.Conv3d(channels, channels, kernel_size=1, stride=1)\n",
        "    self.three_three_one = nn.Conv3d(8, 8, kernel_size=(3,3,1), padding=(1,1,0))\n",
        "    self.one_three_three = nn.Conv3d(channels, channels, kernel_size=(1,3,3), padding=(0,1,1))\n",
        "  def forward(self, x):\n",
        "    x1 = self.one_one_one1(x)\n",
        "    print(x1.shape)\n",
        "\n",
        "    # [2, 32, 64, 64, 64]\n",
        "\n",
        "    print(\"channel groups\")\n",
        "    # split into 4\n",
        "    channel_group1 = x1[:, 0:8, :, :, :] # one modality\n",
        "    print(channel_group1.shape)\n",
        "\n",
        "    channel_group2 = x1[:, 8:16, :, :, :]\n",
        "    print(channel_group2.shape)\n",
        "\n",
        "    channel_group3 = x1[:, 16:24, :, :, :]\n",
        "    print(channel_group3.shape)\n",
        "\n",
        "    channel_group4 = x1[:, 24:32, :, :, :]\n",
        "    print(channel_group4.shape)\n",
        "\n",
        "    x2 = self.three_three_one(channel_group2)\n",
        "    print(x2.shape)\n",
        "    x3 = self.three_three_one(channel_group3+x2)\n",
        "    print(x3.shape)\n",
        "    x4 = self.three_three_one(channel_group4+x3)\n",
        "    print(x4.shape)\n",
        "\n",
        "    end = torch.cat([channel_group1, x2, x3, x4], dim=1)\n",
        "    print(end.shape)\n",
        "\n",
        "    x5 = self.one_one_one1(end)\n",
        "    print(x5.shape)\n",
        "\n",
        "    out = self.one_three_three(x5)\n",
        "    print(out.shape)\n",
        "    return out"
      ],
      "metadata": {
        "id": "5EfLQVgJ7r5n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(size=(2, 32, 64, 64, 64), dtype=torch.float32)\n",
        "print(x.shape)\n",
        "\n",
        "model = HDC_Block(32)\n",
        "print(model)\n",
        "print()\n",
        "\n",
        "out = model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCTjnTjRKHJN",
        "outputId": "ba8c5f7c-4df6-484c-d23f-39fe735aa679"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 32, 64, 64, 64])\n",
            "HDC_Block(\n",
            "  (one_one_one1): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "  (three_three_one): Conv3d(8, 8, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=(1, 1, 0))\n",
            "  (one_three_three): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
            ")\n",
            "\n",
            "torch.Size([2, 32, 64, 64, 64])\n",
            "channel groups\n",
            "torch.Size([2, 8, 64, 64, 64])\n",
            "torch.Size([2, 8, 64, 64, 64])\n",
            "torch.Size([2, 8, 64, 64, 64])\n",
            "torch.Size([2, 8, 64, 64, 64])\n",
            "torch.Size([2, 8, 64, 64, 64])\n",
            "torch.Size([2, 8, 64, 64, 64])\n",
            "torch.Size([2, 8, 64, 64, 64])\n",
            "torch.Size([2, 32, 64, 64, 64])\n",
            "torch.Size([2, 32, 64, 64, 64])\n",
            "torch.Size([2, 32, 64, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HDC_Net(nn.Module):\n",
        "  def __init__(self, x):\n",
        "    super().__init__()\n",
        "    # self.pds = torch.nn.functional.interpolate(x)\n",
        "    self.conv1 = nn.Conv3d(in_channels=4, out_channels=32, kernel_size=3, padding=1, stride=1)\n",
        "    self.downsample = nn.Conv3d(32, 32, kernel_size=2, stride=2)\n",
        "    self.HDC = HDC_Block(32)\n",
        "    self.upsample = nn.ConvTranspose3d(32, 32, kernel_size=2, stride=2)\n",
        "    self.upinterpolate = nn.Upsample(scale_factor=2, mode='trilinear')\n",
        "    self.conv2 = nn.Conv3d(in_channels=32, out_channels=3, kernel_size=1, stride=1)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "  def forward(self, x):\n",
        "    print(x.shape)\n",
        "    nimages, channels, width, height, depth = x.shape\n",
        "    print(nimages, channels, width, height, depth)\n",
        "    print(x.type)\n",
        "    # x1 = torch.tensor(scipy.ndimage.zoom(x, [1, 8.0, 0.5, 0.5, 0.5])) # using this function took about 2 minutes and for many images, it's not reasonable\n",
        "    x1 = torch.nn.functional.interpolate(x, scale_factor=[0.5, 0.5, 0.5]) # PDS - interpolate only looks at dim 2,3,4... (doesn't regard for channel and number of images)\n",
        "    # instead, I used torch.nn.functional.interpolate to interpolate the spatial dimensions, but for the channels I used a 1x1x1 conv\n",
        "    # because they did want to avoid using 3x3x3 conv and it will work same\n",
        "    print(x1.shape)\n",
        "    x1 = self.conv1(x1)\n",
        "    print(x1.shape)\n",
        "\n",
        "    x2 = self.HDC(x1)\n",
        "    print(x2.shape)\n",
        "    print()\n",
        "    x3 = self.downsample(x2)\n",
        "    print(x3.shape)\n",
        "\n",
        "    x4 = self.HDC(x3)\n",
        "    print(x4.shape)\n",
        "    print()\n",
        "    x5 = self.downsample(x4)\n",
        "    print(x5.shape)\n",
        "\n",
        "    x6 = self.HDC(x5)\n",
        "    print(x6.shape)\n",
        "    print()\n",
        "    x7 = self.downsample(x6)\n",
        "    print(x7.shape)\n",
        "\n",
        "    x8 = self.HDC(x7)\n",
        "    print(x8.shape)\n",
        "    print()\n",
        "\n",
        "    print(\"decoder time\")\n",
        "\n",
        "    x9 = self.upsample(x8)\n",
        "    print(x9.shape)\n",
        "    x10 = torch.add(x9, x6)\n",
        "    print(x10.shape)\n",
        "    x11 = self.HDC(x10)\n",
        "    print(x11.shape)\n",
        "\n",
        "    x12 = self.upsample(x11)\n",
        "    print(x12.shape)\n",
        "    x13 = torch.add(x12, x4)\n",
        "    print(x13.shape)\n",
        "    x14 = self.HDC(x13)\n",
        "    print(x14.shape)\n",
        "\n",
        "    x15 = self.upsample(x14)\n",
        "    print(x15.shape)\n",
        "    x16 = torch.add(x15, x2)\n",
        "    print(x16.shape)\n",
        "    x17 = self.HDC(x16)\n",
        "    print(x17.shape)\n",
        "\n",
        "    print(\"\\nupsampling\\n\") # by this they meant interpolation\n",
        "\n",
        "    x18 = self.upinterpolate(x17)\n",
        "    print(x18.shape)\n",
        "\n",
        "    x19 = self.conv2(x18)\n",
        "    print(x19.shape)\n",
        "\n",
        "    out = self.softmax(x19)\n",
        "    print(out.shape)\n",
        "\n",
        "    return x19, out"
      ],
      "metadata": {
        "id": "FnUEVd9i3bS8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(size=(2, 4, 128, 128, 128), dtype=torch.float32)\n",
        "print(x.shape)\n",
        "\n",
        "model = HDC_Net(x)\n",
        "print(model)\n",
        "print()\n",
        "\n",
        "# out = model(x)\n",
        "# print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bf1ybX73Wjd",
        "outputId": "b0332d55-a35d-4780-9174-42be031675ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 128, 128, 128])\n",
            "HDC_Net(\n",
            "  (conv1): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (downsample): Conv3d(32, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "  (HDC): HDC_Block(\n",
            "    (one_one_one1): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "    (three_three_one): Conv3d(8, 8, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=(1, 1, 0))\n",
            "    (one_three_three): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
            "  )\n",
            "  (upsample): ConvTranspose3d(32, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "  (upinterpolate): Upsample(scale_factor=2.0, mode=trilinear)\n",
            "  (conv2): Conv3d(32, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output, probability = out"
      ],
      "metadata": {
        "id": "Z4lqjhQl9QRF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(output.shape)\n",
        "# print(output)"
      ],
      "metadata": {
        "id": "dX3XHUnj9SiF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(probability.shape)\n",
        "# print(probability)"
      ],
      "metadata": {
        "id": "3NRhxzDM9Z8n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "KYmHvHW0gvIB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_images = []\n",
        "for i in range(2):\n",
        "  newx = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_training_images.append(newx)"
      ],
      "metadata": {
        "id": "lRm-uxBpgzku"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_training_images))\n",
        "print(randomized_training_images[0].shape)\n",
        "# print(randomized_training_images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOFYVukkg0wi",
        "outputId": "810b6771-8b78-4916-e891-7a4a3a56734c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 3, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_training_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "lLHd0_Jlg2D8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_training_segmentations))\n",
        "print(randomized_training_segmentations[0].shape)\n",
        "# print(randomized_training_segmentations[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-Lpq1Yeg3ta",
        "outputId": "8fc00bee-2ebe-4f7a-f805-6900cfeaac27"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 3, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_data = list(zip(randomized_training_images, randomized_training_segmentations))"
      ],
      "metadata": {
        "id": "nO6zxE6rg48U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(dataset=randomized_training_data, batch_size=2, shuffle=True) # batch size should be 10"
      ],
      "metadata": {
        "id": "tm7kwmsHg6F4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVib00lgg7UU",
        "outputId": "e6d84286-16a2-494c-f95f-fd70363b85ca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_images = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_validation_images.append(newy)"
      ],
      "metadata": {
        "id": "0OhQ4jPeg9NF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_validation_images))\n",
        "print(randomized_validation_images[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRQD263Dg_F-",
        "outputId": "ea726176-d473-4fc2-e0ef-68d05a45280d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 3, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_validation_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "gRq6qPWAhBSd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_validation_segmentations))\n",
        "print(randomized_validation_segmentations[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsV3ZlvFhCrk",
        "outputId": "59e0cd40-c86a-4e17-bd35-cfdb82a94384"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 3, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_data = list(zip(randomized_validation_images, randomized_validation_segmentations))"
      ],
      "metadata": {
        "id": "2jXVLmIvhDxh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validationloader = torch.utils.data.DataLoader(dataset=randomized_validation_data, batch_size=2, shuffle=True) # batch size should be 10"
      ],
      "metadata": {
        "id": "-PjukyAkhFP3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(validationloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPI6wgumhGZA",
        "outputId": "801d90ca-a32d-4942-dd14-fc04fa6b0f50"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_images = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_testing_images.append(newy)"
      ],
      "metadata": {
        "id": "ggGc5XCNhHu4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_testing_images))\n",
        "print(randomized_testing_images[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VcPrDJhhJHS",
        "outputId": "5fe5ada0-37c8-45e9-d596-f694750fe4ac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 3, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_testing_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "YKFnOHG0hKsv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_testing_segmentations))\n",
        "print(randomized_testing_segmentations[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wkfD4POhMQU",
        "outputId": "1cca9a10-22c4-4797-9c9c-0941e4dcc17f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 3, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_data = list(zip(randomized_testing_images, randomized_testing_segmentations))"
      ],
      "metadata": {
        "id": "qnIpezVFhNt8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testingloader = torch.utils.data.DataLoader(dataset=randomized_testing_data, batch_size=2, shuffle=True) # batch size should be 10"
      ],
      "metadata": {
        "id": "zvIhDCEchQii"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training + Validation:\n",
        "multi-class soft Dice function as the loss function\n",
        "\n",
        "Testing:\n",
        "mean accuracy\n",
        "dice coefficient\n",
        "hausdorff implementation\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "njyI0anWCXzA",
        "outputId": "99632534-ca0e-4911-c71f-d97c4ebb7079"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTraining + Validation:\\nmulti-class soft Dice function as the loss function\\n\\nTesting:\\nmean accuracy\\ndice coefficient\\nhausdorff implementation\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chose to make this a class because when you call dice loss in criterion, you don't have anything to input, but when u run the prediction through inside the training, then you have params\n",
        "# also because most sources I saw used a class\n",
        "class DiceLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.smooth = 1\n",
        "  def forward(self, true, pred):\n",
        "    # flatten to easily do it pixel by pixel\n",
        "    true = true.view(-1)\n",
        "    pred = pred.view(-1)\n",
        "    numerator = 2*(true*pred).sum()\n",
        "    denominator = true.sum() + pred.sum()\n",
        "    dice_loss = 1 - (numerator + self.smooth) / (denominator + self.smooth)\n",
        "    return dice_loss"
      ],
      "metadata": {
        "id": "j1mhy3eKdCM9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim"
      ],
      "metadata": {
        "id": "kvtMJWXLgeJl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epochs\n",
        "epochs = 2 # should be 800\n",
        "# loss\n",
        "criterion = DiceLoss()\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=10**-3, weight_decay=10**-5)"
      ],
      "metadata": {
        "id": "2__9J37GgnAq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "ZLqeEDDUl1L1"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "for i in range(epochs): # for every epoch\n",
        "  start_epoch = datetime.datetime.now() # start the timer in datetime module\n",
        "  training_loss = 0 # training loss for this epoch\n",
        "  validation_loss = 0 # validation loss for this epoch\n",
        "  print(\"training time\")\n",
        "  for images, segs in trainloader: # getting one batch from trainloader\n",
        "    optimizer.zero_grad() # turn on gradients (used for forward and backprop)\n",
        "    print(len(images), len(segs))\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    # need to squeeze the 1 in first dimension\n",
        "    images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "    # segs = segs.long() - no\n",
        "    segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    outputs, softmax_outputs = model(images) # put images through model\n",
        "    print(outputs.shape)\n",
        "    print(softmax_outputs.shape)\n",
        "\n",
        "    # arg_outputs = outputs.argmax(dim=1)\n",
        "    # print(arg_outputs.shape)\n",
        "    # print(arg_outputs)\n",
        "    # print(segs.shape)\n",
        "    print()\n",
        "    loss = criterion(softmax_outputs.float(), segs)\n",
        "    print(loss) # loss with random tensors will be really high because none of the tensors are related to each other\n",
        "    \n",
        "    loss.backward() # back propagation\n",
        "    training_loss += loss.item() # add loss to training loss\n",
        "    print()\n",
        "  print(\"validation time\")\n",
        "\n",
        "  for images, segs in validationloader: # getting one batch from validationloader\n",
        "    optimizer.zero_grad() # keep gradients on (used for forward and backprop)\n",
        "    print(len(images), len(segs))\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    # need to squeeze the 1 in first dimension\n",
        "    images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "    # segs = segs.long() - no\n",
        "    segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    outputs, softmax_outputs = model(images) # put images through model\n",
        "    print(outputs.shape)\n",
        "    print(softmax_outputs.shape)\n",
        "    loss = criterion(softmax_outputs.float(), segs) # calculate loss\n",
        "    print(loss) # loss with random tensors will be really high because none of the tensors are related to each other\n",
        "    loss.backward() # backpropagation\n",
        "    validation_loss += loss.item() # add loss to validation loss\n",
        "  \n",
        "  # find avg losses for each image\n",
        "  training_losses.append(training_loss/len(trainloader))\n",
        "  validation_losses.append(validation_loss/len(validationloader))\n",
        "\n",
        "  # print results about epoch\n",
        "  print(\"Epoch: {}/{}... Training Loss: {}... Validation Loss: {}...\".format(i+1,epochs, training_losses[-1], validation_losses[-1]))\n",
        "  if validation_loss < min(validation_losses):\n",
        "    print(\"Validation loss has decreased...saving model\")\n",
        "    torch.save(model.state_dict(), \"fcn.pth\")\n",
        "\n",
        "  # end timer\n",
        "  end_epoch = datetime.datetime.now()\n",
        "  # calculate and print out time it takes for every epoch\n",
        "  time_epoch = end_epoch-start_epoch\n",
        "  print(\"Epoch time:\", str(time_epoch), \"\\n\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "F02PVic5gpXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_losses)\n",
        "print(validation_losses)"
      ],
      "metadata": {
        "id": "g0I2BsZrNHcj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75349ccb-d5b9-414b-bf2c-a269e7961b45"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5999975800514221, 0.5999975800514221]\n",
            "[0.5999869108200073, 0.5999869108200073]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "N0NootnrlxW7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize losses\n",
        "plt.plot(training_losses, label=\"Training loss\")\n",
        "plt.plot(validation_losses, label=\"Validation loss\")\n",
        "plt.legend(frameon=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "lTegyA5-lw3J",
        "outputId": "bfb0898a-186e-4e59-e021-5ec8f6155761"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f90886dcd10>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_score(outputs, segmentations): # they find individual\n",
        "  # outputs = torch.Size([2, 3, 128, 128, 128])\n",
        "  # segmentations = torch.Size([2, 3, 128, 128, 128])\n",
        "  # print(outputs.shape)\n",
        "  # print(segmentations.shape)\n",
        "  n_classes = segmentations.shape[1]\n",
        "  region_dice_scores = []\n",
        "  for i in range(n_classes):\n",
        "    outputs = outputs.view(-1)\n",
        "    segmentations = segmentations.view(-1)\n",
        "    numerator = 2*(outputs*segmentations).sum()\n",
        "    denominator = outputs.sum() + segmentations.sum()\n",
        "    dice = (numerator) / (denominator)\n",
        "    region_dice_scores.append(dice)\n",
        "  return region_dice_scores"
      ],
      "metadata": {
        "id": "c8DSrR2MXfHC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1 NECROTIC TUMOUR CORE (NCR — label 1) - index 0\n",
        "\n",
        "2 GD-ENHANCING TUMOUR (ET — label 2) - index 1\n",
        "\n",
        "3 PERITUMORAL EDEMATOUS/INVADED TISSUE (ED — label 3) - index 2\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "liYRO6MtW-H4",
        "outputId": "1a43e156-92bd-49b9-bbbd-0f0b25a409ca"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1 NECROTIC TUMOUR CORE (NCR — label 1) - index 0\\n\\n2 GD-ENHANCING TUMOUR (ET — label 2) - index 1\\n\\n3 PERITUMORAL EDEMATOUS/INVADED TISSUE (ED — label 3) - index 2\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "VQfPcWW2nygw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import directed_hausdorff"
      ],
      "metadata": {
        "id": "Xj5rZBc6-aKW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hausdorff(outputs, segmentations): # for region for every image\n",
        "  # print(outputs.shape)\n",
        "  # print(segmentations.shape)\n",
        "\n",
        "  # for each image in batch\n",
        "  h_imgs = [0,0,0]\n",
        "  len_imgs = len(outputs)\n",
        "\n",
        "  for idx in range(len_imgs): # for each image\n",
        "    out = outputs[idx]\n",
        "    seg = segmentations[idx]\n",
        "    n_classes = out.shape[0]\n",
        "    # print(n_classes)\n",
        "\n",
        "    region_hd_scores = []\n",
        "    for i in range(n_classes): # for each class\n",
        "      region_out = out[i]\n",
        "      region_seg = seg[i]\n",
        "      # print(region_out.shape)\n",
        "      # print(region_seg.shape)\n",
        "\n",
        "      # print()\n",
        "      total_hd_image = 0\n",
        "      for j in range(region_out.shape[0]): # for each slice\n",
        "        out_slice = region_out[j]\n",
        "        seg_slice = region_seg[j]\n",
        "        # print(out_slice.shape)\n",
        "        # print(seg_slice.shape)\n",
        "        a = directed_hausdorff(out_slice, seg_slice)[0]\n",
        "        b = directed_hausdorff(seg_slice, out_slice)[0]\n",
        "        hd_slice = max(a, b)\n",
        "        total_hd_image += hd_slice\n",
        "      avg_hd_image = total_hd_image/region_out.shape[0]\n",
        "      # print(avg_hd_image)\n",
        "      region_hd_scores.append(avg_hd_image)\n",
        "    # print(region_hd95_scores)\n",
        "    h_imgs[0] += region_hd_scores[0]\n",
        "    h_imgs[1] += region_hd_scores[1]\n",
        "    h_imgs[2] += region_hd_scores[2]\n",
        "  \n",
        "  for each in range(len(h_imgs)):\n",
        "    h_imgs[each] = h_imgs[each]/len_imgs\n",
        "  \n",
        "  print(h_imgs)\n",
        "\n",
        "  return h_imgs"
      ],
      "metadata": {
        "id": "EP9p1B_u7CA3"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, segs in testingloader:\n",
        "    with torch.no_grad(): # no gradients (there's no backprop, only evaluation)\n",
        "      print(len(images), len(segs))\n",
        "      print(images.shape)\n",
        "      print(segs.shape)\n",
        "      # exclude first dimension of 1\n",
        "      images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "      # segs = segs.long() - no\n",
        "      segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "      print(images.shape)\n",
        "      print(segs.shape)\n",
        "      outputs, softmax_outputs = model(images) # put images through model\n",
        "      print(outputs.shape)\n",
        "      print(softmax_outputs.shape)\n",
        "      print()\n",
        "\n",
        "      print(\"\\n\", \".......\"*5, \"\\n\")\n",
        "\n",
        "      region_dice_scores = dice_score(softmax_outputs, segs)\n",
        "\n",
        "      print(len(region_dice_scores))\n",
        "      print(region_dice_scores)\n",
        "      print()\n",
        "\n",
        "      print(\"1 NECROTIC TUMOUR CORE (NCR — label 1)\")\n",
        "      print(region_dice_scores[0].item())\n",
        "      print()\n",
        "\n",
        "      print(\"2 GD-ENHANCING TUMOUR (ET — label 2)\")\n",
        "      print(region_dice_scores[1].item())\n",
        "      print()\n",
        "\n",
        "      print(\"3 PERITUMORAL EDEMATOUS/INVADED TISSUE (ED — label 3)\")\n",
        "      print(region_dice_scores[2].item())\n",
        "\n",
        "      print(\"\\n\", \".......\"*5, \"\\n\")\n",
        "\n",
        "      hd_scores = hausdorff(softmax_outputs, segs)\n",
        "\n",
        "      print()\n",
        "      print(len(hd_scores))\n",
        "      print(hd_scores)\n",
        "      print()\n",
        "\n",
        "      print(\"1 NECROTIC TUMOUR CORE (NCR — label 1)\")\n",
        "      print(hd_scores[0])\n",
        "      print()\n",
        "\n",
        "      print(\"2 GD-ENHANCING TUMOUR (ET — label 2)\")\n",
        "      print(hd_scores[1])\n",
        "      print()\n",
        "\n",
        "      print(\"3 PERITUMORAL EDEMATOUS/INVADED TISSUE (ED — label 3)\")\n",
        "      print(hd_scores[2])"
      ],
      "metadata": {
        "id": "XI2kt2CTXcnm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}