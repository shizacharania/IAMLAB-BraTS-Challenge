{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P_MbWzdtmbBU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import conv\n",
        "class Layer1(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    # nn.Sequential doesn't work\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2) # concatenate this before 2nd ReLU in res block\n",
        "    # print(x3.shape)\n",
        "\n",
        "    # block 2\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) # concatenate this before 2nd ReLU in res block\n",
        "    # print(x8.shape)\n",
        "\n",
        "    # element wise add\n",
        "    x9 = torch.add(x3, x8)\n",
        "\n",
        "    # relu\n",
        "    x10 = self.relu(x9)\n",
        "    return x10"
      ],
      "metadata": {
        "id": "7kCncP7WoiQN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer2(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) # concat\n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "    return x10"
      ],
      "metadata": {
        "id": "uXDYAroiRJ8z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer3(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) # concat\n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14) # concat\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "    return x17"
      ],
      "metadata": {
        "id": "cjt8QRpBO80p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer4(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) \n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14)\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "\n",
        "    # block 2 x3\n",
        "    x18 = self.conv(x17)\n",
        "    x19 = self.instnorm(x18)\n",
        "    x20 = self.relu(x19)\n",
        "    x21 = self.conv(x20)\n",
        "    x22 = self.instnorm(x21)\n",
        "    x23 = torch.add(x17, x22)\n",
        "    x24 = self.relu(x23)\n",
        "    return x24"
      ],
      "metadata": {
        "id": "wyTA3AkiPopy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer5(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) \n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14)\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "\n",
        "    # block 2 x3\n",
        "    x18 = self.conv(x17)\n",
        "    x19 = self.instnorm(x18)\n",
        "    x20 = self.relu(x19)\n",
        "    x21 = self.conv(x20)\n",
        "    x22 = self.instnorm(x21)\n",
        "    x23 = torch.add(x17, x22)\n",
        "    x24 = self.relu(x23)\n",
        "\n",
        "    # block 2 x4\n",
        "    x25 = self.conv(x24)\n",
        "    x26 = self.instnorm(x25)\n",
        "    x27 = self.relu(x26)\n",
        "    x28 = self.conv(x27)\n",
        "    x29 = self.instnorm(x28)\n",
        "    x30 = torch.add(x24, x29)\n",
        "    x31 = self.relu(x30)\n",
        "    return x31"
      ],
      "metadata": {
        "id": "B0BUmRhkRshf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer6(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) \n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14)\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "\n",
        "    # block 2 x3\n",
        "    x18 = self.conv(x17)\n",
        "    x19 = self.instnorm(x18)\n",
        "    x20 = self.relu(x19)\n",
        "    x21 = self.conv(x20)\n",
        "    x22 = self.instnorm(x21)\n",
        "    x23 = torch.add(x17, x22)\n",
        "    x24 = self.relu(x23)\n",
        "\n",
        "    # block 2 x4\n",
        "    x25 = self.conv(x24)\n",
        "    x26 = self.instnorm(x25)\n",
        "    x27 = self.relu(x26)\n",
        "    x28 = self.conv(x27)\n",
        "    x29 = self.instnorm(x28)\n",
        "    x30 = torch.add(x24, x29)\n",
        "    x31 = self.relu(x30)\n",
        "\n",
        "    # block 2 x5\n",
        "    x32 = self.conv(x31)\n",
        "    x33 = self.instnorm(x32)\n",
        "    x34 = self.relu(x33)\n",
        "    x35 = self.conv(x34)\n",
        "    x36 = self.instnorm(x35)\n",
        "    x37 = torch.add(x31, x36)\n",
        "    x38 = self.relu(x37)\n",
        "    return x38"
      ],
      "metadata": {
        "id": "B8f_Cz09Sxdg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlockDecoder(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.transcov = nn.ConvTranspose3d(in_channels=input_channels, out_channels=input_channels, kernel_size=2, stride=2)\n",
        "    self.conv = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x, concat_tensor):\n",
        "    x1 = self.transcov(x)\n",
        "    x2 = torch.add(x1, concat_tensor)\n",
        "    x3 = self.conv(x2)\n",
        "    x4 = self.instnorm(x3)\n",
        "    x5 = self.relu(x4)\n",
        "    return x5"
      ],
      "metadata": {
        "id": "6njHhcTiT8kk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualUNet(nn.Module):\n",
        "  def __init__(self, input_shape):\n",
        "    super().__init__()\n",
        "    self.layer1 = Layer1(4, 24)\n",
        "    self.layer2 = Layer2(24, 48)\n",
        "    self.layer3 = Layer3(48, 96)\n",
        "    self.layer4 = Layer4(96, 192)\n",
        "    self.layer5 = Layer5(192, 320)\n",
        "    self.layer6 = Layer6(320, 320)\n",
        "    self.layer7 = ConvBlockDecoder(320, 192)\n",
        "    self.layer8 = ConvBlockDecoder(192, 96)\n",
        "    self.layer9 = ConvBlockDecoder(96, 48)\n",
        "    self.layer10 = ConvBlockDecoder(48, 24)\n",
        "    self.layer11 = ConvBlockDecoder(24, 24)\n",
        "    self.conv1x1x1 = nn.Conv3d(in_channels=24, out_channels=3, kernel_size=1, stride=1)\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    print(x.shape) # [1, 4, 128, 128, 128]\n",
        "    x1 = self.layer1(x)\n",
        "    print(x1.shape) # [1, 24, 128, 128, 128]\n",
        "\n",
        "    x2 = self.layer2(x1)\n",
        "    print(x2.shape) # [1, 48, 64, 64, 64]\n",
        "\n",
        "    x3 = self.layer3(x2)\n",
        "    print(x3.shape) # [1, 96, 32, 32, 32]\n",
        "\n",
        "    x4 = self.layer4(x3)\n",
        "    print(x4.shape) # [1, 192, 16, 16, 16])\n",
        "\n",
        "    x5 = self.layer5(x4)\n",
        "    print(x5.shape) # [1, 320, 8, 8, 8]\n",
        "\n",
        "    x6 = self.layer6(x5)\n",
        "    print(x6.shape) # [1, 320, 4, 4, 4]\n",
        "\n",
        "    x7 = self.layer7(x6, x5)\n",
        "    print(x7.shape) # [1, 320, 4, 4, 4]\n",
        "\n",
        "    x8 = self.layer8(x7, x4)\n",
        "    print(x8.shape) # [1, 192, 8, 8, 8]\n",
        "\n",
        "    x9 = self.layer9(x8, x3)\n",
        "    print(x9.shape) # [1, 96, 16, 16, 16]\n",
        "\n",
        "    x10 = self.layer10(x9, x2)\n",
        "    print(x10.shape) # [1, 48, 32, 32, 32]\n",
        "\n",
        "    x11 = self.layer11(x10, x1)\n",
        "    print(x11.shape) # [1, 24, 64, 64, 64]\n",
        "\n",
        "    x12 = self.conv1x1x1(x11)\n",
        "    print(x12.shape) # [1, 24, 128, 128, 128]\n",
        "\n",
        "    prob = self.softmax(x12) # [1, 3, 128, 128, 128]\n",
        "\n",
        "    return x12, prob"
      ],
      "metadata": {
        "id": "tJpWDC3oRgAU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(size=(2, 4, 128, 128, 128), dtype=torch.float32)\n",
        "# print(x.shape)\n",
        "\n",
        "model = ResidualUNet(x.shape)\n",
        "print(model)\n",
        "print()\n",
        "\n",
        "out = model(x)\n",
        "# print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7zsz3sFtFMZ",
        "outputId": "3f117fb9-8c6e-4bbc-f39c-3019711d94d0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResidualUNet(\n",
            "  (layer1): Layer1(\n",
            "    (conv1): Conv3d(4, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer2): Layer2(\n",
            "    (conv1): Conv3d(24, 48, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer3): Layer3(\n",
            "    (conv1): Conv3d(48, 96, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer4): Layer4(\n",
            "    (conv1): Conv3d(96, 192, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer5): Layer5(\n",
            "    (conv1): Conv3d(192, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer6): Layer6(\n",
            "    (conv1): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer7): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(320, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer8): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(192, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer9): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(96, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer10): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(48, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(48, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer11): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(24, 24, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (conv1x1x1): Conv3d(24, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output, probability = out"
      ],
      "metadata": {
        "id": "6uIe4jxJd_40"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spnR9HzFelYy",
        "outputId": "03ac07fc-1b67-4c4e-e56c-66c252758939"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 128, 128, 128])\n",
            "tensor([[[[[-4.0322e-01, -3.2692e-01, -3.5789e-01,  ..., -2.8682e-01,\n",
            "            -3.2582e-01, -3.2179e-01],\n",
            "           [-6.7351e-01, -3.9433e-01, -4.5287e-01,  ..., -4.9533e-01,\n",
            "             4.4077e-02,  2.8955e-01],\n",
            "           [-4.7208e-01,  3.0325e-01, -4.1836e-01,  ..., -2.4422e-01,\n",
            "            -6.8165e-01, -2.8528e-01],\n",
            "           ...,\n",
            "           [-1.9903e-01, -1.9600e-01, -3.2846e-01,  ...,  2.9032e-01,\n",
            "            -4.3482e-01, -8.6181e-02],\n",
            "           [-5.3923e-01, -3.8530e-01, -3.7607e-01,  ..., -5.1147e-01,\n",
            "            -6.6293e-01, -5.7707e-01],\n",
            "           [-5.0612e-01, -7.5126e-02, -1.8303e-01,  ..., -1.2571e-01,\n",
            "            -1.3236e-01,  2.6948e-02]],\n",
            "\n",
            "          [[-5.3824e-01,  2.3221e-02, -2.6910e-01,  ..., -2.5939e-01,\n",
            "            -5.1840e-01, -2.2604e-01],\n",
            "           [-9.4763e-01, -5.9054e-01, -2.8033e-01,  ..., -4.8589e-01,\n",
            "            -7.3988e-01, -4.5067e-01],\n",
            "           [-6.3328e-01, -5.6095e-01, -4.3606e-01,  ..., -1.5940e-01,\n",
            "             4.9099e-01, -1.2958e-01],\n",
            "           ...,\n",
            "           [-8.1074e-01,  3.6152e-02, -2.7692e-01,  ..., -3.1947e-01,\n",
            "             1.9167e-01, -3.3308e-01],\n",
            "           [-7.3427e-01, -4.2853e-01, -1.2179e-01,  ..., -3.2168e-01,\n",
            "            -5.7114e-01, -2.1346e-01],\n",
            "           [-4.9328e-01, -2.9420e-01, -7.4027e-01,  ..., -3.0569e-01,\n",
            "            -4.2687e-01, -3.6612e-01]],\n",
            "\n",
            "          [[-2.6477e-01,  7.4514e-03, -3.7177e-01,  ..., -6.1254e-01,\n",
            "            -8.2137e-01, -1.3771e-01],\n",
            "           [-6.0921e-01, -3.0709e-01, -5.5629e-01,  ..., -2.9245e-01,\n",
            "            -2.5394e-01,  4.4469e-02],\n",
            "           [-2.1586e-01, -1.4410e-01, -6.0484e-01,  ..., -4.4487e-01,\n",
            "            -2.4882e-01,  6.7925e-02],\n",
            "           ...,\n",
            "           [-6.5904e-01, -5.7895e-01, -4.1697e-01,  ..., -1.8889e-01,\n",
            "            -2.2118e-01,  8.6175e-02],\n",
            "           [-4.6604e-01, -7.1849e-01, -7.8525e-01,  ...,  7.9640e-02,\n",
            "            -7.1103e-01, -7.5750e-02],\n",
            "           [-6.2589e-01, -1.9186e-01, -3.8705e-01,  ...,  2.1955e-01,\n",
            "            -3.7491e-01,  1.3322e-04]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-4.5035e-01, -4.4323e-01, -8.6527e-01,  ...,  2.6174e-02,\n",
            "            -4.5504e-01, -2.9655e-01],\n",
            "           [-5.7194e-01, -1.7133e-01, -2.1186e-01,  ..., -1.6328e-01,\n",
            "            -8.4399e-01, -2.4062e-01],\n",
            "           [-6.8877e-01, -5.8648e-02, -1.1758e-01,  ..., -1.7875e-01,\n",
            "            -5.4400e-01, -2.6885e-01],\n",
            "           ...,\n",
            "           [-3.4468e-01, -3.8012e-01, -1.4763e-01,  ..., -7.5244e-01,\n",
            "            -6.8027e-01, -1.6555e-01],\n",
            "           [-6.3112e-01, -4.3024e-01, -5.1658e-01,  ..., -3.4105e-01,\n",
            "            -1.6032e-01, -3.6293e-01],\n",
            "           [-4.7205e-01, -1.6961e-01, -2.8608e-01,  ..., -3.9407e-01,\n",
            "            -1.0309e-01, -3.6796e-01]],\n",
            "\n",
            "          [[-5.2894e-01, -1.0355e-01, -5.1933e-01,  ..., -3.7083e-02,\n",
            "            -4.1712e-01, -4.9324e-01],\n",
            "           [-3.1083e-01, -3.0968e-02, -2.9062e-01,  ..., -2.9075e-01,\n",
            "            -5.3484e-01, -1.0356e-01],\n",
            "           [-1.1960e-01, -3.4555e-01, -2.7111e-01,  ...,  4.3325e-02,\n",
            "            -7.0289e-01, -5.3174e-01],\n",
            "           ...,\n",
            "           [-2.0698e-01, -6.4704e-02, -6.7387e-01,  ..., -5.7987e-01,\n",
            "            -9.1277e-02,  2.1352e-02],\n",
            "           [-2.4685e-01,  3.1029e-02,  3.4332e-02,  ..., -9.7885e-03,\n",
            "            -6.7022e-01, -1.5214e-01],\n",
            "           [-8.4182e-01, -3.8793e-01, -4.8825e-01,  ..., -2.1005e-01,\n",
            "            -3.0148e-01, -4.7719e-01]],\n",
            "\n",
            "          [[-7.2771e-02, -4.1213e-01, -6.7586e-02,  ..., -2.0835e-01,\n",
            "            -1.4473e-01, -2.4992e-01],\n",
            "           [-2.0661e-01, -4.3613e-02, -1.2666e-01,  ..., -5.0475e-02,\n",
            "            -4.8257e-01, -3.3979e-01],\n",
            "           [-6.1241e-01, -3.4638e-01, -1.1062e-01,  ...,  2.7494e-01,\n",
            "            -5.0479e-01,  1.1406e-01],\n",
            "           ...,\n",
            "           [-3.5542e-01, -4.0142e-01, -3.4783e-02,  ...,  1.7430e-04,\n",
            "             2.5929e-02, -9.0839e-02],\n",
            "           [-4.8685e-01, -3.5304e-01,  4.3662e-02,  ..., -2.4450e-01,\n",
            "            -3.6045e-01, -1.1578e-01],\n",
            "           [-2.0507e-01, -8.4531e-02,  3.1007e-01,  ..., -5.0146e-01,\n",
            "            -3.2677e-01, -1.0185e-01]]],\n",
            "\n",
            "\n",
            "         [[[ 4.4959e-01,  3.8857e-01,  3.4692e-01,  ...,  2.1731e-01,\n",
            "             5.1883e-01,  2.4268e-03],\n",
            "           [ 7.6190e-01,  9.5304e-01,  4.8087e-01,  ...,  7.9988e-01,\n",
            "             2.4304e-01,  2.6981e-01],\n",
            "           [ 6.7976e-01,  9.1972e-01,  1.0749e+00,  ...,  1.0382e+00,\n",
            "             1.1601e+00,  6.5014e-01],\n",
            "           ...,\n",
            "           [ 4.7574e-01,  2.6822e-01,  8.9365e-02,  ...,  5.7234e-01,\n",
            "             1.0956e+00,  5.5026e-01],\n",
            "           [ 3.4665e-01,  5.7559e-01,  3.2258e-01,  ...,  2.4518e-02,\n",
            "             7.4216e-01,  1.9842e-01],\n",
            "           [ 4.4700e-01,  5.9940e-01,  7.0096e-01,  ...,  2.7449e-01,\n",
            "             8.6237e-01,  2.7523e-02]],\n",
            "\n",
            "          [[ 6.8349e-01,  6.3089e-01,  1.5622e-01,  ...,  3.9122e-01,\n",
            "             1.7546e-01,  2.6644e-01],\n",
            "           [ 2.7508e-01,  9.6843e-01,  4.6946e-01,  ...,  6.6231e-01,\n",
            "             7.8057e-01,  5.9700e-01],\n",
            "           [ 7.1405e-02,  9.7700e-01,  1.0452e+00,  ...,  6.7133e-01,\n",
            "             2.0726e-01,  3.6991e-01],\n",
            "           ...,\n",
            "           [ 5.2555e-01,  3.4429e-01,  1.4318e-01,  ...,  1.9059e-01,\n",
            "             2.7942e-01,  2.5721e-01],\n",
            "           [ 4.1430e-01,  7.1931e-01,  4.2762e-01,  ...,  3.6751e-02,\n",
            "             8.4711e-01,  1.5334e-01],\n",
            "           [ 9.2973e-02,  8.2648e-01,  3.9226e-02,  ...,  3.4362e-01,\n",
            "             1.1049e+00, -1.3795e-01]],\n",
            "\n",
            "          [[ 4.5667e-01,  1.0694e+00,  7.7586e-01,  ..., -5.2460e-01,\n",
            "             2.8735e-01, -1.1205e-01],\n",
            "           [ 9.6399e-02,  3.8536e-01,  9.4417e-01,  ..., -1.3039e-01,\n",
            "             9.6871e-03,  4.6183e-01],\n",
            "           [ 5.4669e-01,  2.9112e-01,  3.0040e-01,  ..., -4.3815e-01,\n",
            "            -4.6100e-02,  1.4222e-01],\n",
            "           ...,\n",
            "           [-1.7148e-01, -1.0749e-01,  1.6382e-01,  ...,  3.9154e-01,\n",
            "             1.9941e-01,  9.7453e-02],\n",
            "           [ 3.9741e-01,  6.2626e-01,  1.6485e-01,  ..., -2.5680e-01,\n",
            "             7.0939e-01,  5.5027e-02],\n",
            "           [-6.5607e-02,  2.4993e-01, -7.3021e-01,  ...,  5.2678e-01,\n",
            "             1.1527e-01, -1.6342e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 5.0842e-01, -3.8829e-01, -2.7172e-01,  ...,  2.1891e-02,\n",
            "             1.7463e-01, -3.4442e-01],\n",
            "           [ 2.2420e-01, -2.7232e-02,  8.8088e-02,  ...,  1.9363e-01,\n",
            "             2.1333e-01,  5.1347e-01],\n",
            "           [ 6.2232e-01,  5.0939e-01, -6.8903e-01,  ...,  5.3589e-01,\n",
            "             9.1945e-02,  2.4743e-01],\n",
            "           ...,\n",
            "           [ 2.9664e-01, -3.4699e-02, -3.0724e-02,  ...,  7.4375e-01,\n",
            "             7.4827e-01,  7.8914e-01],\n",
            "           [ 1.9986e-01,  1.0289e-01,  1.1590e-01,  ...,  1.7407e-01,\n",
            "             1.8761e-01, -1.8756e-01],\n",
            "           [ 2.1202e-01,  2.9008e-01,  3.3509e-02,  ...,  5.8166e-01,\n",
            "             5.6032e-01,  4.1697e-01]],\n",
            "\n",
            "          [[ 2.9996e-01, -2.9327e-01,  1.0971e-01,  ..., -3.0943e-02,\n",
            "             3.7469e-01, -3.0829e-02],\n",
            "           [ 7.0352e-02,  2.8653e-01, -1.4730e-01,  ..., -1.0371e-01,\n",
            "             9.3013e-01,  4.4943e-01],\n",
            "           [ 1.9098e-01, -2.5035e-01, -3.5839e-02,  ..., -1.4274e-01,\n",
            "             1.6845e-01,  9.0698e-01],\n",
            "           ...,\n",
            "           [ 8.3692e-02,  7.8926e-03,  3.0530e-01,  ...,  4.8974e-01,\n",
            "             2.7282e-01, -1.6038e-01],\n",
            "           [-5.1158e-02, -2.8860e-01,  5.7944e-02,  ..., -3.9622e-01,\n",
            "             7.2537e-01, -2.0090e-02],\n",
            "           [ 5.9614e-02, -3.4454e-01,  2.2001e-01,  ..., -3.2488e-01,\n",
            "             6.3963e-01,  3.0774e-01]],\n",
            "\n",
            "          [[-3.1539e-01, -1.3635e-01,  2.1811e-01,  ...,  1.7652e-01,\n",
            "             1.8906e-01,  7.8650e-02],\n",
            "           [ 2.7157e-01, -1.4507e-01,  2.5815e-01,  ..., -4.1629e-01,\n",
            "            -2.3927e-02,  8.5501e-01],\n",
            "           [-4.1441e-01, -4.3298e-01, -5.8576e-01,  ..., -2.2607e-01,\n",
            "             4.5414e-01,  8.8807e-02],\n",
            "           ...,\n",
            "           [ 2.3563e-02, -2.1477e-01, -4.5904e-01,  ..., -5.2618e-01,\n",
            "             5.7161e-01,  2.8088e-01],\n",
            "           [-6.0327e-02, -4.6862e-01, -2.2124e-01,  ..., -4.6237e-01,\n",
            "             2.3491e-01,  6.0942e-02],\n",
            "           [-7.8059e-02,  1.0048e-01,  1.2609e-02,  ..., -3.4131e-01,\n",
            "             3.7913e-01,  2.7551e-01]]],\n",
            "\n",
            "\n",
            "         [[[-9.0323e-02, -6.1274e-03, -5.6984e-02,  ..., -2.0000e-01,\n",
            "             1.6144e-01, -2.1439e-01],\n",
            "           [ 6.1783e-02,  4.8471e-02,  1.6367e-01,  ...,  5.8821e-01,\n",
            "            -1.2668e-01,  2.5210e-01],\n",
            "           [ 1.5488e-01,  2.1258e-01,  1.1634e+00,  ...,  7.0420e-01,\n",
            "             6.1946e-01,  4.9196e-01],\n",
            "           ...,\n",
            "           [ 9.8992e-02, -3.5123e-03,  4.6684e-01,  ...,  5.4258e-01,\n",
            "            -3.4157e-01,  2.8365e-01],\n",
            "           [-1.1438e-01,  5.0945e-01,  5.4103e-02,  ...,  7.2555e-02,\n",
            "             1.6668e-01, -1.4932e-01],\n",
            "           [-1.1800e-01, -3.7558e-01,  2.4328e-01,  ...,  3.0591e-01,\n",
            "            -1.4636e-02, -3.2388e-01]],\n",
            "\n",
            "          [[ 5.2189e-01,  7.0787e-01,  6.6249e-01,  ...,  2.0360e-01,\n",
            "             1.9453e-01, -5.5551e-01],\n",
            "           [ 1.8425e-01,  5.7716e-01,  5.3098e-01,  ..., -1.0964e-01,\n",
            "             1.5021e-01,  1.8234e-01],\n",
            "           [-1.0568e-01,  4.0164e-01,  5.5149e-01,  ..., -3.3191e-01,\n",
            "            -6.4980e-01, -3.1294e-01],\n",
            "           ...,\n",
            "           [ 9.5819e-02, -7.0942e-01,  1.2074e-01,  ..., -2.5782e-03,\n",
            "            -3.2433e-01, -3.4610e-01],\n",
            "           [-3.1924e-01,  1.5088e-02,  5.1721e-01,  ..., -1.3090e-01,\n",
            "            -5.1469e-01, -2.0320e-01],\n",
            "           [ 4.3658e-01, -1.3077e-01,  4.6574e-01,  ..., -2.6818e-01,\n",
            "             3.4360e-01, -1.8781e-01]],\n",
            "\n",
            "          [[-1.4041e-01,  7.9892e-01,  9.6427e-02,  ..., -4.6412e-01,\n",
            "             2.6600e-01, -3.1970e-01],\n",
            "           [-2.1770e-01, -3.8118e-02,  1.0496e+00,  ...,  7.1692e-01,\n",
            "            -2.0578e-01, -3.7014e-01],\n",
            "           [-2.4828e-01, -5.6689e-01,  2.2300e-01,  ..., -1.3986e-01,\n",
            "            -7.8259e-01, -6.9004e-02],\n",
            "           ...,\n",
            "           [ 1.4533e-02,  5.6240e-02, -4.7159e-02,  ..., -5.2506e-01,\n",
            "            -4.5785e-01, -3.9198e-01],\n",
            "           [ 7.5777e-02,  6.3073e-01,  7.3161e-01,  ..., -3.6577e-01,\n",
            "             3.5889e-01, -6.8904e-01],\n",
            "           [ 3.0282e-01, -5.5590e-01, -6.8264e-01,  ...,  3.6707e-01,\n",
            "            -1.9056e-01, -4.3743e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-2.4681e-01,  6.1324e-02,  2.4178e-01,  ..., -3.2005e-01,\n",
            "            -1.5249e-01, -1.1408e-01],\n",
            "           [ 5.8231e-02,  1.5716e-01,  4.5763e-01,  ..., -2.0277e-01,\n",
            "             1.5513e-01, -2.1119e-01],\n",
            "           [-3.5576e-02,  1.4319e-01, -9.0065e-01,  ...,  5.4123e-01,\n",
            "            -8.8390e-02, -1.8129e-02],\n",
            "           ...,\n",
            "           [-7.6266e-02, -4.8491e-02, -6.4589e-01,  ...,  4.8287e-01,\n",
            "            -1.5859e-01, -2.8777e-01],\n",
            "           [-2.8440e-01,  2.9110e-03, -4.3583e-01,  ...,  4.2016e-01,\n",
            "            -6.0038e-01, -8.5819e-01],\n",
            "           [ 1.3003e-01, -1.7584e-01, -4.8060e-01,  ..., -8.4651e-02,\n",
            "             1.2613e-01,  2.0984e-01]],\n",
            "\n",
            "          [[ 1.5586e-02, -4.1932e-01,  9.3564e-01,  ..., -4.3384e-01,\n",
            "             1.2629e-01, -2.5989e-01],\n",
            "           [-4.1215e-01, -2.0723e-01, -1.8443e-01,  ...,  4.7396e-02,\n",
            "            -2.0851e-01, -2.1922e-01],\n",
            "           [-5.7755e-01, -4.6837e-01, -6.6416e-02,  ..., -6.5941e-01,\n",
            "            -9.2522e-02, -2.3257e-01],\n",
            "           ...,\n",
            "           [-9.0932e-01, -8.1744e-01, -4.0927e-01,  ...,  9.7250e-03,\n",
            "            -3.9971e-01, -4.4490e-01],\n",
            "           [-2.6782e-01, -5.8655e-01, -2.8200e-01,  ..., -1.7881e-01,\n",
            "             5.0270e-01, -5.3188e-01],\n",
            "           [-3.3778e-01, -8.0691e-01, -2.8203e-02,  ..., -4.4734e-01,\n",
            "            -2.6233e-01, -2.8419e-01]],\n",
            "\n",
            "          [[-5.0863e-01, -6.1671e-01, -2.0493e-01,  ..., -6.4355e-01,\n",
            "            -3.2841e-01, -5.4593e-01],\n",
            "           [-4.7908e-01, -7.4743e-01, -4.0345e-01,  ..., -6.0841e-01,\n",
            "            -3.5115e-01,  2.4701e-01],\n",
            "           [-1.0956e-01, -5.0779e-01, -4.6597e-01,  ..., -4.9575e-01,\n",
            "            -4.6522e-01, -7.5112e-01],\n",
            "           ...,\n",
            "           [-5.7241e-01, -5.7371e-01, -1.0718e+00,  ..., -4.8797e-01,\n",
            "            -4.2927e-01, -5.7885e-01],\n",
            "           [ 2.7043e-02, -5.9299e-01, -6.4559e-01,  ..., -5.8534e-01,\n",
            "            -4.9335e-01, -7.8453e-01],\n",
            "           [-3.7586e-01, -2.4503e-01, -3.3280e-01,  ..., -6.1243e-01,\n",
            "            -8.6267e-03, -3.0457e-01]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-6.3313e-01, -2.7872e-01, -4.2047e-01,  ..., -6.8679e-01,\n",
            "            -1.8673e-01, -3.5723e-01],\n",
            "           [-4.6210e-01, -3.3008e-01, -4.5517e-01,  ..., -6.0927e-01,\n",
            "            -4.8489e-01, -3.6118e-02],\n",
            "           [-5.3276e-01, -1.1316e-01, -4.0956e-01,  ..., -7.0136e-01,\n",
            "            -2.1175e-01, -3.0034e-01],\n",
            "           ...,\n",
            "           [-6.6953e-01, -1.5105e-01, -3.9672e-01,  ..., -2.0606e-01,\n",
            "            -2.2920e-01,  6.9854e-02],\n",
            "           [-1.7469e-01, -2.4454e-01, -3.5574e-01,  ..., -3.2140e-01,\n",
            "            -2.0842e-01, -3.0565e-01],\n",
            "           [-4.6498e-01, -1.0584e-01, -2.7263e-01,  ..., -2.9988e-01,\n",
            "            -4.0583e-01, -8.7412e-03]],\n",
            "\n",
            "          [[-2.1994e-01, -4.2149e-01, -2.0414e-01,  ..., -1.8195e-01,\n",
            "            -5.1644e-01, -3.2500e-01],\n",
            "           [-6.4675e-01, -7.2697e-01, -7.1619e-01,  ..., -4.4463e-01,\n",
            "            -5.1551e-01,  7.1072e-02],\n",
            "           [-1.0366e+00, -3.9161e-01,  2.8288e-01,  ..., -4.0341e-02,\n",
            "            -3.9962e-01, -2.2870e-01],\n",
            "           ...,\n",
            "           [-6.8249e-01, -4.7447e-01,  1.4492e-02,  ..., -6.1125e-01,\n",
            "            -2.3306e-01, -1.1274e-01],\n",
            "           [-7.2755e-01, -6.7323e-01, -3.3776e-01,  ..., -3.7263e-01,\n",
            "            -3.0166e-02, -5.9597e-02],\n",
            "           [-3.7133e-01, -3.9344e-01, -3.5444e-01,  ..., -2.4108e-01,\n",
            "            -5.7086e-02, -2.7635e-01]],\n",
            "\n",
            "          [[-4.0945e-01, -1.9296e-01, -4.9310e-01,  ..., -8.2565e-02,\n",
            "            -8.0629e-01, -6.1612e-01],\n",
            "           [-5.7506e-01, -2.2458e-01, -4.1548e-01,  ..., -2.0697e-01,\n",
            "             3.9074e-03, -4.8468e-02],\n",
            "           [-7.5230e-01, -4.8689e-01, -4.2618e-01,  ..., -4.7589e-01,\n",
            "            -4.7567e-01,  1.5623e-01],\n",
            "           ...,\n",
            "           [-3.7917e-02, -3.1911e-01,  1.7467e-01,  ..., -2.2679e-01,\n",
            "            -5.2048e-01, -1.8224e-01],\n",
            "           [-5.2514e-01, -7.4359e-01, -4.8485e-01,  ..., -3.9020e-01,\n",
            "            -1.6826e-01, -1.0948e-01],\n",
            "           [-3.2318e-01, -5.1854e-01, -4.4107e-01,  ..., -4.2487e-01,\n",
            "            -2.4375e-01, -1.8258e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-5.3651e-01, -9.5593e-02, -1.9253e-01,  ..., -8.8573e-02,\n",
            "            -5.2883e-01, -3.5395e-01],\n",
            "           [-4.6295e-01, -2.6293e-01, -1.5689e-03,  ...,  5.4725e-03,\n",
            "            -8.6246e-01,  3.2283e-01],\n",
            "           [-6.6581e-01, -5.9979e-01, -2.5542e-01,  ..., -5.2032e-01,\n",
            "            -3.1802e-01,  3.9566e-02],\n",
            "           ...,\n",
            "           [-3.3378e-01, -5.5665e-01, -6.5919e-02,  ..., -9.4093e-01,\n",
            "             2.1483e-01,  1.0968e-01],\n",
            "           [-1.7213e-01, -1.8760e-01, -9.2827e-01,  ..., -9.2836e-01,\n",
            "            -2.8163e-01, -2.0749e-02],\n",
            "           [-6.8879e-01, -2.9672e-01,  1.9548e-01,  ..., -1.7871e-01,\n",
            "             2.2539e-01, -3.4178e-01]],\n",
            "\n",
            "          [[-3.9247e-01, -3.7955e-02, -5.6312e-01,  ..., -3.4065e-01,\n",
            "            -2.8227e-01, -6.5850e-01],\n",
            "           [-3.3287e-01, -2.5663e-01, -4.3198e-01,  ..., -1.8485e-01,\n",
            "            -3.6148e-01, -3.8863e-01],\n",
            "           [-4.1460e-01, -2.7215e-01, -1.0817e-01,  ..., -6.4564e-01,\n",
            "            -3.7483e-01, -3.9121e-01],\n",
            "           ...,\n",
            "           [-3.2795e-01, -1.4901e-01, -3.1492e-01,  ..., -5.7482e-01,\n",
            "            -4.5562e-01, -1.5035e-01],\n",
            "           [-3.3636e-01, -1.5666e-01, -4.2473e-01,  ..., -4.8489e-01,\n",
            "            -3.1066e-01, -2.4336e-01],\n",
            "           [-6.6343e-01, -5.2478e-01, -7.2274e-01,  ..., -5.0588e-02,\n",
            "            -3.7521e-01, -1.5843e-01]],\n",
            "\n",
            "          [[-2.7845e-01, -3.8614e-01, -1.2328e-01,  ...,  2.5905e-02,\n",
            "            -4.4412e-01, -4.7941e-01],\n",
            "           [-3.9099e-01, -1.2156e-01, -2.9590e-01,  ...,  3.7151e-02,\n",
            "            -4.5335e-01, -3.2441e-01],\n",
            "           [-3.7669e-01, -1.8057e-01, -1.9594e-01,  ..., -3.5827e-01,\n",
            "            -5.1771e-01, -3.9144e-01],\n",
            "           ...,\n",
            "           [-7.7517e-01, -2.2604e-01,  4.2076e-02,  ..., -1.8982e-01,\n",
            "            -4.3160e-01,  1.6698e-01],\n",
            "           [-3.3021e-01, -3.6431e-01, -2.9350e-01,  ..., -1.9319e-01,\n",
            "            -2.9980e-01, -3.3123e-01],\n",
            "           [-4.2897e-01, -7.6345e-02, -6.9881e-02,  ..., -1.0151e-01,\n",
            "             8.2060e-02, -2.4986e-01]]],\n",
            "\n",
            "\n",
            "         [[[ 6.2169e-01,  8.4078e-01,  9.5548e-01,  ...,  8.1480e-01,\n",
            "             6.8361e-01, -1.0174e-02],\n",
            "           [ 6.3290e-01,  1.1123e+00,  5.6153e-01,  ...,  1.0259e+00,\n",
            "             1.2485e+00,  4.0586e-01],\n",
            "           [ 6.2002e-01,  1.1223e+00,  8.2507e-01,  ...,  1.3999e+00,\n",
            "             1.1734e+00,  6.2313e-01],\n",
            "           ...,\n",
            "           [ 2.4129e-01,  9.4680e-01,  9.6235e-01,  ...,  8.5196e-01,\n",
            "             7.8531e-01,  2.3900e-01],\n",
            "           [ 2.6051e-01,  4.9043e-01,  5.4439e-01,  ...,  5.5695e-01,\n",
            "             5.6646e-01,  1.6968e-01],\n",
            "           [ 5.5962e-01,  3.4703e-01,  6.7267e-01,  ...,  1.4945e-01,\n",
            "             2.4508e-01,  2.3906e-01]],\n",
            "\n",
            "          [[ 5.6405e-01,  4.6506e-01,  5.3229e-01,  ...,  4.3584e-01,\n",
            "             7.4631e-01,  2.6195e-01],\n",
            "           [ 2.9383e-01,  9.6885e-01,  1.5449e+00,  ...,  7.4314e-01,\n",
            "             1.2711e+00,  3.9102e-01],\n",
            "           [ 5.2688e-01,  8.4761e-01,  4.3601e-01,  ...,  6.2846e-01,\n",
            "             8.1061e-01, -7.8619e-02],\n",
            "           ...,\n",
            "           [ 4.4512e-01,  1.3341e+00,  5.3732e-01,  ...,  5.8964e-01,\n",
            "             5.5545e-01,  4.8973e-01],\n",
            "           [ 5.8212e-02,  9.1915e-01,  3.5145e-01,  ...,  1.2029e+00,\n",
            "             2.7961e-01,  3.8998e-01],\n",
            "           [ 4.4870e-01,  1.4609e-01,  6.8945e-02,  ...,  2.5372e-01,\n",
            "             5.8646e-02,  6.6093e-02]],\n",
            "\n",
            "          [[ 4.6692e-01,  2.3352e-01, -2.8116e-03,  ...,  2.1300e-01,\n",
            "             5.6157e-01, -2.3391e-01],\n",
            "           [-2.7267e-02,  2.3246e-01,  7.2828e-01,  ...,  3.3151e-01,\n",
            "             2.6291e-01,  4.1823e-02],\n",
            "           [ 6.3291e-01, -1.4028e-01,  3.5732e-01,  ...,  2.1457e-01,\n",
            "             7.0329e-01, -1.9749e-01],\n",
            "           ...,\n",
            "           [-3.3908e-01,  2.6828e-01,  1.8522e-01,  ..., -1.1716e-01,\n",
            "             2.8436e-01,  6.4016e-02],\n",
            "           [ 6.4782e-02,  1.5693e-01,  3.1144e-01,  ...,  3.4813e-01,\n",
            "            -3.6292e-02,  1.3733e-02],\n",
            "           [ 2.5592e-02, -9.3972e-02,  1.9492e-01,  ..., -1.5784e-01,\n",
            "             2.6155e-01, -3.6000e-02]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 3.8127e-01,  2.4572e-01, -4.7359e-02,  ...,  3.5470e-01,\n",
            "             2.8835e-01,  2.2772e-01],\n",
            "           [-2.4784e-01,  2.4274e-01, -6.5320e-02,  ..., -1.3297e-01,\n",
            "             8.0994e-01,  7.0860e-01],\n",
            "           [-1.8252e-01, -1.4019e-02,  1.0425e-01,  ..., -1.0644e-01,\n",
            "             3.9169e-01,  7.4108e-01],\n",
            "           ...,\n",
            "           [-2.1493e-01,  2.5775e-02,  4.4952e-01,  ..., -9.4403e-02,\n",
            "             3.6832e-01,  1.8310e-01],\n",
            "           [ 1.9055e-01,  1.6048e-01, -4.8499e-01,  ..., -2.3889e-01,\n",
            "             2.5677e-01,  5.5066e-02],\n",
            "           [-1.8577e-01,  4.6678e-01,  3.4447e-01,  ...,  1.5931e-01,\n",
            "             2.3455e-01,  1.1995e-01]],\n",
            "\n",
            "          [[ 2.0656e-01, -1.1152e-02, -1.8511e-01,  ..., -8.9774e-03,\n",
            "             3.4820e-02,  7.8670e-01],\n",
            "           [ 2.9883e-01,  3.4323e-01,  8.2934e-01,  ...,  2.9314e-01,\n",
            "             3.5557e-01,  2.3242e-01],\n",
            "           [ 2.1819e-01, -1.1092e-01, -1.6216e-01,  ...,  7.9100e-02,\n",
            "             7.7775e-01,  6.9997e-01],\n",
            "           ...,\n",
            "           [ 3.8436e-01,  1.1924e-01, -1.9207e-02,  ..., -1.1224e-01,\n",
            "             5.4001e-01,  4.0925e-01],\n",
            "           [-1.3819e-02,  9.2271e-03, -4.5452e-01,  ...,  3.6556e-01,\n",
            "             3.7806e-01,  3.6513e-03],\n",
            "           [ 1.1703e-01,  1.0951e-01, -1.9877e-01,  ..., -3.0033e-02,\n",
            "             5.1799e-01,  2.6767e-01]],\n",
            "\n",
            "          [[ 7.0809e-02,  1.8776e-01,  2.5237e-03,  ..., -3.2833e-01,\n",
            "             2.7566e-01,  1.5637e-01],\n",
            "           [ 9.7715e-02, -5.1542e-02,  3.0833e-01,  ..., -8.5316e-02,\n",
            "             3.4840e-01,  1.0670e+00],\n",
            "           [-1.1133e-01, -4.7713e-01, -2.4958e-01,  ..., -2.3607e-01,\n",
            "             6.3738e-02,  5.4182e-01],\n",
            "           ...,\n",
            "           [-1.2689e-01, -4.3643e-01, -3.2659e-01,  ..., -3.1603e-01,\n",
            "             1.7468e-01,  2.0251e-01],\n",
            "           [-3.7323e-02, -3.0429e-02, -3.6187e-01,  ..., -8.5828e-01,\n",
            "            -1.8788e-01,  2.8571e-02],\n",
            "           [-2.2066e-01, -1.8182e-01, -1.4431e-01,  ..., -3.4457e-01,\n",
            "             2.5098e-01,  7.6288e-02]]],\n",
            "\n",
            "\n",
            "         [[[ 4.9822e-02, -5.9745e-02,  9.6857e-02,  ...,  2.6799e-01,\n",
            "             4.1252e-01,  9.8330e-02],\n",
            "           [ 1.8224e-01,  4.5619e-01,  5.5383e-01,  ...,  9.7694e-01,\n",
            "             3.8212e-01,  1.5450e-01],\n",
            "           [-1.5640e-02,  7.2266e-01,  6.7494e-01,  ...,  1.0160e+00,\n",
            "             5.0880e-01,  5.9584e-01],\n",
            "           ...,\n",
            "           [ 1.6936e-01,  2.7666e-01,  4.4321e-01,  ...,  2.5842e-01,\n",
            "             1.3820e-01, -2.2211e-01],\n",
            "           [-3.6215e-01,  5.4646e-01,  9.4414e-02,  ...,  2.5028e-01,\n",
            "             3.5259e-02,  1.2233e-01],\n",
            "           [-1.4289e-02, -1.0129e-01,  5.4261e-01,  ...,  3.3584e-02,\n",
            "            -1.7103e-01, -1.0859e-01]],\n",
            "\n",
            "          [[ 1.9436e-01,  4.3514e-01,  2.1771e-01,  ...,  4.8238e-01,\n",
            "             4.9746e-02, -2.9585e-01],\n",
            "           [ 1.7938e-01,  1.2174e+00, -3.2970e-01,  ...,  2.3070e-01,\n",
            "            -1.8075e-01,  1.6992e-02],\n",
            "           [ 7.4157e-01,  4.7908e-01,  7.9192e-02,  ...,  4.9891e-01,\n",
            "            -4.5547e-01, -3.2443e-01],\n",
            "           ...,\n",
            "           [ 5.6537e-04,  4.2484e-01, -4.0398e-01,  ...,  2.5201e-01,\n",
            "             9.0249e-02,  2.0609e-01],\n",
            "           [-1.9475e-01,  5.9884e-01,  1.6741e-01,  ...,  8.4937e-01,\n",
            "            -1.7805e-02, -3.1632e-01],\n",
            "           [ 2.2619e-01, -1.7524e-01, -8.0108e-02,  ..., -2.1291e-01,\n",
            "            -4.5614e-01, -2.5536e-01]],\n",
            "\n",
            "          [[-3.3934e-01,  2.9470e-02,  3.2495e-01,  ...,  1.6673e-01,\n",
            "             1.4223e-02,  5.9788e-02],\n",
            "           [-2.0340e-03, -2.2405e-01, -5.0725e-01,  ..., -5.7193e-01,\n",
            "             1.2725e-01, -7.3645e-02],\n",
            "           [-1.9473e-01, -5.2870e-02,  7.1817e-01,  ...,  3.2801e-01,\n",
            "            -3.4254e-01, -2.8444e-01],\n",
            "           ...,\n",
            "           [-7.3958e-01, -3.4496e-02,  1.3252e-01,  ..., -2.0988e-01,\n",
            "            -1.8105e-03, -5.0343e-01],\n",
            "           [ 1.4760e-01,  2.8611e-01, -1.0098e-01,  ...,  2.5277e-01,\n",
            "            -3.3379e-01, -4.1079e-01],\n",
            "           [-7.2766e-02, -5.7572e-01,  2.2251e-02,  ..., -2.6223e-01,\n",
            "            -4.6463e-02, -3.3923e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-2.1051e-02,  1.1508e-02, -3.9015e-02,  ..., -6.5191e-02,\n",
            "             1.9707e-01,  9.1342e-02],\n",
            "           [-1.8893e-01,  2.9300e-01, -4.0104e-01,  ...,  1.1102e-01,\n",
            "             6.4774e-01,  4.2264e-01],\n",
            "           [-2.8317e-01,  3.8020e-02,  3.4831e-01,  ...,  9.2486e-01,\n",
            "             3.5479e-01,  3.1082e-01],\n",
            "           ...,\n",
            "           [ 1.5800e-01,  4.4136e-02,  2.1176e-01,  ...,  7.9861e-02,\n",
            "            -4.6980e-01,  8.5398e-02],\n",
            "           [-2.6426e-01,  3.0958e-01, -6.1696e-01,  ..., -1.9968e-01,\n",
            "            -6.6032e-01, -1.7494e-01],\n",
            "           [-2.5855e-01, -7.9577e-02, -8.9892e-02,  ..., -6.1652e-02,\n",
            "            -2.7835e-01, -4.0371e-01]],\n",
            "\n",
            "          [[-8.3388e-02, -2.5374e-01,  4.1489e-02,  ..., -1.9083e-01,\n",
            "            -4.2773e-01, -2.5246e-04],\n",
            "           [-1.8043e-01, -7.7779e-02,  5.4045e-01,  ..., -2.9921e-01,\n",
            "            -6.9410e-01, -4.7623e-01],\n",
            "           [-5.0495e-01, -1.6352e-01, -1.2252e-01,  ...,  3.1552e-01,\n",
            "            -7.4141e-02, -3.2042e-02],\n",
            "           ...,\n",
            "           [-5.2957e-01, -6.2778e-01, -1.1199e-01,  ..., -3.5229e-01,\n",
            "            -4.8532e-01,  9.5869e-02],\n",
            "           [-1.3667e-01, -5.6881e-01, -1.5626e-01,  ..., -5.4212e-01,\n",
            "            -1.5011e-01, -2.5806e-01],\n",
            "           [-1.0546e-01, -8.4047e-01, -1.1583e-01,  ...,  4.9868e-02,\n",
            "            -3.0993e-01, -2.2628e-01]],\n",
            "\n",
            "          [[-2.2795e-01,  1.2785e-02, -3.6936e-01,  ..., -6.1737e-01,\n",
            "             2.7965e-02, -3.9314e-01],\n",
            "           [-3.4881e-01, -3.5822e-01, -6.0579e-01,  ..., -6.9162e-01,\n",
            "            -1.9025e-01, -2.9714e-01],\n",
            "           [-4.1593e-01, -3.7990e-01, -1.0237e+00,  ..., -7.3189e-01,\n",
            "            -5.5999e-01, -4.1741e-01],\n",
            "           ...,\n",
            "           [-3.5915e-01, -7.3801e-01, -6.8167e-01,  ..., -7.8502e-01,\n",
            "            -3.3618e-01, -3.6558e-01],\n",
            "           [-1.3048e-01, -3.4603e-01, -4.2184e-01,  ..., -6.6111e-01,\n",
            "            -6.7043e-01, -8.5936e-01],\n",
            "           [-2.4848e-01, -4.7719e-01, -4.5918e-01,  ..., -6.2421e-01,\n",
            "            -2.5086e-01, -1.9145e-01]]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(probability.shape)\n",
        "print(probability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxBKEIIneFRa",
        "outputId": "e9c3fa34-a17c-40dd-bade-4de5f7e02eea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 128, 128, 128])\n",
            "tensor([[[[[0.2122, 0.2261, 0.2286,  ..., 0.2669, 0.2018, 0.2860],\n",
            "           [0.1372, 0.1561, 0.1853,  ..., 0.1315, 0.3265, 0.3397],\n",
            "           [0.1657, 0.2656, 0.0969,  ..., 0.1391, 0.0911, 0.1747],\n",
            "           ...,\n",
            "           [0.2320, 0.2629, 0.2112,  ..., 0.2768, 0.1489, 0.2306],\n",
            "           [0.2018, 0.1650, 0.2198,  ..., 0.2221, 0.1357, 0.2125],\n",
            "           [0.1973, 0.2700, 0.2019,  ..., 0.2480, 0.2071, 0.3697]],\n",
            "\n",
            "          [[0.1374, 0.2075, 0.1973,  ..., 0.2219, 0.1984, 0.2980],\n",
            "           [0.1334, 0.1115, 0.1863,  ..., 0.1783, 0.1248, 0.1744],\n",
            "           [0.2120, 0.1209, 0.1237,  ..., 0.2417, 0.4825, 0.2873],\n",
            "           ...,\n",
            "           [0.1373, 0.3527, 0.2493,  ..., 0.2476, 0.3719, 0.2637],\n",
            "           [0.1764, 0.1751, 0.2161,  ..., 0.2746, 0.1616, 0.2896],\n",
            "           [0.1876, 0.1907, 0.1534,  ..., 0.2530, 0.1284, 0.2897]],\n",
            "\n",
            "          [[0.2387, 0.1640, 0.1740,  ..., 0.3075, 0.1429, 0.3497],\n",
            "           [0.2220, 0.2322, 0.0955,  ..., 0.2033, 0.2984, 0.3146],\n",
            "           [0.2432, 0.3124, 0.1736,  ..., 0.2973, 0.3557, 0.3391],\n",
            "           ...,\n",
            "           [0.2179, 0.2227, 0.2361,  ..., 0.2856, 0.3019, 0.3800],\n",
            "           [0.1964, 0.1151, 0.1228,  ..., 0.4247, 0.1242, 0.3730],\n",
            "           [0.1893, 0.3077, 0.4076,  ..., 0.2842, 0.2608, 0.4008]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.2069, 0.2693, 0.1714,  ..., 0.3699, 0.2364, 0.3171],\n",
            "           [0.1963, 0.2822, 0.2324,  ..., 0.2950, 0.1516, 0.2406],\n",
            "           [0.1508, 0.2507, 0.4946,  ..., 0.1962, 0.2239, 0.2525],\n",
            "           ...,\n",
            "           [0.2377, 0.2628, 0.3661,  ..., 0.1123, 0.1458, 0.2231],\n",
            "           [0.2123, 0.2355, 0.2521,  ..., 0.2077, 0.3268, 0.3570],\n",
            "           [0.2080, 0.2795, 0.3125,  ..., 0.1994, 0.2381, 0.2010]],\n",
            "\n",
            "          [[0.1994, 0.3912, 0.1397,  ..., 0.3733, 0.2029, 0.2597],\n",
            "           [0.2969, 0.3113, 0.3062,  ..., 0.2772, 0.1490, 0.2755],\n",
            "           [0.3337, 0.3351, 0.2863,  ..., 0.4300, 0.1912, 0.1523],\n",
            "           ...,\n",
            "           [0.3530, 0.3927, 0.2014,  ..., 0.1749, 0.3151, 0.4063],\n",
            "           [0.3129, 0.4414, 0.3633,  ..., 0.3962, 0.1209, 0.3540],\n",
            "           [0.1954, 0.3701, 0.2167,  ..., 0.3731, 0.2173, 0.2270]],\n",
            "\n",
            "          [[0.4113, 0.3192, 0.3123,  ..., 0.3209, 0.3097, 0.3192],\n",
            "           [0.2963, 0.4170, 0.3098,  ..., 0.4413, 0.2686, 0.1639],\n",
            "           [0.2582, 0.3613, 0.4305,  ..., 0.4834, 0.2151, 0.4174],\n",
            "           ...,\n",
            "           [0.3062, 0.3282, 0.4978,  ..., 0.4536, 0.2976, 0.3264],\n",
            "           [0.2379, 0.3735, 0.4407,  ..., 0.3976, 0.2711, 0.3696],\n",
            "           [0.3357, 0.3273, 0.4408,  ..., 0.3259, 0.2273, 0.3053]]],\n",
            "\n",
            "\n",
            "         [[[0.4978, 0.4624, 0.4626,  ..., 0.4419, 0.4697, 0.3955],\n",
            "           [0.5765, 0.6007, 0.4714,  ..., 0.4801, 0.3983, 0.3331],\n",
            "           [0.5242, 0.4919, 0.4316,  ..., 0.5017, 0.5744, 0.4452],\n",
            "           ...,\n",
            "           [0.4555, 0.4183, 0.3208,  ..., 0.3670, 0.6877, 0.4357],\n",
            "           [0.4895, 0.4313, 0.4421,  ..., 0.3796, 0.5532, 0.4615],\n",
            "           [0.5118, 0.5301, 0.4888,  ..., 0.3701, 0.5600, 0.3699]],\n",
            "\n",
            "          [[0.4661, 0.3810, 0.3019,  ..., 0.4254, 0.3970, 0.4876],\n",
            "           [0.4530, 0.5301, 0.3943,  ..., 0.5620, 0.5711, 0.4972],\n",
            "           [0.4288, 0.5626, 0.5442,  ..., 0.5548, 0.3633, 0.4735],\n",
            "           ...,\n",
            "           [0.5226, 0.4800, 0.3795,  ..., 0.4124, 0.4061, 0.4759],\n",
            "           [0.5564, 0.5519, 0.3744,  ..., 0.3930, 0.6674, 0.4179],\n",
            "           [0.3371, 0.5848, 0.3344,  ..., 0.4843, 0.5941, 0.3640]],\n",
            "\n",
            "          [[0.4910, 0.4742, 0.5482,  ..., 0.3358, 0.4331, 0.3588],\n",
            "           [0.4496, 0.4640, 0.4284,  ..., 0.2390, 0.3884, 0.4776],\n",
            "           [0.5214, 0.4828, 0.4292,  ..., 0.2993, 0.4357, 0.3652],\n",
            "           ...,\n",
            "           [0.3548, 0.3569, 0.4221,  ..., 0.5103, 0.4598, 0.3844],\n",
            "           [0.4658, 0.4415, 0.3175,  ..., 0.3033, 0.5139, 0.4251],\n",
            "           [0.3315, 0.4786, 0.2892,  ..., 0.3864, 0.4257, 0.3404]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.5396, 0.2846, 0.3102,  ..., 0.3684, 0.4437, 0.3023],\n",
            "           [0.4351, 0.3259, 0.3137,  ..., 0.4215, 0.4365, 0.5115],\n",
            "           [0.5595, 0.4425, 0.2793,  ..., 0.4008, 0.4229, 0.4231],\n",
            "           ...,\n",
            "           [0.4514, 0.3712, 0.4115,  ..., 0.5014, 0.6085, 0.5795],\n",
            "           [0.4874, 0.4013, 0.4746,  ..., 0.3477, 0.4628, 0.4254],\n",
            "           [0.4122, 0.4427, 0.4302,  ..., 0.5290, 0.4623, 0.4407]],\n",
            "\n",
            "          [[0.4568, 0.3236, 0.2620,  ..., 0.3756, 0.4478, 0.4124],\n",
            "           [0.4347, 0.4277, 0.3534,  ..., 0.3342, 0.6446, 0.4790],\n",
            "           [0.4552, 0.3686, 0.3623,  ..., 0.3570, 0.4569, 0.6422],\n",
            "           ...,\n",
            "           [0.4721, 0.4223, 0.5362,  ..., 0.5097, 0.4535, 0.3388],\n",
            "           [0.3806, 0.3206, 0.3720,  ..., 0.2692, 0.4883, 0.4039],\n",
            "           [0.4812, 0.3865, 0.4400,  ..., 0.3326, 0.5568, 0.4977]],\n",
            "\n",
            "          [[0.3227, 0.4206, 0.4155,  ..., 0.4715, 0.4325, 0.4434],\n",
            "           [0.4780, 0.3767, 0.4552,  ..., 0.3061, 0.4250, 0.5414],\n",
            "           [0.3148, 0.3313, 0.2677,  ..., 0.2929, 0.5611, 0.4069],\n",
            "           ...,\n",
            "           [0.4473, 0.3955, 0.3257,  ..., 0.2680, 0.5136, 0.4733],\n",
            "           [0.3644, 0.3327, 0.3381,  ..., 0.3197, 0.4916, 0.4410],\n",
            "           [0.3812, 0.3939, 0.3274,  ..., 0.3825, 0.4604, 0.4453]]],\n",
            "\n",
            "\n",
            "         [[[0.2901, 0.3116, 0.3089,  ..., 0.2911, 0.3285, 0.3184],\n",
            "           [0.2863, 0.2431, 0.3433,  ..., 0.3885, 0.2752, 0.3272],\n",
            "           [0.3101, 0.2425, 0.4715,  ..., 0.3592, 0.3345, 0.3801],\n",
            "           ...,\n",
            "           [0.3125, 0.3188, 0.4679,  ..., 0.3562, 0.1634, 0.3337],\n",
            "           [0.3087, 0.4037, 0.3380,  ..., 0.3983, 0.3111, 0.3260],\n",
            "           [0.2909, 0.1999, 0.3093,  ..., 0.3819, 0.2330, 0.2603]],\n",
            "\n",
            "          [[0.3965, 0.4115, 0.5008,  ..., 0.3526, 0.4046, 0.2144],\n",
            "           [0.4136, 0.3584, 0.4194,  ..., 0.2597, 0.3041, 0.3284],\n",
            "           [0.3592, 0.3165, 0.3321,  ..., 0.2034, 0.1542, 0.2392],\n",
            "           ...,\n",
            "           [0.3400, 0.1673, 0.3711,  ..., 0.3400, 0.2220, 0.2603],\n",
            "           [0.2672, 0.2729, 0.4095,  ..., 0.3324, 0.1710, 0.2926],\n",
            "           [0.4753, 0.2245, 0.5123,  ..., 0.2627, 0.2775, 0.3463]],\n",
            "\n",
            "          [[0.2703, 0.3618, 0.2779,  ..., 0.3567, 0.4240, 0.2915],\n",
            "           [0.3284, 0.3038, 0.4760,  ..., 0.5577, 0.3131, 0.2078],\n",
            "           [0.2354, 0.2047, 0.3972,  ..., 0.4034, 0.2086, 0.2957],\n",
            "           ...,\n",
            "           [0.4273, 0.4204, 0.3418,  ..., 0.2041, 0.2383, 0.2356],\n",
            "           [0.3377, 0.4435, 0.5597,  ..., 0.2720, 0.3619, 0.2020],\n",
            "           [0.4792, 0.2138, 0.3033,  ..., 0.3294, 0.3135, 0.2588]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.2536, 0.4461, 0.5184,  ..., 0.2617, 0.3199, 0.3806],\n",
            "           [0.3686, 0.3919, 0.4539,  ..., 0.2835, 0.4118, 0.2478],\n",
            "           [0.2898, 0.3068, 0.2260,  ..., 0.4030, 0.3531, 0.3244],\n",
            "           ...,\n",
            "           [0.3109, 0.3661, 0.2224,  ..., 0.3863, 0.2457, 0.1974],\n",
            "           [0.3003, 0.3632, 0.2733,  ..., 0.4447, 0.2104, 0.2176],\n",
            "           [0.3798, 0.2778, 0.2573,  ..., 0.2717, 0.2995, 0.3583]],\n",
            "\n",
            "          [[0.3438, 0.2853, 0.5984,  ..., 0.2511, 0.3493, 0.3279],\n",
            "           [0.2683, 0.2610, 0.3405,  ..., 0.3887, 0.2064, 0.2454],\n",
            "           [0.2111, 0.2964, 0.3514,  ..., 0.2130, 0.3519, 0.2055],\n",
            "           ...,\n",
            "           [0.1749, 0.1850, 0.2624,  ..., 0.3154, 0.2315, 0.2549],\n",
            "           [0.3065, 0.2380, 0.2648,  ..., 0.3346, 0.3908, 0.2421],\n",
            "           [0.3234, 0.2434, 0.3433,  ..., 0.2943, 0.2259, 0.2753]],\n",
            "\n",
            "          [[0.2660, 0.2602, 0.2722,  ..., 0.2076, 0.2578, 0.2374],\n",
            "           [0.2257, 0.2063, 0.2349,  ..., 0.2526, 0.3064, 0.2947],\n",
            "           [0.4270, 0.3074, 0.3018,  ..., 0.2237, 0.2238, 0.1757],\n",
            "           ...,\n",
            "           [0.2465, 0.2763, 0.1765,  ..., 0.2784, 0.1888, 0.2003],\n",
            "           [0.3977, 0.2938, 0.2212,  ..., 0.2827, 0.2373, 0.1894],\n",
            "           [0.2830, 0.2788, 0.2318,  ..., 0.2916, 0.3124, 0.2493]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0.1542, 0.1884, 0.1507,  ..., 0.1237, 0.1920, 0.2505],\n",
            "           [0.1697, 0.1347, 0.1537,  ..., 0.0908, 0.1106, 0.2655],\n",
            "           [0.1711, 0.1482, 0.1352,  ..., 0.0678, 0.1418, 0.1676],\n",
            "           ...,\n",
            "           [0.1724, 0.1808, 0.1387,  ..., 0.1828, 0.1922, 0.3412],\n",
            "           [0.2964, 0.1890, 0.1989,  ..., 0.1931, 0.2249, 0.2414],\n",
            "           [0.1867, 0.2795, 0.1714,  ..., 0.2523, 0.2391, 0.3139]],\n",
            "\n",
            "          [[0.2126, 0.1730, 0.2168,  ..., 0.2084, 0.1588, 0.2612],\n",
            "           [0.1711, 0.0744, 0.0829,  ..., 0.1601, 0.1195, 0.3008],\n",
            "           [0.0855, 0.1462, 0.3354,  ..., 0.2143, 0.1887, 0.3257],\n",
            "           ...,\n",
            "           [0.1648, 0.1046, 0.2990,  ..., 0.1494, 0.2183, 0.2380],\n",
            "           [0.2042, 0.1054, 0.2151,  ..., 0.1084, 0.2962, 0.2993],\n",
            "           [0.1965, 0.2526, 0.2602,  ..., 0.2726, 0.3580, 0.2916]],\n",
            "\n",
            "          [[0.2235, 0.2645, 0.2041,  ..., 0.2757, 0.1389, 0.2257],\n",
            "           [0.2221, 0.2793, 0.1980,  ..., 0.2935, 0.2918, 0.3258],\n",
            "           [0.1483, 0.2527, 0.1580,  ..., 0.1912, 0.1854, 0.4263],\n",
            "           ...,\n",
            "           [0.4473, 0.2422, 0.3368,  ..., 0.3192, 0.2034, 0.3328],\n",
            "           [0.2099, 0.1597, 0.2134,  ..., 0.2002, 0.3346, 0.3483],\n",
            "           [0.2701, 0.2879, 0.2233,  ..., 0.2871, 0.2580, 0.3319]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.1931, 0.2841, 0.3010,  ..., 0.2792, 0.1876, 0.2299],\n",
            "           [0.2813, 0.2272, 0.3833,  ..., 0.3353, 0.0921, 0.2797],\n",
            "           [0.2446, 0.2133, 0.2346,  ..., 0.1480, 0.2003, 0.2310],\n",
            "           ...,\n",
            "           [0.2659, 0.2167, 0.2503,  ..., 0.1637, 0.3745, 0.3276],\n",
            "           [0.2986, 0.2463, 0.2549,  ..., 0.1974, 0.2943, 0.3406],\n",
            "           [0.2386, 0.2279, 0.3434,  ..., 0.2836, 0.3826, 0.2835]],\n",
            "\n",
            "          [[0.2391, 0.3530, 0.2331,  ..., 0.2813, 0.3089, 0.1394],\n",
            "           [0.2472, 0.2489, 0.1394,  ..., 0.2853, 0.2656, 0.2648],\n",
            "           [0.2634, 0.3040, 0.3409,  ..., 0.1761, 0.1813, 0.1848],\n",
            "           ...,\n",
            "           [0.2593, 0.3416, 0.2802,  ..., 0.2606, 0.2138, 0.2482],\n",
            "           [0.2776, 0.3518, 0.3050,  ..., 0.2334, 0.2401, 0.3062],\n",
            "           [0.2029, 0.2766, 0.2211,  ..., 0.3199, 0.2217, 0.2885]],\n",
            "\n",
            "          [[0.2882, 0.2344, 0.3429,  ..., 0.4490, 0.2147, 0.2513],\n",
            "           [0.2722, 0.3494, 0.2806,  ..., 0.4224, 0.2207, 0.1653],\n",
            "           [0.3062, 0.3902, 0.4193,  ..., 0.3548, 0.2669, 0.2214],\n",
            "           ...,\n",
            "           [0.2258, 0.4150, 0.4594,  ..., 0.4110, 0.2542, 0.3812],\n",
            "           [0.2808, 0.2928, 0.3554,  ..., 0.4672, 0.3560, 0.3308],\n",
            "           [0.2916, 0.3892, 0.3838,  ..., 0.4207, 0.3447, 0.2902]]],\n",
            "\n",
            "\n",
            "         [[[0.5407, 0.5771, 0.5965,  ..., 0.5551, 0.4584, 0.3544],\n",
            "           [0.5072, 0.5697, 0.4248,  ..., 0.4657, 0.6261, 0.4131],\n",
            "           [0.5419, 0.5099, 0.4648,  ..., 0.5545, 0.5666, 0.4219],\n",
            "           ...,\n",
            "           [0.4287, 0.5419, 0.5400,  ..., 0.5265, 0.5302, 0.4040],\n",
            "           [0.4579, 0.3941, 0.4892,  ..., 0.4648, 0.4881, 0.3883],\n",
            "           [0.5202, 0.4397, 0.4412,  ..., 0.3955, 0.4585, 0.4021]],\n",
            "\n",
            "          [[0.4657, 0.4197, 0.4527,  ..., 0.3866, 0.5614, 0.4698],\n",
            "           [0.4382, 0.4056, 0.7951,  ..., 0.5252, 0.7134, 0.4142],\n",
            "           [0.4083, 0.5047, 0.3909,  ..., 0.4183, 0.6329, 0.3784],\n",
            "           ...,\n",
            "           [0.5089, 0.6383, 0.5043,  ..., 0.4964, 0.4802, 0.4347],\n",
            "           [0.4480, 0.5183, 0.4285,  ..., 0.5238, 0.4038, 0.4692],\n",
            "           [0.4462, 0.4332, 0.3974,  ..., 0.4471, 0.4019, 0.4107]],\n",
            "\n",
            "          [[0.5368, 0.4051, 0.3333,  ..., 0.3705, 0.5455, 0.3307],\n",
            "           [0.3841, 0.4412, 0.6214,  ..., 0.5028, 0.3781, 0.3566],\n",
            "           [0.5926, 0.3573, 0.3459,  ..., 0.3815, 0.6028, 0.2993],\n",
            "           ...,\n",
            "           [0.3310, 0.4358, 0.3403,  ..., 0.3562, 0.4549, 0.4258],\n",
            "           [0.3787, 0.3930, 0.4733,  ..., 0.4189, 0.3818, 0.3940],\n",
            "           [0.3829, 0.4402, 0.4218,  ..., 0.3750, 0.4277, 0.3843]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.4835, 0.3997, 0.3480,  ..., 0.4350, 0.4247, 0.4113],\n",
            "           [0.3488, 0.3767, 0.3596,  ..., 0.2920, 0.4907, 0.4113],\n",
            "           [0.3967, 0.3831, 0.3362,  ..., 0.2239, 0.4072, 0.4659],\n",
            "           ...,\n",
            "           [0.2994, 0.3880, 0.4192,  ..., 0.3818, 0.4366, 0.3526],\n",
            "           [0.4291, 0.3488, 0.3971,  ..., 0.3934, 0.5042, 0.3674],\n",
            "           [0.3946, 0.4890, 0.3985,  ..., 0.3976, 0.3862, 0.4499]],\n",
            "\n",
            "          [[0.4352, 0.3626, 0.3402,  ..., 0.3919, 0.4241, 0.5914],\n",
            "           [0.4649, 0.4535, 0.4920,  ..., 0.4602, 0.5440, 0.4927],\n",
            "           [0.4960, 0.3572, 0.3230,  ..., 0.3635, 0.5739, 0.5504],\n",
            "           ...,\n",
            "           [0.5287, 0.4467, 0.3766,  ..., 0.4139, 0.5786, 0.4343],\n",
            "           [0.3833, 0.4153, 0.2961,  ..., 0.5462, 0.4780, 0.3920],\n",
            "           [0.4427, 0.5216, 0.3733,  ..., 0.3265, 0.5416, 0.4418]],\n",
            "\n",
            "          [[0.4087, 0.4162, 0.3889,  ..., 0.3151, 0.4410, 0.4747],\n",
            "           [0.4438, 0.3748, 0.5135,  ..., 0.3737, 0.4921, 0.6647],\n",
            "           [0.3993, 0.2901, 0.3974,  ..., 0.4010, 0.4773, 0.5629],\n",
            "           ...,\n",
            "           [0.4318, 0.3363, 0.3178,  ..., 0.3623, 0.4661, 0.3950],\n",
            "           [0.3763, 0.4089, 0.3319,  ..., 0.2402, 0.3982, 0.4741],\n",
            "           [0.3591, 0.3502, 0.3562,  ..., 0.3299, 0.4082, 0.4021]]],\n",
            "\n",
            "\n",
            "         [[[0.3052, 0.2345, 0.2528,  ..., 0.3213, 0.3496, 0.3951],\n",
            "           [0.3232, 0.2956, 0.4215,  ..., 0.4435, 0.2633, 0.3213],\n",
            "           [0.2870, 0.3419, 0.4000,  ..., 0.3777, 0.2915, 0.4105],\n",
            "           ...,\n",
            "           [0.3989, 0.2773, 0.3213,  ..., 0.2908, 0.2776, 0.2548],\n",
            "           [0.2457, 0.4169, 0.3119,  ..., 0.3421, 0.2870, 0.3703],\n",
            "           [0.2931, 0.2808, 0.3874,  ..., 0.3522, 0.3024, 0.2840]],\n",
            "\n",
            "          [[0.3217, 0.4073, 0.3305,  ..., 0.4050, 0.2798, 0.2690],\n",
            "           [0.3908, 0.5200, 0.1220,  ..., 0.3146, 0.1670, 0.2850],\n",
            "           [0.5061, 0.3491, 0.2736,  ..., 0.3674, 0.1784, 0.2959],\n",
            "           ...,\n",
            "           [0.3263, 0.2571, 0.1967,  ..., 0.3542, 0.3016, 0.3273],\n",
            "           [0.3479, 0.3762, 0.3564,  ..., 0.3678, 0.2999, 0.2315],\n",
            "           [0.3572, 0.3142, 0.3424,  ..., 0.2804, 0.2402, 0.2978]],\n",
            "\n",
            "          [[0.2397, 0.3304, 0.4626,  ..., 0.3538, 0.3156, 0.4436],\n",
            "           [0.3939, 0.2795, 0.1806,  ..., 0.2037, 0.3301, 0.3177],\n",
            "           [0.2590, 0.3900, 0.4962,  ..., 0.4273, 0.2118, 0.2744],\n",
            "           ...,\n",
            "           [0.2217, 0.3220, 0.3229,  ..., 0.3246, 0.3417, 0.2414],\n",
            "           [0.4114, 0.4472, 0.3133,  ..., 0.3808, 0.2836, 0.2577],\n",
            "           [0.3470, 0.2719, 0.3549,  ..., 0.3378, 0.3143, 0.2838]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.3234, 0.3162, 0.3510,  ..., 0.2858, 0.3877, 0.3588],\n",
            "           [0.3699, 0.3961, 0.2571,  ..., 0.3727, 0.4172, 0.3090],\n",
            "           [0.3587, 0.4036, 0.4291,  ..., 0.6280, 0.3925, 0.3030],\n",
            "           ...,\n",
            "           [0.4347, 0.3952, 0.3305,  ..., 0.4545, 0.1889, 0.3198],\n",
            "           [0.2723, 0.4049, 0.3480,  ..., 0.4092, 0.2015, 0.2919],\n",
            "           [0.3669, 0.2831, 0.2581,  ..., 0.3188, 0.2312, 0.2665]],\n",
            "\n",
            "          [[0.3257, 0.2845, 0.4267,  ..., 0.3268, 0.2670, 0.2692],\n",
            "           [0.2879, 0.2976, 0.3686,  ..., 0.2545, 0.1904, 0.2426],\n",
            "           [0.2406, 0.3389, 0.3361,  ..., 0.4604, 0.2448, 0.2647],\n",
            "           ...,\n",
            "           [0.2120, 0.2116, 0.3432,  ..., 0.3255, 0.2075, 0.3175],\n",
            "           [0.3390, 0.2330, 0.3989,  ..., 0.2204, 0.2819, 0.3018],\n",
            "           [0.3544, 0.2017, 0.4056,  ..., 0.3537, 0.2367, 0.2696]],\n",
            "\n",
            "          [[0.3031, 0.3494, 0.2681,  ..., 0.2360, 0.3443, 0.2740],\n",
            "           [0.2840, 0.2758, 0.2059,  ..., 0.2038, 0.2872, 0.1699],\n",
            "           [0.2945, 0.3197, 0.1833,  ..., 0.2442, 0.2558, 0.2157],\n",
            "           ...,\n",
            "           [0.3423, 0.2487, 0.2228,  ..., 0.2267, 0.2797, 0.2238],\n",
            "           [0.3429, 0.2982, 0.3126,  ..., 0.2926, 0.2458, 0.1951],\n",
            "           [0.3493, 0.2606, 0.2600,  ..., 0.2494, 0.2471, 0.3077]]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pouMPGESf3DW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_images = []\n",
        "for i in range(2):\n",
        "  newx = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_training_images.append(newx)"
      ],
      "metadata": {
        "id": "0ohSRRMqgJPg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_training_images))\n",
        "print(randomized_training_images[0].shape)\n",
        "# print(randomized_training_images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEM8Z-YVgK4L",
        "outputId": "f3487b5a-7a7d-4b81-884f-48ebe1d9c443"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 3, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_training_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "je2iTekggR-n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_training_segmentations))\n",
        "print(randomized_training_segmentations[0].shape)\n",
        "# print(randomized_training_segmentations[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQI7ldbUgTss",
        "outputId": "ccd37c54-1037-40f2-b569-2d9e49facbb7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 3, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_data = list(zip(randomized_training_images, randomized_training_segmentations))"
      ],
      "metadata": {
        "id": "0Id-C28UgVDx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(dataset=randomized_training_data, batch_size=2, shuffle=True) # batch size should be 2"
      ],
      "metadata": {
        "id": "0kXxYbR2gWRx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1mBIdOfgXms",
        "outputId": "610d339a-7b1b-4356-9072-e788ed0f60c7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_images = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_validation_images.append(newy)"
      ],
      "metadata": {
        "id": "jH0ms_qigaKE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_validation_images))\n",
        "print(randomized_validation_images[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDOlu_IigcUt",
        "outputId": "c3066d7a-52be-4617-f49c-e35b47d230a0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 3, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_validation_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "S9ruiz4Tgevx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_validation_segmentations))\n",
        "print(randomized_validation_segmentations[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOYt_EuigfYa",
        "outputId": "fc5b921e-b695-4069-bd65-d034f95cadbc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 3, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_data = list(zip(randomized_validation_images, randomized_validation_segmentations))"
      ],
      "metadata": {
        "id": "zFPZkDakgz_p"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validationloader = torch.utils.data.DataLoader(dataset=randomized_validation_data, batch_size=2, shuffle=True) # batch size should be 2"
      ],
      "metadata": {
        "id": "6Wb-FS1Lg1jM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(validationloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fchjms5Gg2au",
        "outputId": "89b4b216-43ae-4df1-dd81-94f7ecd15034"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_images = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_testing_images.append(newy)"
      ],
      "metadata": {
        "id": "527yVWmyggOd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_testing_images))\n",
        "print(randomized_testing_images[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDHqy02vhBWt",
        "outputId": "b4bffad6-9af4-49fd-f189-898320ec670d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 3, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_testing_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "y4Zu_IyFhC92"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_testing_segmentations))\n",
        "print(randomized_testing_segmentations[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1SbByMohEmA",
        "outputId": "796d74be-9949-4e6c-e266-70f848bd3ef9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 3, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_data = list(zip(randomized_testing_images, randomized_testing_segmentations))"
      ],
      "metadata": {
        "id": "O0KeJ9qyhSIB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testingloader = torch.utils.data.DataLoader(dataset=randomized_testing_data, batch_size=2, shuffle=True) # batch size should be 2"
      ],
      "metadata": {
        "id": "Y-6FP0OmhTWX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training\n",
        "The sum of cross-entropy and dice loss \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9Rja_lBDtooP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eaf69818-1f60-43d8-a962-89b078e7a99c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTraining\\nThe sum of cross-entropy and dice loss \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chose to make this a class because when you call dice loss in criterion, you don't have anything to input, but when u run the prediction through inside the training, then you have params\n",
        "# also because most sources I saw used a class\n",
        "class DiceLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self, true, pred):\n",
        "    # flatten to easily do it pixel by pixel\n",
        "    true = true.view(-1)\n",
        "    pred = pred.view(-1)\n",
        "    numerator = 2*(true*pred).sum()\n",
        "    denominator = true.sum() + pred.sum()\n",
        "    dice_loss = 1 - (numerator) / (denominator)\n",
        "    return dice_loss"
      ],
      "metadata": {
        "id": "b8VIZVcJhTYT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim"
      ],
      "metadata": {
        "id": "6JfmRbwXiE-l"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# btw im not doing patches\n",
        "# epochs\n",
        "epochs = 2 # should be 1000\n",
        "# loss\n",
        "criterion1 = DiceLoss()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01, momentum= 0.99)"
      ],
      "metadata": {
        "id": "kay03ku8iGA0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "1AxaWfi7nPmU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  start_epoch = datetime.datetime.now()\n",
        "  training_loss = 0\n",
        "  validation_loss = 0\n",
        "  print(\"training time\")\n",
        "  for images, segs in trainloader:\n",
        "    optimizer.zero_grad()\n",
        "    print(len(images), len(segs))\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "    # segs = segs.long() - no\n",
        "    segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    outputs, softmax_outputs = model(images)\n",
        "    print(outputs.shape)\n",
        "    print(softmax_outputs.shape)\n",
        "\n",
        "    # arg_outputs = outputs.argmax(dim=1)\n",
        "    # print(arg_outputs.shape)\n",
        "    # print(arg_outputs)\n",
        "    # print(segs.shape)\n",
        "    print()\n",
        "    diceloss = criterion1(softmax_outputs.float(), segs)\n",
        "    celoss = criterion2(outputs.float(), segs)\n",
        "    loss = diceloss + celoss\n",
        "    print(loss) # loss with random tensors will be really high because none of the tensors are related to each other\n",
        "    # loss can be > 1 - https://ai.stackexchange.com/questions/24685/can-the-sparse-categorical-cross-entropy-be-greater-than-one, https://stats.stackexchange.com/questions/392681/cross-entropy-loss-max-value\n",
        "    loss.backward()\n",
        "    training_loss += loss.item()\n",
        "    print()\n",
        "  print(\"validation time\")\n",
        "  for images, segs in validationloader:\n",
        "    optimizer.zero_grad()\n",
        "    print(len(images), len(segs))\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "    # segs = segs.long() - no\n",
        "    segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    outputs, softmax_outputs = model(images)\n",
        "    print(outputs.shape)\n",
        "    print(softmax_outputs.shape)\n",
        "    diceloss = criterion1(softmax_outputs.float(), segs)\n",
        "    celoss = criterion2(outputs.float(), segs)\n",
        "    loss = diceloss + celoss\n",
        "    print(loss) # loss with random tensors will be really high because none of the tensors are related to each other\n",
        "    # loss can be > 1 - https://ai.stackexchange.com/questions/24685/can-the-sparse-categorical-cross-entropy-be-greater-than-one, https://stats.stackexchange.com/questions/392681/cross-entropy-loss-max-value\n",
        "    loss.backward()\n",
        "    validation_loss += loss.item()\n",
        "  training_losses.append(training_loss/len(trainloader))\n",
        "  validation_losses.append(validation_loss/len(validationloader))\n",
        "  print(\"Epoch: {}/{}... Training Loss: {}... Validation Loss: {}...\".format(i+1,epochs, training_losses[-1], validation_losses[-1]))\n",
        "  if validation_loss < min(validation_losses):\n",
        "    print(\"Validation loss has decreased...saving model\")\n",
        "    torch.save(model.state_dict(), \"fcn.pth\")\n",
        "  end_epoch = datetime.datetime.now()\n",
        "  time_epoch = end_epoch-start_epoch\n",
        "  print(\"Epoch time:\", str(time_epoch), \"\\n\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqJhvR3HpfQC",
        "outputId": "2138231a-2041-4cfa-adc6-f7c59e220a96"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "\n",
            "tensor(2.3268, grad_fn=<AddBackward0>)\n",
            "\n",
            "validation time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "tensor(2.3262, grad_fn=<AddBackward0>)\n",
            "Epoch: 1/2... Training Loss: 2.3268051147460938... Validation Loss: 2.326204299926758...\n",
            "Epoch time: 0:01:48.231169 \n",
            "\n",
            "\n",
            "training time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "\n",
            "tensor(2.3268, grad_fn=<AddBackward0>)\n",
            "\n",
            "validation time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "tensor(2.3262, grad_fn=<AddBackward0>)\n",
            "Epoch: 2/2... Training Loss: 2.3268051147460938... Validation Loss: 2.326204299926758...\n",
            "Epoch time: 0:01:28.400245 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_losses)\n",
        "print(validation_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de7ESYI8rt6b",
        "outputId": "2748693f-e486-4fa3-e2d6-dd80aca2ae96"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.3268051147460938, 2.3268051147460938]\n",
            "[2.326204299926758, 2.326204299926758]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "d8DIl6uPnbo-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize losses\n",
        "plt.plot(training_losses, label=\"Training loss\")\n",
        "plt.plot(validation_losses, label=\"Validation loss\")\n",
        "plt.legend(frameon=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "jYa_U1vnncFt",
        "outputId": "4f9b9fb9-64da-473f-ff8d-bab2a99a3d31"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4cdd043c90>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_score(outputs, segmentations): # they find individual\n",
        "  # outputs = torch.Size([2, 3, 128, 128, 128])\n",
        "  # segmentations = torch.Size([2, 3, 128, 128, 128])\n",
        "  # print(output.shape)\n",
        "  # print(segmentations.shape)\n",
        "  n_classes = segmentations.shape[1]\n",
        "  region_scores = []\n",
        "  for i in range(n_classes):\n",
        "    outputs = outputs.view(-1)\n",
        "    segmentations = segmentations.view(-1)\n",
        "    numerator = 2*(outputs*segmentations).sum()\n",
        "    denominator = outputs.sum() + segmentations.sum()\n",
        "    dice = (numerator) / (denominator)\n",
        "    region_scores.append(dice)\n",
        "  return region_scores"
      ],
      "metadata": {
        "id": "W37y7di3oqmK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1 NECROTIC TUMOUR CORE (NCR — label 1) - index 0\n",
        "\n",
        "2 GD-ENHANCING TUMOUR (ET — label 2) - index 1\n",
        "\n",
        "3 PERITUMORAL EDEMATOUS/INVADED TISSUE (ED — label 3) - index 2\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "usrtV9LTRQ_U",
        "outputId": "cf17a6ab-3fdb-47e1-c503-8ae7233815b8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1 NECROTIC TUMOUR CORE (NCR — label 1) - index 0\\n\\n2 GD-ENHANCING TUMOUR (ET — label 2) - index 1\\n\\n3 PERITUMORAL EDEMATOUS/INVADED TISSUE (ED — label 3) - index 2\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, segs in testingloader:\n",
        "    with torch.no_grad():\n",
        "      print(len(images), len(segs))\n",
        "      print(images.shape)\n",
        "      print(segs.shape)\n",
        "      images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "      # segs = segs.long() - no\n",
        "      segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "      print(images.shape)\n",
        "      print(segs.shape)\n",
        "      outputs, softmax_outputs = model(images)\n",
        "      print(outputs.shape)\n",
        "      print(softmax_outputs.shape)\n",
        "      print()\n",
        "\n",
        "      print(\".......\"*5)\n",
        "\n",
        "      region_scores = dice_score(softmax_outputs, segs)\n",
        "\n",
        "      print(len(region_scores))\n",
        "      print(region_scores)\n",
        "\n",
        "      print(\"1 NECROTIC TUMOUR CORE (NCR — label 1)\")\n",
        "      print(region_scores[0].item())\n",
        "      print()\n",
        "\n",
        "      print(\"2 GD-ENHANCING TUMOUR (ET — label 2)\")\n",
        "      print(region_scores[1].item())\n",
        "      print()\n",
        "\n",
        "      print(\"3 PERITUMORAL EDEMATOUS/INVADED TISSUE (ED — label 3)\")\n",
        "      print(region_scores[2].item())"
      ],
      "metadata": {
        "id": "L_c-dbmIocIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d5bb10-a233-4b91-e5ac-dc460856a149"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "\n",
            "...................................\n",
            "3\n",
            "[tensor(0.4000), tensor(0.4000), tensor(0.4000)]\n",
            "1 NECROTIC TUMOUR CORE (NCR — label 1)\n",
            "0.39997899532318115\n",
            "\n",
            "2 GD-ENHANCING TUMOUR (ET — label 2)\n",
            "0.39997899532318115\n",
            "\n",
            "3 PERITUMORAL EDEMATOUS/INVADED TISSUE (ED — label 3)\n",
            "0.39997899532318115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    }
  ]
}
