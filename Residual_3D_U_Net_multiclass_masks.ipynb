{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P_MbWzdtmbBU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import conv\n",
        "class Layer1(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    # nn.Sequential doesn't work\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2) # concatenate this before 2nd ReLU in res block\n",
        "    # print(x3.shape)\n",
        "\n",
        "    # block 2\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) # concatenate this before 2nd ReLU in res block\n",
        "    # print(x8.shape)\n",
        "\n",
        "    # element wise add\n",
        "    x9 = torch.add(x3, x8)\n",
        "\n",
        "    # relu\n",
        "    x10 = self.relu(x9)\n",
        "    return x10"
      ],
      "metadata": {
        "id": "7kCncP7WoiQN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer2(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) # concat\n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "    return x10"
      ],
      "metadata": {
        "id": "uXDYAroiRJ8z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer3(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) # concat\n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14) # concat\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "    return x17"
      ],
      "metadata": {
        "id": "cjt8QRpBO80p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer4(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) \n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14)\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "\n",
        "    # block 2 x3\n",
        "    x18 = self.conv(x17)\n",
        "    x19 = self.instnorm(x18)\n",
        "    x20 = self.relu(x19)\n",
        "    x21 = self.conv(x20)\n",
        "    x22 = self.instnorm(x21)\n",
        "    x23 = torch.add(x17, x22)\n",
        "    x24 = self.relu(x23)\n",
        "    return x24"
      ],
      "metadata": {
        "id": "wyTA3AkiPopy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer5(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) \n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14)\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "\n",
        "    # block 2 x3\n",
        "    x18 = self.conv(x17)\n",
        "    x19 = self.instnorm(x18)\n",
        "    x20 = self.relu(x19)\n",
        "    x21 = self.conv(x20)\n",
        "    x22 = self.instnorm(x21)\n",
        "    x23 = torch.add(x17, x22)\n",
        "    x24 = self.relu(x23)\n",
        "\n",
        "    # block 2 x4\n",
        "    x25 = self.conv(x24)\n",
        "    x26 = self.instnorm(x25)\n",
        "    x27 = self.relu(x26)\n",
        "    x28 = self.conv(x27)\n",
        "    x29 = self.instnorm(x28)\n",
        "    x30 = torch.add(x24, x29)\n",
        "    x31 = self.relu(x30)\n",
        "    return x31"
      ],
      "metadata": {
        "id": "B0BUmRhkRshf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer6(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) \n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14)\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "\n",
        "    # block 2 x3\n",
        "    x18 = self.conv(x17)\n",
        "    x19 = self.instnorm(x18)\n",
        "    x20 = self.relu(x19)\n",
        "    x21 = self.conv(x20)\n",
        "    x22 = self.instnorm(x21)\n",
        "    x23 = torch.add(x17, x22)\n",
        "    x24 = self.relu(x23)\n",
        "\n",
        "    # block 2 x4\n",
        "    x25 = self.conv(x24)\n",
        "    x26 = self.instnorm(x25)\n",
        "    x27 = self.relu(x26)\n",
        "    x28 = self.conv(x27)\n",
        "    x29 = self.instnorm(x28)\n",
        "    x30 = torch.add(x24, x29)\n",
        "    x31 = self.relu(x30)\n",
        "\n",
        "    # block 2 x5\n",
        "    x32 = self.conv(x31)\n",
        "    x33 = self.instnorm(x32)\n",
        "    x34 = self.relu(x33)\n",
        "    x35 = self.conv(x34)\n",
        "    x36 = self.instnorm(x35)\n",
        "    x37 = torch.add(x31, x36)\n",
        "    x38 = self.relu(x37)\n",
        "    return x38"
      ],
      "metadata": {
        "id": "B8f_Cz09Sxdg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlockDecoder(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.transcov = nn.ConvTranspose3d(in_channels=input_channels, out_channels=input_channels, kernel_size=2, stride=2)\n",
        "    self.conv = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x, concat_tensor):\n",
        "    x1 = self.transcov(x)\n",
        "    x2 = torch.add(x1, concat_tensor)\n",
        "    x3 = self.conv(x2)\n",
        "    x4 = self.instnorm(x3)\n",
        "    x5 = self.relu(x4)\n",
        "    return x5"
      ],
      "metadata": {
        "id": "6njHhcTiT8kk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualUNet(nn.Module):\n",
        "  def __init__(self, input_shape):\n",
        "    super().__init__()\n",
        "    self.layer1 = Layer1(4, 24)\n",
        "    self.layer2 = Layer2(24, 48)\n",
        "    self.layer3 = Layer3(48, 96)\n",
        "    self.layer4 = Layer4(96, 192)\n",
        "    self.layer5 = Layer5(192, 320)\n",
        "    self.layer6 = Layer6(320, 320)\n",
        "    self.layer7 = ConvBlockDecoder(320, 192)\n",
        "    self.layer8 = ConvBlockDecoder(192, 96)\n",
        "    self.layer9 = ConvBlockDecoder(96, 48)\n",
        "    self.layer10 = ConvBlockDecoder(48, 24)\n",
        "    self.layer11 = ConvBlockDecoder(24, 24)\n",
        "    self.conv1x1x1 = nn.Conv3d(in_channels=24, out_channels=3, kernel_size=1, stride=1)\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    print(x.shape) # [1, 4, 128, 128, 128]\n",
        "    x1 = self.layer1(x)\n",
        "    print(x1.shape) # [1, 24, 128, 128, 128]\n",
        "\n",
        "    x2 = self.layer2(x1)\n",
        "    print(x2.shape) # [1, 48, 64, 64, 64]\n",
        "\n",
        "    x3 = self.layer3(x2)\n",
        "    print(x3.shape) # [1, 96, 32, 32, 32]\n",
        "\n",
        "    x4 = self.layer4(x3)\n",
        "    print(x4.shape) # [1, 192, 16, 16, 16])\n",
        "\n",
        "    x5 = self.layer5(x4)\n",
        "    print(x5.shape) # [1, 320, 8, 8, 8]\n",
        "\n",
        "    x6 = self.layer6(x5)\n",
        "    print(x6.shape) # [1, 320, 4, 4, 4]\n",
        "\n",
        "    x7 = self.layer7(x6, x5)\n",
        "    print(x7.shape) # [1, 320, 4, 4, 4]\n",
        "\n",
        "    x8 = self.layer8(x7, x4)\n",
        "    print(x8.shape) # [1, 192, 8, 8, 8]\n",
        "\n",
        "    x9 = self.layer9(x8, x3)\n",
        "    print(x9.shape) # [1, 96, 16, 16, 16]\n",
        "\n",
        "    x10 = self.layer10(x9, x2)\n",
        "    print(x10.shape) # [1, 48, 32, 32, 32]\n",
        "\n",
        "    x11 = self.layer11(x10, x1)\n",
        "    print(x11.shape) # [1, 24, 64, 64, 64]\n",
        "\n",
        "    x12 = self.conv1x1x1(x11)\n",
        "    print(x12.shape) # [1, 24, 128, 128, 128]\n",
        "\n",
        "    prob = self.softmax(x12) # [1, 3, 128, 128, 128]\n",
        "\n",
        "    return x12, prob"
      ],
      "metadata": {
        "id": "tJpWDC3oRgAU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(size=(2, 4, 128, 128, 128), dtype=torch.float32)\n",
        "# print(x.shape)\n",
        "\n",
        "model = ResidualUNet(x.shape)\n",
        "print(model)\n",
        "print()\n",
        "\n",
        "out = model(x)\n",
        "# print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7zsz3sFtFMZ",
        "outputId": "dd59ec4e-e10a-4437-f578-e9e1ff9786d0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResidualUNet(\n",
            "  (layer1): Layer1(\n",
            "    (conv1): Conv3d(4, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer2): Layer2(\n",
            "    (conv1): Conv3d(24, 48, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer3): Layer3(\n",
            "    (conv1): Conv3d(48, 96, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer4): Layer4(\n",
            "    (conv1): Conv3d(96, 192, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer5): Layer5(\n",
            "    (conv1): Conv3d(192, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer6): Layer6(\n",
            "    (conv1): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer7): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(320, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer8): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(192, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer9): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(96, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer10): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(48, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(48, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer11): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(24, 24, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (conv1x1x1): Conv3d(24, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output, probability = out"
      ],
      "metadata": {
        "id": "6uIe4jxJd_40"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spnR9HzFelYy",
        "outputId": "17ba55b1-1f29-48ff-d239-0e8b21afbe84"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 128, 128, 128])\n",
            "tensor([[[[[-1.7141e-01, -1.2407e-01, -1.5094e-01,  ..., -3.7535e-01,\n",
            "            -9.4687e-02,  8.0470e-02],\n",
            "           [-1.7536e-01, -1.7444e-01,  9.9358e-02,  ...,  3.0959e-01,\n",
            "             2.2003e-01,  5.2682e-02],\n",
            "           [-1.6213e-01, -3.8462e-01,  9.8936e-02,  ..., -2.6112e-01,\n",
            "             2.4133e-01,  2.8188e-01],\n",
            "           ...,\n",
            "           [-2.5798e-01, -5.7171e-02,  4.5084e-01,  ...,  8.8942e-02,\n",
            "             3.3481e-02,  2.3826e-01],\n",
            "           [-3.2367e-01, -4.0994e-02,  5.7070e-01,  ...,  7.0946e-02,\n",
            "            -1.0223e-02,  1.8731e-01],\n",
            "           [-4.6066e-01, -3.5090e-01, -3.1301e-01,  ..., -2.1985e-01,\n",
            "            -5.2573e-02, -4.2181e-02]],\n",
            "\n",
            "          [[-2.3808e-01,  2.2156e-01,  3.2266e-01,  ...,  3.1447e-01,\n",
            "             5.9398e-01,  3.7170e-01],\n",
            "           [ 5.7349e-02,  1.4551e-01, -4.9151e-02,  ...,  6.7386e-01,\n",
            "             8.4047e-01,  5.4284e-01],\n",
            "           [-2.0472e-01,  3.8149e-01,  6.4044e-01,  ...,  6.4307e-01,\n",
            "             3.3202e-01,  5.0066e-01],\n",
            "           ...,\n",
            "           [-9.6346e-02,  4.0761e-01,  5.6039e-01,  ...,  4.6830e-01,\n",
            "             7.2998e-01,  6.0945e-01],\n",
            "           [-6.6020e-01,  4.3444e-03,  2.3298e-01,  ...,  7.5503e-01,\n",
            "             6.7761e-01, -4.6003e-01],\n",
            "           [-2.4429e-01, -2.0895e-01, -5.7477e-01,  ..., -5.5930e-01,\n",
            "            -2.1109e-01, -1.6949e-01]],\n",
            "\n",
            "          [[ 3.5577e-01,  6.1771e-01,  6.5112e-01,  ...,  5.5981e-01,\n",
            "             7.2043e-01,  6.8686e-01],\n",
            "           [ 1.1278e-01,  3.5243e-02,  4.6493e-01,  ...,  2.4429e-01,\n",
            "             3.3831e-01,  3.0205e-01],\n",
            "           [ 2.9873e-01,  3.1757e-01,  6.1160e-01,  ...,  1.7479e-01,\n",
            "             3.4193e-01,  3.3349e-01],\n",
            "           ...,\n",
            "           [ 2.6127e-02,  5.8652e-01, -1.4658e-01,  ...,  4.7386e-01,\n",
            "             1.8077e-01, -1.9379e-01],\n",
            "           [ 3.0827e-03,  6.3349e-01,  1.9588e-01,  ...,  1.7801e-01,\n",
            "             8.2087e-01,  1.1010e-01],\n",
            "           [-4.1268e-01, -1.4733e-01,  4.9900e-01,  ..., -2.0321e-01,\n",
            "            -2.3189e-01, -2.4714e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 1.8820e-01,  4.7022e-01,  3.6823e-01,  ...,  8.8395e-01,\n",
            "             5.7675e-01,  4.7490e-01],\n",
            "           [ 1.9692e-02,  2.0028e-01,  4.0964e-01,  ...,  3.7548e-01,\n",
            "             4.1793e-01,  5.8819e-01],\n",
            "           [-1.9796e-01, -2.6999e-01, -1.4408e-01,  ..., -1.6054e-02,\n",
            "             6.9966e-02,  1.5576e-01],\n",
            "           ...,\n",
            "           [-2.7410e-01,  4.8372e-01,  6.5004e-01,  ...,  3.2881e-01,\n",
            "             2.4061e-01,  6.2768e-02],\n",
            "           [-1.0308e-02, -1.2358e-01,  7.9614e-02,  ..., -8.2234e-02,\n",
            "            -4.5826e-02, -3.0073e-01],\n",
            "           [-2.7540e-01,  2.1471e-01, -2.1469e-01,  ..., -2.8999e-01,\n",
            "            -1.5633e-01, -4.9458e-01]],\n",
            "\n",
            "          [[ 3.1749e-01,  5.0580e-01,  1.5005e-01,  ...,  3.3414e-01,\n",
            "             5.5164e-01,  6.8492e-01],\n",
            "           [ 5.2782e-02,  5.6057e-01,  1.7169e-01,  ..., -1.9977e-01,\n",
            "            -2.9068e-01,  3.7071e-01],\n",
            "           [ 2.0340e-01,  3.4043e-01, -1.0315e-01,  ...,  5.1450e-01,\n",
            "             4.7205e-01,  2.4880e-02],\n",
            "           ...,\n",
            "           [ 3.8745e-01,  9.8124e-02,  4.8381e-01,  ..., -1.9199e-01,\n",
            "             3.0926e-01,  1.3427e-01],\n",
            "           [ 1.2561e-02,  5.2724e-01,  4.8511e-01,  ...,  4.1075e-01,\n",
            "            -1.0863e-02,  3.7415e-01],\n",
            "           [-1.2754e-01, -2.5519e-01, -1.5809e-01,  ..., -4.4979e-01,\n",
            "            -4.5560e-01, -2.4728e-01]],\n",
            "\n",
            "          [[ 2.5861e-01,  2.1556e-01,  2.6800e-01,  ...,  3.3672e-01,\n",
            "             1.3871e-01,  2.0843e-01],\n",
            "           [ 1.4940e-02,  1.4385e-01,  2.4279e-01,  ...,  1.7272e-01,\n",
            "            -5.3176e-02, -1.4927e-01],\n",
            "           [ 6.3634e-02, -7.0096e-03, -2.2283e-02,  ..., -3.5136e-02,\n",
            "             5.5026e-01, -1.8184e-01],\n",
            "           ...,\n",
            "           [ 1.7420e-01,  4.4778e-01,  5.3974e-01,  ...,  4.3679e-01,\n",
            "             2.7037e-01, -2.1058e-02],\n",
            "           [ 1.3429e-01,  7.8063e-02, -2.4318e-01,  ...,  3.2869e-02,\n",
            "             2.1536e-01, -3.0988e-01],\n",
            "           [ 1.8777e-01, -1.8673e-01, -2.7609e-01,  ..., -3.7069e-01,\n",
            "            -5.2533e-01, -2.6716e-01]]],\n",
            "\n",
            "\n",
            "         [[[-8.6431e-01, -6.3986e-01, -8.6686e-01,  ..., -6.5419e-01,\n",
            "            -6.2755e-01, -5.4742e-01],\n",
            "           [-7.7788e-01, -7.6725e-01, -7.3386e-01,  ..., -3.8539e-01,\n",
            "            -4.9504e-01, -3.7461e-01],\n",
            "           [-8.7172e-01, -8.1168e-01, -5.3834e-01,  ..., -1.1370e+00,\n",
            "            -7.9941e-01, -3.6952e-01],\n",
            "           ...,\n",
            "           [-8.1992e-01, -5.3380e-01, -7.5150e-01,  ..., -1.1227e+00,\n",
            "            -1.2847e+00, -9.4493e-01],\n",
            "           [-9.8279e-01, -1.0245e+00, -7.0989e-01,  ..., -5.3567e-01,\n",
            "            -8.2608e-01, -5.2592e-01],\n",
            "           [-1.3186e+00, -1.1668e+00, -8.7620e-01,  ..., -1.5193e+00,\n",
            "            -7.8530e-01, -8.5089e-01]],\n",
            "\n",
            "          [[-9.2327e-01, -2.0934e-01, -2.1742e-01,  ..., -1.9935e-01,\n",
            "            -2.2287e-01, -3.1130e-01],\n",
            "           [-7.1384e-01, -4.7438e-01, -1.9234e-01,  ..., -1.4609e-01,\n",
            "            -5.1524e-01, -7.3752e-01],\n",
            "           [-1.1400e+00, -2.1072e-01, -5.1620e-01,  ..., -4.3771e-01,\n",
            "            -2.1873e-01, -5.9044e-01],\n",
            "           ...,\n",
            "           [-8.4686e-01, -4.4595e-01, -5.5599e-01,  ..., -5.6712e-01,\n",
            "            -1.1691e-01, -6.3362e-01],\n",
            "           [-1.1118e+00, -6.9741e-01, -2.8356e-01,  ..., -5.3089e-01,\n",
            "            -5.5462e-01, -8.7457e-01],\n",
            "           [-1.4814e+00, -1.4301e+00, -1.0397e+00,  ..., -1.4983e+00,\n",
            "            -1.0341e+00, -1.3666e+00]],\n",
            "\n",
            "          [[-9.9800e-01, -1.8293e-01, -1.0458e-01,  ..., -3.4066e-01,\n",
            "            -4.8900e-01, -2.7027e-01],\n",
            "           [-8.3231e-01, -4.3980e-01, -3.7836e-01,  ..., -3.6401e-01,\n",
            "            -3.9791e-01, -3.8824e-01],\n",
            "           [-1.0674e+00, -2.0261e-01, -9.3148e-01,  ..., -1.2809e-01,\n",
            "            -1.1312e+00, -8.4844e-01],\n",
            "           ...,\n",
            "           [-9.6603e-01, -1.0930e-01, -5.7851e-01,  ..., -3.4631e-01,\n",
            "            -2.2887e-01, -1.0944e+00],\n",
            "           [-1.3550e+00, -2.8716e-01, -2.4840e-01,  ..., -6.4757e-01,\n",
            "            -9.2795e-01, -5.6794e-01],\n",
            "           [-1.4953e+00, -1.0134e+00, -1.2380e+00,  ..., -9.4647e-01,\n",
            "            -1.3967e+00, -1.5698e+00]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-9.1158e-01, -2.1742e-01, -3.5014e-01,  ..., -1.8986e-01,\n",
            "            -3.9977e-01, -3.9814e-01],\n",
            "           [-6.4009e-01, -6.2538e-01, -1.7009e-02,  ..., -2.7461e-01,\n",
            "            -9.6608e-02, -9.7639e-01],\n",
            "           [-7.2485e-01, -9.7897e-01, -4.4338e-01,  ..., -8.4268e-01,\n",
            "            -1.5406e-01, -8.4210e-01],\n",
            "           ...,\n",
            "           [-8.0771e-01, -5.3328e-01, -2.7349e-01,  ..., -2.6635e-01,\n",
            "            -5.9089e-01, -7.3376e-01],\n",
            "           [-1.0810e+00, -7.3184e-01, -3.0415e-01,  ..., -1.0727e+00,\n",
            "            -2.1071e-01, -1.5038e+00],\n",
            "           [-7.7643e-01, -9.6447e-01, -1.1742e+00,  ..., -1.8220e+00,\n",
            "            -1.0728e+00, -1.3065e+00]],\n",
            "\n",
            "          [[-5.5134e-01, -3.2436e-02, -6.9613e-01,  ..., -7.5912e-01,\n",
            "            -4.8703e-01, -5.9995e-02],\n",
            "           [-1.2269e+00, -3.8535e-01, -6.6421e-01,  ..., -9.0574e-01,\n",
            "            -7.2731e-01, -8.6563e-01],\n",
            "           [-7.5338e-01, -2.5348e-01, -1.0036e+00,  ..., -8.8538e-01,\n",
            "            -6.7231e-01, -9.2380e-01],\n",
            "           ...,\n",
            "           [-7.5877e-01, -2.3326e-01,  1.2035e-01,  ..., -8.5961e-01,\n",
            "            -2.6534e-01, -7.2792e-01],\n",
            "           [-8.4930e-01, -3.8159e-01, -1.1244e+00,  ..., -4.3431e-01,\n",
            "            -8.0631e-01, -2.2903e-01],\n",
            "           [-1.0957e+00, -1.0056e+00, -1.1110e+00,  ..., -1.3436e+00,\n",
            "            -1.6059e+00, -1.4872e+00]],\n",
            "\n",
            "          [[-9.0841e-01, -7.1754e-01, -8.3959e-01,  ..., -7.6147e-01,\n",
            "            -9.7388e-01, -8.9272e-01],\n",
            "           [-1.0411e+00, -7.6785e-01, -1.9583e-01,  ..., -7.8965e-01,\n",
            "            -8.6622e-01, -1.1031e+00],\n",
            "           [-8.0430e-01, -1.2119e+00, -1.1048e+00,  ..., -6.8634e-01,\n",
            "            -7.3529e-01, -1.2026e+00],\n",
            "           ...,\n",
            "           [-7.5007e-01, -8.8559e-01, -8.5335e-01,  ..., -7.7609e-01,\n",
            "            -5.7898e-01, -1.3768e+00],\n",
            "           [-8.6583e-01, -1.2296e+00, -1.1301e+00,  ..., -3.9252e-01,\n",
            "            -9.3774e-01, -1.2704e+00],\n",
            "           [-8.8527e-01, -9.4039e-01, -8.2108e-01,  ..., -9.7541e-01,\n",
            "            -9.7836e-01, -1.2896e+00]]],\n",
            "\n",
            "\n",
            "         [[[-2.4574e-02,  8.9124e-02,  3.3692e-01,  ...,  9.5277e-03,\n",
            "             3.5067e-02, -1.8899e-01],\n",
            "           [ 3.4188e-01,  1.8913e-01,  4.4635e-01,  ...,  1.5080e-01,\n",
            "             8.0453e-02, -2.1120e-01],\n",
            "           [ 7.3735e-01,  3.5398e-01,  1.3969e-01,  ...,  2.0105e-01,\n",
            "             3.4763e-01, -2.4453e-01],\n",
            "           ...,\n",
            "           [ 7.2956e-01,  3.0929e-01,  2.5095e-01,  ...,  3.9315e-01,\n",
            "             7.1228e-01,  3.2097e-01],\n",
            "           [ 4.8617e-01,  4.6627e-01, -1.1176e-01,  ...,  6.6973e-01,\n",
            "             3.8639e-01, -2.6554e-01],\n",
            "           [ 7.1489e-01,  4.2373e-01,  2.9958e-01,  ...,  7.2103e-01,\n",
            "            -4.6979e-02, -8.4370e-02]],\n",
            "\n",
            "          [[ 5.1938e-01,  1.0371e-01,  2.3274e-01,  ...,  1.1034e-01,\n",
            "             1.9279e-02, -4.9115e-01],\n",
            "           [ 3.2919e-01,  3.8266e-01,  2.2187e-01,  ...,  1.5062e-01,\n",
            "             4.1437e-01,  3.1724e-01],\n",
            "           [ 7.2970e-01, -1.7596e-01,  2.6191e-01,  ...,  3.9773e-01,\n",
            "            -2.4950e-01, -3.3544e-01],\n",
            "           ...,\n",
            "           [ 7.1832e-01,  2.6900e-01,  8.0286e-01,  ...,  3.6614e-01,\n",
            "            -1.8975e-01,  1.3814e-01],\n",
            "           [ 4.6979e-01,  5.2405e-01, -2.9181e-01,  ...,  8.2070e-01,\n",
            "             3.6701e-01, -3.4614e-01],\n",
            "           [ 6.7866e-01,  3.3143e-01,  1.9998e-02,  ...,  3.2202e-02,\n",
            "             3.1544e-02, -1.3048e-01]],\n",
            "\n",
            "          [[ 7.1435e-01, -9.9429e-03,  3.9317e-01,  ...,  1.4239e-01,\n",
            "             5.4227e-01, -2.4257e-01],\n",
            "           [ 7.0027e-01,  1.0036e-01,  5.7861e-01,  ..., -1.8545e-02,\n",
            "            -1.4103e-02, -2.7826e-03],\n",
            "           [ 1.2526e+00, -2.2271e-01,  8.5913e-01,  ...,  7.1125e-01,\n",
            "             3.1622e-01,  1.7918e-01],\n",
            "           ...,\n",
            "           [ 8.8973e-01,  1.1932e-01,  4.3762e-01,  ...,  1.2705e-02,\n",
            "             2.3107e-01, -5.2591e-01],\n",
            "           [ 5.0560e-01,  5.1425e-01, -5.5310e-02,  ...,  3.3075e-01,\n",
            "            -1.4754e-01, -4.9524e-01],\n",
            "           [ 4.4764e-01,  4.6244e-01,  1.2994e+00,  ...,  3.2592e-01,\n",
            "             6.1892e-01,  1.3513e-02]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 4.6186e-01, -3.8719e-04,  1.6038e-01,  ...,  2.3132e-01,\n",
            "            -2.7114e-01, -3.3738e-01],\n",
            "           [ 7.1255e-01,  6.3431e-01, -3.9060e-01,  ...,  8.1860e-02,\n",
            "            -1.0539e-01, -2.5856e-01],\n",
            "           [ 1.1153e-01, -5.4026e-02, -1.4092e-01,  ...,  1.0644e-01,\n",
            "            -1.6953e-01, -4.9183e-01],\n",
            "           ...,\n",
            "           [ 3.3626e-01,  5.5611e-01, -2.9226e-01,  ..., -9.2110e-02,\n",
            "             2.6132e-01, -2.0583e-01],\n",
            "           [ 3.9638e-01,  4.3993e-01, -1.6674e-01,  ...,  2.3865e-01,\n",
            "             4.2718e-02, -2.8443e-01],\n",
            "           [ 5.6895e-01,  7.2083e-01,  7.6611e-02,  ...,  6.1278e-01,\n",
            "             2.2046e-01, -2.0753e-01]],\n",
            "\n",
            "          [[ 5.4665e-01, -1.7750e-01, -2.0665e-02,  ..., -5.2151e-02,\n",
            "             1.0594e-01, -2.7190e-01],\n",
            "           [ 3.9854e-01, -2.6583e-02, -3.2601e-02,  ...,  6.1220e-02,\n",
            "             4.5618e-01,  1.7570e-01],\n",
            "           [ 6.9641e-01,  2.9803e-01,  2.1748e-01,  ...,  3.2972e-01,\n",
            "            -3.2207e-01, -6.3160e-01],\n",
            "           ...,\n",
            "           [ 5.8712e-01,  4.7744e-01,  5.5874e-01,  ...,  1.7854e-01,\n",
            "             1.6522e-02, -8.2442e-01],\n",
            "           [ 2.4265e-01,  3.0497e-01,  6.1992e-01,  ..., -2.8125e-01,\n",
            "             7.4799e-02, -9.8618e-01],\n",
            "           [ 4.5675e-01,  7.3323e-01,  1.7935e-01,  ...,  4.7920e-01,\n",
            "             5.6070e-01, -2.9039e-01]],\n",
            "\n",
            "          [[ 4.1191e-01, -2.0886e-01, -2.5267e-02,  ..., -5.6114e-02,\n",
            "            -1.4697e-01, -2.5621e-01],\n",
            "           [-2.4458e-02,  2.4980e-01, -1.5252e-01,  ...,  3.2065e-02,\n",
            "            -1.9164e-01,  8.3546e-02],\n",
            "           [ 4.7339e-01,  4.8972e-01,  6.4110e-01,  ..., -3.1656e-01,\n",
            "             2.7989e-01, -6.0966e-02],\n",
            "           ...,\n",
            "           [ 1.0542e-01,  2.6108e-01, -2.3841e-01,  ...,  2.0806e-01,\n",
            "             4.4356e-01,  2.8469e-02],\n",
            "           [ 2.2413e-01,  2.1969e-01,  7.8289e-01,  ...,  2.5640e-01,\n",
            "             1.8211e-02,  6.1707e-02],\n",
            "           [ 4.5233e-01,  1.9063e-01,  1.6050e-01,  ...,  2.1649e-01,\n",
            "             2.8250e-01, -4.0758e-02]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[-2.2987e-01, -1.6379e-01,  2.7415e-02,  ..., -1.3651e-01,\n",
            "            -1.7983e-01,  9.4531e-02],\n",
            "           [-8.1679e-03,  1.0979e-01,  3.0911e-02,  ...,  4.5323e-01,\n",
            "            -5.2985e-02,  5.8257e-02],\n",
            "           [-3.3511e-01, -3.4438e-02,  3.0537e-01,  ..., -1.5760e-01,\n",
            "             4.2779e-01,  1.8193e-01],\n",
            "           ...,\n",
            "           [ 3.0911e-02,  4.7605e-02,  1.5017e-01,  ...,  3.1330e-01,\n",
            "             2.6947e-01, -3.1556e-01],\n",
            "           [-2.0416e-01, -8.4907e-02,  3.8966e-01,  ..., -1.3683e-01,\n",
            "            -2.7638e-01,  4.8490e-03],\n",
            "           [-4.8787e-01, -4.6464e-01, -6.3932e-02,  ...,  1.1289e-01,\n",
            "            -1.7789e-01, -1.8059e-01]],\n",
            "\n",
            "          [[ 1.9228e-01,  5.3774e-01,  3.3788e-01,  ...,  1.1283e-01,\n",
            "             6.0184e-01,  4.6981e-01],\n",
            "           [-4.2707e-01,  2.0362e-01,  7.5103e-01,  ...,  5.9468e-01,\n",
            "             5.7503e-01,  7.9099e-02],\n",
            "           [-3.8632e-01,  3.1297e-01,  2.3604e-01,  ...,  1.1039e+00,\n",
            "             1.0075e+00,  5.7911e-01],\n",
            "           ...,\n",
            "           [-3.0618e-01,  7.0536e-01,  3.0077e-01,  ...,  3.0542e-01,\n",
            "             1.7405e-01,  6.0512e-01],\n",
            "           [-3.9815e-01,  3.4534e-01,  6.8294e-02,  ...,  2.2544e-01,\n",
            "             3.4682e-01, -7.9606e-02],\n",
            "           [-2.6008e-01,  2.4547e-01, -4.9040e-01,  ..., -3.5335e-01,\n",
            "            -3.2342e-01,  1.4108e-01]],\n",
            "\n",
            "          [[ 3.6141e-01,  2.6913e-01,  1.9971e-01,  ...,  3.3093e-01,\n",
            "             5.3965e-01,  5.5353e-01],\n",
            "           [-1.0184e-01,  3.7855e-01,  2.3036e-01,  ...,  1.0609e+00,\n",
            "             7.8875e-01,  1.8732e-01],\n",
            "           [-2.5479e-01,  3.2388e-02,  5.2502e-01,  ...,  2.2916e-01,\n",
            "             8.6485e-01,  4.1359e-01],\n",
            "           ...,\n",
            "           [ 5.1532e-02,  4.8384e-02,  1.8174e-01,  ...,  9.7709e-02,\n",
            "             2.8060e-01,  2.1272e-01],\n",
            "           [-1.3505e-01,  3.0951e-01, -3.1877e-03,  ...,  5.6779e-01,\n",
            "             1.2798e-01, -1.2705e-01],\n",
            "           [-1.1044e-01,  4.1678e-02,  1.7639e-01,  ...,  1.6285e-01,\n",
            "             4.7380e-02, -5.8205e-02]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 1.7749e-01,  1.8455e-01,  3.7788e-01,  ...,  5.1649e-01,\n",
            "             8.8343e-01,  4.7881e-01],\n",
            "           [ 7.5160e-02,  2.5622e-01,  5.9129e-01,  ...,  3.2128e-01,\n",
            "             2.7473e-01, -1.6357e-01],\n",
            "           [-3.4371e-01, -1.6195e-01,  8.5730e-01,  ...,  3.3104e-01,\n",
            "             8.2126e-01,  7.0526e-01],\n",
            "           ...,\n",
            "           [-2.1864e-01,  1.8691e-01,  1.0537e-01,  ...,  4.0862e-01,\n",
            "             6.4651e-01,  2.9598e-02],\n",
            "           [-1.6447e-01,  4.7879e-01,  5.3909e-01,  ...,  1.9487e-01,\n",
            "             3.5067e-01, -8.3202e-02],\n",
            "           [-3.0192e-01, -1.9124e-01, -1.8556e-01,  ..., -1.4178e-01,\n",
            "            -6.6121e-01, -4.0345e-01]],\n",
            "\n",
            "          [[-2.4002e-01,  9.1582e-02,  3.3713e-01,  ...,  5.8177e-01,\n",
            "             4.0395e-01,  3.1241e-01],\n",
            "           [ 1.0420e-01,  7.3905e-01, -2.4527e-02,  ...,  2.8223e-01,\n",
            "             8.3715e-01,  6.9444e-02],\n",
            "           [ 8.8779e-02,  4.1141e-01, -3.0150e-01,  ...,  2.2560e-01,\n",
            "             2.7846e-01,  4.3462e-01],\n",
            "           ...,\n",
            "           [-6.1632e-02,  2.3034e-01, -1.2251e-01,  ...,  1.0515e-01,\n",
            "             4.8556e-01,  1.0294e-01],\n",
            "           [ 9.6186e-02,  4.4354e-01,  4.1657e-02,  ...,  1.6205e-01,\n",
            "             4.9224e-01, -3.2561e-01],\n",
            "           [-3.5355e-01, -1.8610e-01, -5.1481e-01,  ..., -2.7930e-01,\n",
            "            -6.3792e-01, -6.2131e-01]],\n",
            "\n",
            "          [[ 2.7386e-01,  3.8104e-01,  2.5922e-01,  ...,  3.9554e-01,\n",
            "             5.7638e-01,  5.0655e-01],\n",
            "           [-1.3721e-01,  3.2953e-01,  1.8121e-01,  ..., -3.0094e-01,\n",
            "             3.0082e-02,  4.1448e-02],\n",
            "           [-8.9744e-02, -5.7017e-02, -2.6174e-02,  ...,  1.0103e-01,\n",
            "             7.0276e-01,  1.3147e-02],\n",
            "           ...,\n",
            "           [ 2.5059e-01,  4.9770e-01,  3.4149e-01,  ...,  4.8966e-01,\n",
            "             7.2040e-02,  3.2779e-01],\n",
            "           [-2.5645e-01, -1.6689e-02,  2.8767e-01,  ...,  1.3710e-01,\n",
            "             6.6559e-02, -3.7138e-01],\n",
            "           [-1.9939e-01, -1.4602e-03, -1.1318e-01,  ..., -2.9530e-01,\n",
            "            -2.6035e-01, -4.1975e-01]]],\n",
            "\n",
            "\n",
            "         [[[-7.1051e-01, -7.3525e-01, -3.6315e-01,  ..., -6.3840e-01,\n",
            "            -6.9570e-01, -4.9404e-01],\n",
            "           [-9.1849e-01, -3.3098e-01, -3.3689e-01,  ..., -7.7371e-01,\n",
            "            -8.2606e-01, -8.7806e-01],\n",
            "           [-9.5984e-01, -6.1396e-01, -1.7755e-01,  ..., -7.4829e-01,\n",
            "            -7.2338e-01, -2.2452e-01],\n",
            "           ...,\n",
            "           [-7.9806e-01, -6.2449e-01, -5.4043e-01,  ..., -6.3606e-01,\n",
            "            -7.2436e-01, -1.0335e+00],\n",
            "           [-9.2558e-01, -7.7377e-01, -3.9718e-02,  ..., -4.1029e-01,\n",
            "            -9.2160e-01, -7.2902e-01],\n",
            "           [-1.2307e+00, -1.4162e+00, -9.0965e-01,  ..., -1.3701e+00,\n",
            "            -1.3459e+00, -1.1029e+00]],\n",
            "\n",
            "          [[-6.3176e-01, -2.6487e-01, -5.6530e-01,  ..., -5.5024e-01,\n",
            "            -2.6856e-01, -2.0011e-02],\n",
            "           [-9.5379e-01, -4.2974e-02,  3.2597e-01,  ..., -1.0504e-01,\n",
            "            -1.5678e-01, -8.9824e-01],\n",
            "           [-1.2427e+00, -2.7153e-01, -3.0842e-01,  ..., -5.9348e-02,\n",
            "             5.5965e-02, -6.0546e-01],\n",
            "           ...,\n",
            "           [-1.1989e+00, -6.4754e-01, -2.2904e-01,  ..., -1.0759e+00,\n",
            "            -9.7049e-01, -8.9527e-01],\n",
            "           [-1.3319e+00, -2.1863e-01, -3.4955e-01,  ..., -7.5179e-01,\n",
            "            -9.9054e-01, -7.7042e-01],\n",
            "           [-1.1248e+00, -7.2845e-01, -9.5987e-01,  ..., -1.3614e+00,\n",
            "            -1.2811e+00, -1.0859e+00]],\n",
            "\n",
            "          [[-9.4623e-01, -4.4145e-01, -3.9602e-01,  ..., -4.5115e-01,\n",
            "            -3.4824e-01, -3.9096e-01],\n",
            "           [-7.9173e-01, -2.7664e-01, -6.6552e-01,  ..., -2.7118e-01,\n",
            "            -3.5845e-01, -1.2407e+00],\n",
            "           [-1.2924e+00, -5.2796e-01, -2.6705e-01,  ..., -1.4953e+00,\n",
            "            -1.0817e+00, -7.2123e-01],\n",
            "           ...,\n",
            "           [-5.4224e-01, -6.3344e-01,  6.3873e-02,  ..., -5.7673e-01,\n",
            "            -9.4827e-01, -7.3425e-01],\n",
            "           [-7.8073e-01,  1.2288e-01, -1.4134e-01,  ..., -3.6277e-01,\n",
            "            -7.7668e-01, -1.2315e+00],\n",
            "           [-8.7792e-01, -5.9375e-01, -4.4191e-01,  ..., -1.2756e+00,\n",
            "            -1.3812e+00, -1.3658e+00]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-7.7302e-01, -2.6092e-01, -3.6726e-01,  ..., -3.2954e-01,\n",
            "            -4.8199e-02, -1.7229e-01],\n",
            "           [-8.3632e-01, -2.9610e-01, -1.1238e-01,  ..., -6.8509e-03,\n",
            "            -8.5015e-01, -8.4376e-01],\n",
            "           [-1.3778e+00, -5.7552e-01, -4.5543e-01,  ..., -1.2062e-01,\n",
            "            -1.0786e+00, -5.3234e-01],\n",
            "           ...,\n",
            "           [-5.7161e-01, -6.3282e-01, -1.5963e-01,  ..., -5.2828e-01,\n",
            "            -4.0345e-01, -9.6832e-01],\n",
            "           [-1.1481e+00, -9.7129e-02, -4.5311e-01,  ..., -6.3950e-01,\n",
            "            -8.8778e-01, -9.6523e-01],\n",
            "           [-1.2024e+00, -1.0586e+00, -1.1511e+00,  ..., -1.5058e+00,\n",
            "            -1.8059e+00, -1.5633e+00]],\n",
            "\n",
            "          [[-9.7771e-01, -2.9397e-01, -2.9324e-01,  ..., -1.0112e-01,\n",
            "            -4.8748e-01, -4.2683e-01],\n",
            "           [-9.6245e-01,  1.9344e-01, -5.4191e-01,  ..., -8.7205e-01,\n",
            "            -4.8839e-01, -1.1707e+00],\n",
            "           [-8.0728e-01, -4.1666e-01, -8.7771e-01,  ..., -5.6441e-01,\n",
            "            -2.6730e-02, -8.9965e-01],\n",
            "           ...,\n",
            "           [-8.5370e-01, -6.5284e-01, -8.4968e-01,  ..., -1.5776e-01,\n",
            "            -4.2565e-01, -1.0844e+00],\n",
            "           [-6.7051e-01, -1.8850e-01, -8.8002e-01,  ..., -4.4198e-01,\n",
            "            -6.4589e-01, -1.1997e+00],\n",
            "           [-1.5527e+00, -1.2391e+00, -1.1108e+00,  ..., -1.0812e+00,\n",
            "            -1.3434e+00, -1.3871e+00]],\n",
            "\n",
            "          [[-1.1552e+00, -6.7646e-01, -5.7711e-01,  ..., -8.0330e-01,\n",
            "            -4.3785e-01, -6.9325e-01],\n",
            "           [-9.8194e-01, -3.9882e-01, -8.4927e-01,  ..., -7.7662e-01,\n",
            "            -8.0872e-01, -1.1241e+00],\n",
            "           [-1.1642e+00, -9.2931e-01, -5.5471e-01,  ..., -1.3414e+00,\n",
            "            -7.3263e-01, -1.0407e+00],\n",
            "           ...,\n",
            "           [-8.0650e-01, -5.7091e-01, -9.9843e-01,  ..., -9.2972e-01,\n",
            "            -7.3703e-01, -9.4841e-01],\n",
            "           [-1.3277e+00, -6.5822e-01, -6.3110e-01,  ..., -8.1669e-01,\n",
            "            -5.3768e-01, -1.1505e+00],\n",
            "           [-1.0257e+00, -1.2903e+00, -1.1797e+00,  ..., -1.3650e+00,\n",
            "            -1.2962e+00, -1.8176e+00]]],\n",
            "\n",
            "\n",
            "         [[[ 1.3960e-01, -1.1340e-01,  8.1436e-03,  ..., -5.4148e-02,\n",
            "             1.0322e-01, -2.6193e-01],\n",
            "           [ 6.2402e-01,  9.5179e-02,  3.0360e-03,  ...,  1.9871e-01,\n",
            "             5.0470e-01,  1.6783e-01],\n",
            "           [ 8.2418e-01, -4.7310e-02, -2.1844e-01,  ...,  5.1876e-01,\n",
            "            -2.5004e-02, -3.3038e-02],\n",
            "           ...,\n",
            "           [ 3.4007e-01,  9.9105e-02,  3.1091e-02,  ...,  7.3530e-01,\n",
            "             3.3809e-01,  3.1082e-01],\n",
            "           [ 6.3083e-01,  3.1947e-01,  7.0247e-01,  ...,  4.3395e-02,\n",
            "             1.5830e-01,  6.5107e-04],\n",
            "           [ 7.9249e-01,  8.1423e-01,  7.0150e-01,  ...,  7.5295e-01,\n",
            "             5.3113e-01, -4.7062e-02]],\n",
            "\n",
            "          [[ 3.8223e-01, -2.5753e-01,  4.9245e-01,  ..., -1.5054e-01,\n",
            "             4.9146e-01, -1.8371e-01],\n",
            "           [ 2.5441e-01,  3.5096e-01,  4.0238e-02,  ..., -1.0101e-02,\n",
            "             3.2204e-01,  6.0451e-01],\n",
            "           [ 6.0868e-01, -9.0823e-02,  2.3172e-01,  ...,  7.3648e-01,\n",
            "            -3.4692e-01,  4.4599e-02],\n",
            "           ...,\n",
            "           [ 7.4959e-01,  4.6415e-01, -1.9848e-01,  ..., -3.7810e-01,\n",
            "             3.1001e-01, -3.0036e-02],\n",
            "           [ 5.9454e-01,  5.7860e-01, -1.0956e-01,  ...,  4.5375e-01,\n",
            "             3.4017e-01, -2.3539e-01],\n",
            "           [ 6.0006e-01,  5.6556e-01,  3.6591e-03,  ..., -1.5311e-01,\n",
            "             1.2455e-02, -2.7762e-01]],\n",
            "\n",
            "          [[ 3.7823e-01, -7.8233e-02,  1.1427e-01,  ...,  2.1292e-01,\n",
            "             9.8103e-02,  9.6879e-02],\n",
            "           [ 4.2304e-01, -1.1381e-02,  9.4354e-02,  ...,  1.3644e-01,\n",
            "             4.9190e-01,  4.5008e-02],\n",
            "           [ 5.9183e-01,  2.5802e-01,  2.2137e-01,  ...,  1.9019e-01,\n",
            "             2.1857e-01, -1.6257e-01],\n",
            "           ...,\n",
            "           [ 3.5947e-01,  3.7027e-01, -2.2599e-02,  ...,  3.4742e-01,\n",
            "             9.7077e-01, -1.7572e-01],\n",
            "           [ 1.1447e+00, -1.5519e-01,  8.1056e-02,  ...,  2.8964e-01,\n",
            "             4.8446e-01, -4.8426e-01],\n",
            "           [ 5.6131e-01,  5.4393e-01,  5.0067e-01,  ...,  3.1217e-01,\n",
            "             5.1056e-01, -1.7589e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 4.8102e-01,  2.7959e-01,  4.5240e-01,  ...,  1.8408e-01,\n",
            "             1.6553e-01, -1.3604e-01],\n",
            "           [ 4.7676e-01,  3.5082e-01,  5.0131e-01,  ...,  4.5643e-02,\n",
            "            -8.1778e-02, -2.4551e-01],\n",
            "           [ 6.2879e-01,  5.1131e-01,  4.1361e-01,  ...,  3.0050e-01,\n",
            "             7.6448e-01,  1.3983e-01],\n",
            "           ...,\n",
            "           [ 2.4264e-01,  5.4549e-01,  1.5868e-01,  ..., -1.8182e-01,\n",
            "            -8.6521e-02, -1.0307e+00],\n",
            "           [ 7.4266e-01,  6.0688e-01,  2.9138e-01,  ..., -3.2345e-01,\n",
            "            -3.2278e-01, -8.3312e-02],\n",
            "           [ 9.0246e-01,  1.2517e+00,  9.2181e-01,  ...,  3.5400e-01,\n",
            "             8.5240e-01,  1.8371e-01]],\n",
            "\n",
            "          [[ 7.8000e-02, -1.0839e-01,  1.0241e-01,  ..., -3.3400e-01,\n",
            "            -9.6290e-02, -2.5645e-01],\n",
            "           [ 5.6514e-01,  8.5819e-02,  4.4047e-01,  ...,  6.3575e-01,\n",
            "             8.9823e-02, -5.3826e-01],\n",
            "           [ 4.0801e-01,  6.2724e-01,  4.7310e-01,  ..., -5.2290e-02,\n",
            "            -3.4383e-01, -5.5758e-01],\n",
            "           ...,\n",
            "           [ 5.3795e-01,  8.0318e-02,  5.9844e-01,  ...,  4.5981e-01,\n",
            "            -1.6182e-01, -2.0357e-01],\n",
            "           [ 1.4323e-01,  3.5741e-01, -7.8994e-02,  ...,  1.7502e-01,\n",
            "            -3.5551e-01, -4.8253e-01],\n",
            "           [ 7.4480e-01,  7.0949e-01,  5.8235e-01,  ...,  7.0641e-01,\n",
            "             7.1389e-01, -6.3558e-02]],\n",
            "\n",
            "          [[ 6.1529e-01,  1.4541e-01,  3.8599e-01,  ...,  2.3616e-01,\n",
            "             2.7584e-01, -2.1072e-01],\n",
            "           [ 4.7268e-01,  1.8608e-01, -1.6715e-02,  ...,  1.0455e-01,\n",
            "             1.2168e-01, -1.0894e-01],\n",
            "           [ 6.0859e-01,  1.7579e-01,  2.7487e-01,  ...,  8.9971e-01,\n",
            "            -1.5124e-01, -2.3161e-01],\n",
            "           ...,\n",
            "           [ 6.1322e-01,  1.1150e-01,  9.7864e-03,  ...,  5.4495e-01,\n",
            "            -2.2129e-01, -1.1802e-01],\n",
            "           [ 3.5434e-01, -2.3948e-01,  3.0259e-01,  ..., -2.7552e-01,\n",
            "            -4.4686e-01,  3.0122e-01],\n",
            "           [ 1.6870e-01,  3.1346e-01,  3.8519e-01,  ...,  5.3761e-01,\n",
            "             4.3966e-01,  7.2507e-02]]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(probability.shape)\n",
        "print(probability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxBKEIIneFRa",
        "outputId": "7978a0b6-79e5-443c-ba34-ae51b851eadc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 128, 128, 128])\n",
            "tensor([[[[[0.3762, 0.3528, 0.3208,  ..., 0.3100, 0.3669, 0.4353],\n",
            "           [0.3101, 0.3343, 0.3509,  ..., 0.4251, 0.4239, 0.4132],\n",
            "           [0.2532, 0.2670, 0.3891,  ..., 0.3329, 0.4056, 0.4735],\n",
            "           ...,\n",
            "           [0.2350, 0.3264, 0.4719,  ..., 0.3769, 0.3087, 0.4180],\n",
            "           [0.2656, 0.3295, 0.5608,  ..., 0.2972, 0.3414, 0.4704],\n",
            "           [0.2144, 0.2769, 0.2929,  ..., 0.2608, 0.4022, 0.4160]],\n",
            "\n",
            "          [[0.2750, 0.3939, 0.4005,  ..., 0.4143, 0.4988, 0.5189],\n",
            "           [0.3604, 0.3564, 0.3147,  ..., 0.4919, 0.5233, 0.4817],\n",
            "           [0.2539, 0.4704, 0.5001,  ..., 0.4713, 0.4683, 0.5652],\n",
            "           ...,\n",
            "           [0.2681, 0.4355, 0.3843,  ..., 0.4429, 0.5472, 0.5228],\n",
            "           [0.2113, 0.3147, 0.4570,  ..., 0.4266, 0.4939, 0.3595],\n",
            "           [0.2627, 0.3321, 0.2906,  ..., 0.3127, 0.3685, 0.4270]],\n",
            "\n",
            "          [[0.3718, 0.5043, 0.4460,  ..., 0.4842, 0.4683, 0.5622],\n",
            "           [0.3137, 0.3719, 0.3921,  ..., 0.4323, 0.4583, 0.4467],\n",
            "           [0.2597, 0.4593, 0.4009,  ..., 0.2900, 0.4538, 0.4622],\n",
            "           ...,\n",
            "           [0.2672, 0.4705, 0.2905,  ..., 0.4829, 0.3683, 0.4709],\n",
            "           [0.3436, 0.4375, 0.4134,  ..., 0.3842, 0.6436, 0.4870],\n",
            "           [0.2701, 0.3067, 0.2939,  ..., 0.3152, 0.2737, 0.3900]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.3777, 0.4701, 0.4348,  ..., 0.5369, 0.5540, 0.5372],\n",
            "           [0.2844, 0.3354, 0.4758,  ..., 0.4410, 0.4566, 0.6105],\n",
            "           [0.3386, 0.3659, 0.3644,  ..., 0.3894, 0.3867, 0.5285],\n",
            "           ...,\n",
            "           [0.2918, 0.4104, 0.5596,  ..., 0.4529, 0.4071, 0.4514],\n",
            "           [0.3515, 0.3029, 0.4060,  ..., 0.3637, 0.3401, 0.4317],\n",
            "           [0.2543, 0.3371, 0.3675,  ..., 0.2716, 0.3500, 0.3602]],\n",
            "\n",
            "          [[0.3736, 0.4788, 0.4401,  ..., 0.4964, 0.5014, 0.5380],\n",
            "           [0.3716, 0.5143, 0.4447,  ..., 0.3582, 0.2662, 0.4732],\n",
            "           [0.3310, 0.3983, 0.3591,  ..., 0.4813, 0.5648, 0.5247],\n",
            "           ...,\n",
            "           [0.3939, 0.3145, 0.3606,  ..., 0.3377, 0.4331, 0.5538],\n",
            "           [0.3730, 0.4538, 0.4266,  ..., 0.5181, 0.3936, 0.5544],\n",
            "           [0.3151, 0.2404, 0.3588,  ..., 0.2537, 0.2451, 0.4450]],\n",
            "\n",
            "          [[0.4037, 0.4884, 0.4817,  ..., 0.4979, 0.4807, 0.5100],\n",
            "           [0.4331, 0.3978, 0.4313,  ..., 0.4443, 0.4321, 0.3777],\n",
            "           [0.3417, 0.3398, 0.3049,  ..., 0.4393, 0.4903, 0.4018],\n",
            "           ...,\n",
            "           [0.4291, 0.4777, 0.5856,  ..., 0.4778, 0.3821, 0.4332],\n",
            "           [0.4062, 0.4128, 0.2380,  ..., 0.3444, 0.4680, 0.3530],\n",
            "           [0.3781, 0.3414, 0.3198,  ..., 0.2989, 0.2578, 0.3826]]],\n",
            "\n",
            "\n",
            "         [[[0.1881, 0.2106, 0.1568,  ..., 0.2345, 0.2153, 0.2323],\n",
            "           [0.1698, 0.1848, 0.1525,  ..., 0.2122, 0.2074, 0.2695],\n",
            "           [0.1245, 0.1742, 0.2057,  ..., 0.1386, 0.1433, 0.2468],\n",
            "           ...,\n",
            "           [0.1340, 0.2027, 0.1418,  ..., 0.1122, 0.0826, 0.1280],\n",
            "           [0.1374, 0.1232, 0.1558,  ..., 0.1620, 0.1510, 0.2305],\n",
            "           [0.0909, 0.1224, 0.1668,  ..., 0.0711, 0.1933, 0.1853]],\n",
            "\n",
            "          [[0.1386, 0.2560, 0.2334,  ..., 0.2479, 0.2204, 0.2621],\n",
            "           [0.1667, 0.1918, 0.2727,  ..., 0.2166, 0.1349, 0.1339],\n",
            "           [0.0997, 0.2602, 0.1573,  ..., 0.1599, 0.2700, 0.1898],\n",
            "           ...,\n",
            "           [0.1266, 0.1855, 0.1259,  ..., 0.1573, 0.2346, 0.1508],\n",
            "           [0.1345, 0.1560, 0.2726,  ..., 0.1179, 0.1440, 0.2375],\n",
            "           [0.0762, 0.0979, 0.1826,  ..., 0.1223, 0.1618, 0.1290]],\n",
            "\n",
            "          [[0.0960, 0.2265, 0.2095,  ..., 0.1968, 0.1397, 0.2159],\n",
            "           [0.1219, 0.2312, 0.1687,  ..., 0.2353, 0.2195, 0.2240],\n",
            "           [0.0662, 0.2730, 0.0857,  ..., 0.2142, 0.1040, 0.1417],\n",
            "           ...,\n",
            "           [0.0991, 0.2346, 0.1886,  ..., 0.2126, 0.2445, 0.1913],\n",
            "           [0.0884, 0.1742, 0.2651,  ..., 0.1683, 0.1120, 0.2472],\n",
            "           [0.0915, 0.1290, 0.0517,  ..., 0.1499, 0.0854, 0.1039]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.1257, 0.2363, 0.2120,  ..., 0.1835, 0.2087, 0.2244],\n",
            "           [0.1470, 0.1469, 0.3105,  ..., 0.2302, 0.2729, 0.1277],\n",
            "           [0.1999, 0.1801, 0.2701,  ..., 0.1704, 0.3090, 0.1949],\n",
            "           ...,\n",
            "           [0.1711, 0.1484, 0.2222,  ..., 0.2498, 0.1773, 0.2035],\n",
            "           [0.1205, 0.1649, 0.2766,  ..., 0.1351, 0.2884, 0.1296],\n",
            "           [0.1541, 0.1037, 0.1408,  ..., 0.0587, 0.1400, 0.1599]],\n",
            "\n",
            "          [[0.1567, 0.2795, 0.1888,  ..., 0.1663, 0.1775, 0.2554],\n",
            "           [0.1033, 0.1997, 0.1928,  ..., 0.1768, 0.1720, 0.1374],\n",
            "           [0.1271, 0.2199, 0.1460,  ..., 0.1187, 0.1799, 0.2032],\n",
            "           ...,\n",
            "           [0.1252, 0.2258, 0.2507,  ..., 0.1732, 0.2438, 0.2338],\n",
            "           [0.1575, 0.1829, 0.0853,  ..., 0.2225, 0.1777, 0.3033],\n",
            "           [0.1197, 0.1135, 0.1384,  ..., 0.1038, 0.0776, 0.1288]],\n",
            "\n",
            "          [[0.1257, 0.1921, 0.1591,  ..., 0.1660, 0.1580, 0.1696],\n",
            "           [0.1506, 0.1599, 0.2782,  ..., 0.1697, 0.1916, 0.1455],\n",
            "           [0.1435, 0.1018, 0.1033,  ..., 0.2291, 0.1356, 0.1448],\n",
            "           ...,\n",
            "           [0.1703, 0.1259, 0.1454,  ..., 0.1421, 0.1634, 0.1117],\n",
            "           [0.1494, 0.1116, 0.0980,  ..., 0.2250, 0.1477, 0.1351],\n",
            "           [0.1293, 0.1607, 0.1854,  ..., 0.1633, 0.1639, 0.1376]]],\n",
            "\n",
            "\n",
            "         [[[0.4357, 0.4366, 0.5225,  ..., 0.4555, 0.4177, 0.3324],\n",
            "           [0.5202, 0.4809, 0.4965,  ..., 0.3627, 0.3687, 0.3173],\n",
            "           [0.6223, 0.5588, 0.4052,  ..., 0.5285, 0.4511, 0.2797],\n",
            "           ...,\n",
            "           [0.6310, 0.4709, 0.3864,  ..., 0.5109, 0.6087, 0.4540],\n",
            "           [0.5970, 0.5472, 0.2834,  ..., 0.5408, 0.5076, 0.2991],\n",
            "           [0.6947, 0.6007, 0.5404,  ..., 0.6681, 0.4045, 0.3988]],\n",
            "\n",
            "          [[0.5865, 0.3501, 0.3661,  ..., 0.3378, 0.2808, 0.2190],\n",
            "           [0.4730, 0.4518, 0.4126,  ..., 0.2915, 0.3418, 0.3844],\n",
            "           [0.6464, 0.2694, 0.3425,  ..., 0.3688, 0.2618, 0.2450],\n",
            "           ...,\n",
            "           [0.6054, 0.3791, 0.4898,  ..., 0.3999, 0.2181, 0.3263],\n",
            "           [0.6542, 0.5292, 0.2704,  ..., 0.4555, 0.3620, 0.4029],\n",
            "           [0.6611, 0.5700, 0.5268,  ..., 0.5650, 0.4697, 0.4440]],\n",
            "\n",
            "          [[0.5322, 0.2692, 0.3446,  ..., 0.3190, 0.3919, 0.2219],\n",
            "           [0.5644, 0.3969, 0.4392,  ..., 0.3324, 0.3222, 0.3293],\n",
            "           [0.6741, 0.2676, 0.5135,  ..., 0.4958, 0.4422, 0.3961],\n",
            "           ...,\n",
            "           [0.6337, 0.2949, 0.5210,  ..., 0.3045, 0.3873, 0.3378],\n",
            "           [0.5680, 0.3883, 0.3215,  ..., 0.4476, 0.2444, 0.2658],\n",
            "           [0.6384, 0.5643, 0.6543,  ..., 0.5350, 0.6409, 0.5061]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.4966, 0.2936, 0.3532,  ..., 0.2796, 0.2373, 0.2384],\n",
            "           [0.5686, 0.5177, 0.2137,  ..., 0.3288, 0.2705, 0.2618],\n",
            "           [0.4614, 0.4541, 0.3655,  ..., 0.4402, 0.3043, 0.2766],\n",
            "           ...,\n",
            "           [0.5371, 0.4412, 0.2181,  ..., 0.2973, 0.4156, 0.3451],\n",
            "           [0.5280, 0.5322, 0.3174,  ..., 0.5013, 0.3716, 0.4387],\n",
            "           [0.5916, 0.5592, 0.4917,  ..., 0.6698, 0.5101, 0.4799]],\n",
            "\n",
            "          [[0.4698, 0.2417, 0.3710,  ..., 0.3373, 0.3211, 0.2066],\n",
            "           [0.5251, 0.2859, 0.3625,  ..., 0.4650, 0.5618, 0.3894],\n",
            "           [0.5419, 0.3818, 0.4949,  ..., 0.4001, 0.2553, 0.2721],\n",
            "           ...,\n",
            "           [0.4809, 0.4596, 0.3887,  ..., 0.4891, 0.3232, 0.2123],\n",
            "           [0.4695, 0.3633, 0.4881,  ..., 0.2593, 0.4288, 0.1423],\n",
            "           [0.5652, 0.6460, 0.5028,  ..., 0.6425, 0.6773, 0.4262]],\n",
            "\n",
            "          [[0.4706, 0.3195, 0.3592,  ..., 0.3361, 0.3613, 0.3205],\n",
            "           [0.4163, 0.4423, 0.2905,  ..., 0.3860, 0.3762, 0.4768],\n",
            "           [0.5148, 0.5584, 0.5919,  ..., 0.3316, 0.3741, 0.4534],\n",
            "           ...,\n",
            "           [0.4006, 0.3964, 0.2690,  ..., 0.3801, 0.4544, 0.4552],\n",
            "           [0.4444, 0.4756, 0.6640,  ..., 0.4306, 0.3843, 0.5119],\n",
            "           [0.4926, 0.4979, 0.4948,  ..., 0.5378, 0.5783, 0.4798]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0.3262, 0.3822, 0.3763,  ..., 0.3716, 0.3420, 0.4434],\n",
            "           [0.3045, 0.3804, 0.3753,  ..., 0.4834, 0.3117, 0.3987],\n",
            "           [0.2117, 0.3926, 0.4526,  ..., 0.2840, 0.5123, 0.4044],\n",
            "           ...,\n",
            "           [0.3573, 0.3901, 0.4186,  ..., 0.3434, 0.4096, 0.2977],\n",
            "           [0.2638, 0.3333, 0.3313,  ..., 0.3380, 0.3258, 0.4039],\n",
            "           [0.1971, 0.2009, 0.2794,  ..., 0.3201, 0.2991, 0.3936]],\n",
            "\n",
            "          [[0.3777, 0.5264, 0.3887,  ..., 0.4379, 0.4321, 0.4688],\n",
            "           [0.2803, 0.3401, 0.4662,  ..., 0.4895, 0.4430, 0.3260],\n",
            "           [0.2422, 0.4494, 0.3882,  ..., 0.4988, 0.6082, 0.5286],\n",
            "           ...,\n",
            "           [0.2334, 0.4892, 0.4554,  ..., 0.5694, 0.4058, 0.5705],\n",
            "           [0.2444, 0.3532, 0.4007,  ..., 0.3798, 0.4433, 0.4243],\n",
            "           [0.2642, 0.3630, 0.3063,  ..., 0.3866, 0.3593, 0.5125]],\n",
            "\n",
            "          [[0.4372, 0.4550, 0.4050,  ..., 0.4262, 0.4867, 0.4945],\n",
            "           [0.3133, 0.4553, 0.4384,  ..., 0.6022, 0.4853, 0.4746],\n",
            "           [0.2713, 0.3541, 0.4564,  ..., 0.4673, 0.6000, 0.5309],\n",
            "           ...,\n",
            "           [0.3433, 0.3466, 0.3698,  ..., 0.3580, 0.3043, 0.4840],\n",
            "           [0.1953, 0.4068, 0.3380,  ..., 0.4648, 0.3530, 0.4924],\n",
            "           [0.2922, 0.3143, 0.3422,  ..., 0.4170, 0.3535, 0.4631]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.3648, 0.3649, 0.3918,  ..., 0.4659, 0.5314, 0.4849],\n",
            "           [0.3453, 0.3739, 0.4152,  ..., 0.4033, 0.4939, 0.4119],\n",
            "           [0.2500, 0.2761, 0.5234,  ..., 0.3837, 0.4775, 0.5382],\n",
            "           ...,\n",
            "           [0.3041, 0.3482, 0.3544,  ..., 0.5139, 0.5463, 0.5831],\n",
            "           [0.2597, 0.3705, 0.4648,  ..., 0.4927, 0.5556, 0.4143],\n",
            "           [0.2109, 0.1769, 0.2269,  ..., 0.3451, 0.1706, 0.3213]],\n",
            "\n",
            "          [[0.3506, 0.4002, 0.4304,  ..., 0.5248, 0.4959, 0.4893],\n",
            "           [0.3413, 0.4762, 0.3137,  ..., 0.3650, 0.5749, 0.5453],\n",
            "           [0.3592, 0.3734, 0.2680,  ..., 0.4522, 0.4398, 0.6120],\n",
            "           ...,\n",
            "           [0.3054, 0.4397, 0.2825,  ..., 0.3130, 0.5194, 0.4899],\n",
            "           [0.3980, 0.4083, 0.4378,  ..., 0.3907, 0.5718, 0.4401],\n",
            "           [0.2325, 0.2633, 0.2199,  ..., 0.2422, 0.1866, 0.3114]],\n",
            "\n",
            "          [[0.3779, 0.4679, 0.3893,  ..., 0.4642, 0.4755, 0.5589],\n",
            "           [0.3058, 0.4257, 0.4593,  ..., 0.3204, 0.3955, 0.4604],\n",
            "           [0.2983, 0.3731, 0.3401,  ..., 0.2891, 0.6011, 0.4692],\n",
            "           ...,\n",
            "           [0.3591, 0.4943, 0.5052,  ..., 0.4350, 0.4564, 0.5210],\n",
            "           [0.3140, 0.4298, 0.4142,  ..., 0.4885, 0.4662, 0.2926],\n",
            "           [0.3469, 0.3780, 0.3344,  ..., 0.2745, 0.2969, 0.3468]]],\n",
            "\n",
            "\n",
            "         [[[0.2017, 0.2158, 0.2546,  ..., 0.2249, 0.2042, 0.2461],\n",
            "           [0.1225, 0.2448, 0.2598,  ..., 0.1417, 0.1439, 0.1563],\n",
            "           [0.1134, 0.2199, 0.2793,  ..., 0.1573, 0.1620, 0.2694],\n",
            "           ...,\n",
            "           [0.1560, 0.1992, 0.2098,  ..., 0.1329, 0.1516, 0.1452],\n",
            "           [0.1282, 0.1674, 0.2157,  ..., 0.2572, 0.1709, 0.1939],\n",
            "           [0.0938, 0.0776, 0.1199,  ..., 0.0727, 0.0930, 0.1565]],\n",
            "\n",
            "          [[0.1657, 0.2359, 0.1575,  ..., 0.2256, 0.1810, 0.2873],\n",
            "           [0.1655, 0.2658, 0.3048,  ..., 0.2431, 0.2131, 0.1227],\n",
            "           [0.1029, 0.2505, 0.2252,  ..., 0.1558, 0.2348, 0.1617],\n",
            "           ...,\n",
            "           [0.0956, 0.1265, 0.2681,  ..., 0.1431, 0.1292, 0.1272],\n",
            "           [0.0961, 0.2009, 0.2639,  ..., 0.1429, 0.1164, 0.2126],\n",
            "           [0.1113, 0.1371, 0.1916,  ..., 0.1411, 0.1379, 0.1503]],\n",
            "\n",
            "          [[0.1182, 0.2236, 0.2232,  ..., 0.1950, 0.2003, 0.1923],\n",
            "           [0.1572, 0.2364, 0.1790,  ..., 0.1589, 0.1541, 0.1138],\n",
            "           [0.0961, 0.2022, 0.2067,  ..., 0.0833, 0.0857, 0.1707],\n",
            "           ...,\n",
            "           [0.1896, 0.1753, 0.3287,  ..., 0.1824, 0.0890, 0.1878],\n",
            "           [0.1024, 0.3376, 0.2944,  ..., 0.1833, 0.1428, 0.1632],\n",
            "           [0.1357, 0.1665, 0.1844,  ..., 0.0989, 0.0847, 0.1252]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.1410, 0.2337, 0.1860,  ..., 0.1999, 0.2093, 0.2529],\n",
            "           [0.1388, 0.2152, 0.2054,  ..., 0.2905, 0.1604, 0.2086],\n",
            "           [0.0889, 0.1826, 0.1408,  ..., 0.2442, 0.0714, 0.1561],\n",
            "           ...,\n",
            "           [0.2136, 0.1534, 0.2719,  ..., 0.2014, 0.1912, 0.2150],\n",
            "           [0.0971, 0.2083, 0.1723,  ..., 0.2139, 0.1610, 0.1715],\n",
            "           [0.0857, 0.0743, 0.0864,  ..., 0.0882, 0.0543, 0.1007]],\n",
            "\n",
            "          [[0.1676, 0.2722, 0.2292,  ..., 0.2651, 0.2034, 0.2336],\n",
            "           [0.1175, 0.2760, 0.1870,  ..., 0.1151, 0.1527, 0.1578],\n",
            "           [0.1466, 0.1632, 0.1506,  ..., 0.2052, 0.3241, 0.1612],\n",
            "           ...,\n",
            "           [0.1383, 0.1818, 0.1365,  ..., 0.2407, 0.2088, 0.1495],\n",
            "           [0.1849, 0.2170, 0.1742,  ..., 0.2135, 0.1832, 0.1836],\n",
            "           [0.0701, 0.0919, 0.1212,  ..., 0.1086, 0.0922, 0.1448]],\n",
            "\n",
            "          [[0.0905, 0.1625, 0.1687,  ..., 0.1400, 0.1725, 0.1684],\n",
            "           [0.1314, 0.2055, 0.1639,  ..., 0.1991, 0.1710, 0.1435],\n",
            "           [0.1019, 0.1560, 0.2004,  ..., 0.0683, 0.1431, 0.1635],\n",
            "           ...,\n",
            "           [0.1248, 0.1698, 0.1323,  ..., 0.1052, 0.2032, 0.1454],\n",
            "           [0.1076, 0.2263, 0.1653,  ..., 0.1882, 0.2548, 0.1342],\n",
            "           [0.1518, 0.1042, 0.1151,  ..., 0.0942, 0.1054, 0.0857]]],\n",
            "\n",
            "\n",
            "         [[[0.4720, 0.4020, 0.3691,  ..., 0.4035, 0.4539, 0.3105],\n",
            "           [0.5730, 0.3749, 0.3650,  ..., 0.3748, 0.5444, 0.4449],\n",
            "           [0.6749, 0.3875, 0.2681,  ..., 0.5586, 0.3257, 0.3262],\n",
            "           ...,\n",
            "           [0.4867, 0.4107, 0.3716,  ..., 0.5237, 0.4387, 0.5570],\n",
            "           [0.6080, 0.4994, 0.4530,  ..., 0.4048, 0.5032, 0.4022],\n",
            "           [0.7091, 0.7216, 0.6007,  ..., 0.6072, 0.6078, 0.4499]],\n",
            "\n",
            "          [[0.4567, 0.2377, 0.4537,  ..., 0.3365, 0.3869, 0.2439],\n",
            "           [0.5541, 0.3941, 0.2290,  ..., 0.2674, 0.3440, 0.5513],\n",
            "           [0.6550, 0.3001, 0.3865,  ..., 0.3454, 0.1570, 0.3097],\n",
            "           ...,\n",
            "           [0.6710, 0.3843, 0.2764,  ..., 0.2875, 0.4649, 0.3023],\n",
            "           [0.6595, 0.4459, 0.3354,  ..., 0.4772, 0.4403, 0.3631],\n",
            "           [0.6245, 0.4999, 0.5021,  ..., 0.4723, 0.5028, 0.3372]],\n",
            "\n",
            "          [[0.4446, 0.3215, 0.3718,  ..., 0.3788, 0.3130, 0.3132],\n",
            "           [0.5295, 0.3083, 0.3826,  ..., 0.2389, 0.3606, 0.4116],\n",
            "           [0.6326, 0.4437, 0.3369,  ..., 0.4494, 0.3144, 0.2984],\n",
            "           ...,\n",
            "           [0.4671, 0.4782, 0.3015,  ..., 0.4596, 0.6067, 0.3282],\n",
            "           [0.7023, 0.2556, 0.3677,  ..., 0.3519, 0.5042, 0.3445],\n",
            "           [0.5721, 0.5193, 0.4733,  ..., 0.4841, 0.5618, 0.4117]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.4942, 0.4013, 0.4222,  ..., 0.3342, 0.2592, 0.2622],\n",
            "           [0.5159, 0.4109, 0.3794,  ..., 0.3062, 0.3458, 0.3795],\n",
            "           [0.6611, 0.5413, 0.3358,  ..., 0.3721, 0.4511, 0.3057],\n",
            "           ...,\n",
            "           [0.4823, 0.4984, 0.3738,  ..., 0.2847, 0.2625, 0.2019],\n",
            "           [0.6432, 0.4212, 0.3628,  ..., 0.2934, 0.2833, 0.4142],\n",
            "           [0.7034, 0.7488, 0.6867,  ..., 0.5666, 0.7751, 0.5780]],\n",
            "\n",
            "          [[0.4818, 0.3277, 0.3404,  ..., 0.2100, 0.3007, 0.2770],\n",
            "           [0.5412, 0.2478, 0.4994,  ..., 0.5199, 0.2723, 0.2970],\n",
            "           [0.4942, 0.4634, 0.5814,  ..., 0.3425, 0.2361, 0.2269],\n",
            "           ...,\n",
            "           [0.5563, 0.3785, 0.5810,  ..., 0.4463, 0.2718, 0.3606],\n",
            "           [0.4171, 0.3746, 0.3880,  ..., 0.3958, 0.2450, 0.3762],\n",
            "           [0.6974, 0.6448, 0.6589,  ..., 0.6491, 0.7212, 0.5439]],\n",
            "\n",
            "          [[0.5316, 0.3696, 0.4420,  ..., 0.3958, 0.3521, 0.2728],\n",
            "           [0.5628, 0.3688, 0.3768,  ..., 0.4805, 0.4335, 0.3961],\n",
            "           [0.5998, 0.4709, 0.4595,  ..., 0.6426, 0.2559, 0.3673],\n",
            "           ...,\n",
            "           [0.5161, 0.3359, 0.3626,  ..., 0.4598, 0.3404, 0.3336],\n",
            "           [0.5784, 0.3439, 0.4205,  ..., 0.3233, 0.2790, 0.5732],\n",
            "           [0.5013, 0.5179, 0.5505,  ..., 0.6313, 0.5978, 0.5674]]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pouMPGESf3DW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_images = []\n",
        "for i in range(2):\n",
        "  newx = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_training_images.append(newx)"
      ],
      "metadata": {
        "id": "0ohSRRMqgJPg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_training_images))\n",
        "print(randomized_training_images[0].shape)\n",
        "# print(randomized_training_images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEM8Z-YVgK4L",
        "outputId": "4169206d-4e25-483d-a073-0792c628d304"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 3, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_training_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "je2iTekggR-n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_training_segmentations))\n",
        "print(randomized_training_segmentations[0].shape)\n",
        "# print(randomized_training_segmentations[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQI7ldbUgTss",
        "outputId": "1e4fd8de-f022-44f1-fc68-162dd209d7b9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 3, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_data = list(zip(randomized_training_images, randomized_training_segmentations))"
      ],
      "metadata": {
        "id": "0Id-C28UgVDx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(dataset=randomized_training_data, batch_size=2, shuffle=True) # batch size should be 2"
      ],
      "metadata": {
        "id": "0kXxYbR2gWRx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1mBIdOfgXms",
        "outputId": "a3e61121-aa24-411c-a900-f6fa708d908d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_images = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_validation_images.append(newy)"
      ],
      "metadata": {
        "id": "jH0ms_qigaKE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_validation_images))\n",
        "print(randomized_validation_images[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDOlu_IigcUt",
        "outputId": "c51d0848-60ce-4d03-8216-60e2d1aa5c4a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 3, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_validation_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "S9ruiz4Tgevx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_validation_segmentations))\n",
        "print(randomized_validation_segmentations[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOYt_EuigfYa",
        "outputId": "8e303cce-b6ea-4311-9736-192d9a1d24a3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 3, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_data = list(zip(randomized_validation_images, randomized_validation_segmentations))"
      ],
      "metadata": {
        "id": "zFPZkDakgz_p"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validationloader = torch.utils.data.DataLoader(dataset=randomized_validation_data, batch_size=2, shuffle=True) # batch size should be 2"
      ],
      "metadata": {
        "id": "6Wb-FS1Lg1jM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(validationloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fchjms5Gg2au",
        "outputId": "22b43950-e6af-4ea4-d0a3-431e9c2da8c7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_images = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_testing_images.append(newy)"
      ],
      "metadata": {
        "id": "527yVWmyggOd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_testing_images))\n",
        "print(randomized_testing_images[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDHqy02vhBWt",
        "outputId": "f8aedd3f-5ef5-494b-aab5-2ed72ad7dd90"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 3, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_testing_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "y4Zu_IyFhC92"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_testing_segmentations))\n",
        "print(randomized_testing_segmentations[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1SbByMohEmA",
        "outputId": "68a7f016-5c77-4990-c456-aebee822421a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 3, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_data = list(zip(randomized_testing_images, randomized_testing_segmentations))"
      ],
      "metadata": {
        "id": "O0KeJ9qyhSIB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testingloader = torch.utils.data.DataLoader(dataset=randomized_testing_data, batch_size=2, shuffle=True) # batch size should be 2"
      ],
      "metadata": {
        "id": "Y-6FP0OmhTWX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training\n",
        "The sum of cross-entropy and dice loss \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9Rja_lBDtooP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "60e42e1c-96c4-4601-cf8a-afbb5143a362"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTraining\\nThe sum of cross-entropy and dice loss \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chose to make this a class because when you call dice loss in criterion, you don't have anything to input, but when u run the prediction through inside the training, then you have params\n",
        "# also because most sources I saw used a class\n",
        "class DiceLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self, true, pred):\n",
        "    # flatten to easily do it pixel by pixel\n",
        "    true = true.view(-1)\n",
        "    pred = pred.view(-1)\n",
        "    numerator = 2*(true*pred).sum()\n",
        "    denominator = true.sum() + pred.sum()\n",
        "    dice_loss = 1 - (numerator) / (denominator)\n",
        "    return dice_loss"
      ],
      "metadata": {
        "id": "b8VIZVcJhTYT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim"
      ],
      "metadata": {
        "id": "6JfmRbwXiE-l"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# btw im not doing patches\n",
        "# epochs\n",
        "epochs = 2 # should be 1000\n",
        "# loss\n",
        "criterion1 = DiceLoss()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01, momentum= 0.99)"
      ],
      "metadata": {
        "id": "kay03ku8iGA0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  training_loss = 0\n",
        "  validation_loss = 0\n",
        "  print(\"training time\")\n",
        "  for images, segs in trainloader:\n",
        "    optimizer.zero_grad()\n",
        "    print(len(images), len(segs))\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "    # segs = segs.long() - no\n",
        "    segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    outputs, softmax_outputs = model(images)\n",
        "    print(outputs.shape)\n",
        "    print(softmax_outputs.shape)\n",
        "\n",
        "    # arg_outputs = outputs.argmax(dim=1)\n",
        "    # print(arg_outputs.shape)\n",
        "    # print(arg_outputs)\n",
        "    # print(segs.shape)\n",
        "    print()\n",
        "    diceloss = criterion1(softmax_outputs.float(), segs)\n",
        "    celoss = criterion2(outputs.float(), segs)\n",
        "    loss = diceloss + celoss\n",
        "    print(loss) # loss with random tensors will be really high because none of the tensors are related to each other\n",
        "    # loss can be > 1 - https://ai.stackexchange.com/questions/24685/can-the-sparse-categorical-cross-entropy-be-greater-than-one, https://stats.stackexchange.com/questions/392681/cross-entropy-loss-max-value\n",
        "    loss.backward()\n",
        "    training_loss += loss.item()\n",
        "    print()\n",
        "  print(\"validation time\")\n",
        "  for images, segs in validationloader:\n",
        "    optimizer.zero_grad()\n",
        "    print(len(images), len(segs))\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "    # segs = segs.long() - no\n",
        "    segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    outputs, softmax_outputs = model(images)\n",
        "    print(outputs.shape)\n",
        "    print(softmax_outputs.shape)\n",
        "    diceloss = criterion1(softmax_outputs.float(), segs)\n",
        "    celoss = criterion2(outputs.float(), segs)\n",
        "    loss = diceloss + celoss\n",
        "    print(loss) # loss with random tensors will be really high because none of the tensors are related to each other\n",
        "    # loss can be > 1 - https://ai.stackexchange.com/questions/24685/can-the-sparse-categorical-cross-entropy-be-greater-than-one, https://stats.stackexchange.com/questions/392681/cross-entropy-loss-max-value\n",
        "    loss.backward()\n",
        "    validation_loss += loss.item()\n",
        "  training_losses.append(training_loss/len(trainloader))\n",
        "  validation_losses.append(validation_loss/len(validationloader))\n",
        "  print(\"Epoch: {}/{}... Training Loss: {}... Validation Loss: {}...\".format(i+1,epochs, training_losses[-1], validation_losses[-1]))\n",
        "  if validation_loss < min(validation_losses):\n",
        "    print(\"Validation loss has decreased...saving model\")\n",
        "    torch.save(model.state_dict(), \"fcn.pth\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqJhvR3HpfQC",
        "outputId": "0f37bf7d-3021-4dc6-f457-fb5221ec6c21"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.4065, grad_fn=<AddBackward0>)\n",
            "\n",
            "validation time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "tensor(2.4067, grad_fn=<AddBackward0>)\n",
            "Epoch: 1/2... Training Loss: 2.4065001010894775... Validation Loss: 2.4067230224609375...\n",
            "\n",
            "training time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "\n",
            "tensor(2.4065, grad_fn=<AddBackward0>)\n",
            "\n",
            "validation time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "tensor(2.4067, grad_fn=<AddBackward0>)\n",
            "Epoch: 2/2... Training Loss: 2.4065001010894775... Validation Loss: 2.4067230224609375...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_losses)\n",
        "print(validation_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de7ESYI8rt6b",
        "outputId": "1aa12701-9105-462c-fd36-a23b73b8e6c6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.4065001010894775, 2.4065001010894775]\n",
            "[2.4067230224609375, 2.4067230224609375]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_score(outputs, segmentations): # they find individual\n",
        "  # outputs = torch.Size([2, 3, 128, 128, 128])\n",
        "  # segmentations = torch.Size([2, 3, 128, 128, 128])\n",
        "  # print(output.shape)\n",
        "  # print(segmentations.shape)\n",
        "  n_classes = segmentations.shape[1]\n",
        "  region_scores = []\n",
        "  for i in range(n_classes):\n",
        "    outputs = outputs.view(-1)\n",
        "    segmentations = segmentations.view(-1)\n",
        "    numerator = 2*(outputs*segmentations).sum()\n",
        "    denominator = outputs.sum() + segmentations.sum()\n",
        "    dice = (numerator) / (denominator)\n",
        "    region_scores.append(dice)\n",
        "  return region_scores"
      ],
      "metadata": {
        "id": "W37y7di3oqmK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1 NECROTIC TUMOUR CORE (NCR — label 1) - index 0\n",
        "\n",
        "2 GD-ENHANCING TUMOUR (ET — label 2) - index 1\n",
        "\n",
        "3 PERITUMORAL EDEMATOUS/INVADED TISSUE (ED — label 3) - index 2\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "usrtV9LTRQ_U",
        "outputId": "1281c07f-7071-4104-dbdd-4103bb61a052"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1 NECROTIC TUMOUR CORE (NCR — label 1) - index 0\\n\\n2 GD-ENHANCING TUMOUR (ET — label 2) - index 1\\n\\n3 PERITUMORAL EDEMATOUS/INVADED TISSUE (ED — label 3) - index 2\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, segs in testingloader:\n",
        "    with torch.no_grad():\n",
        "      print(len(images), len(segs))\n",
        "      print(images.shape)\n",
        "      print(segs.shape)\n",
        "      images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "      # segs = segs.long() - no\n",
        "      segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "      print(images.shape)\n",
        "      print(segs.shape)\n",
        "      outputs, softmax_outputs = model(images)\n",
        "      print(outputs.shape)\n",
        "      print(softmax_outputs.shape)\n",
        "      print()\n",
        "\n",
        "      print(\".......\"*5)\n",
        "\n",
        "      region_scores = dice_score(softmax_outputs, segs)\n",
        "\n",
        "      print(len(region_scores))\n",
        "      print(region_scores)\n",
        "\n",
        "      print(\"1 NECROTIC TUMOUR CORE (NCR — label 1)\")\n",
        "      print(region_scores[0].item())\n",
        "      print()\n",
        "\n",
        "      print(\"2 GD-ENHANCING TUMOUR (ET — label 2)\")\n",
        "      print(region_scores[1].item())\n",
        "      print()\n",
        "\n",
        "      print(\"3 PERITUMORAL EDEMATOUS/INVADED TISSUE (ED — label 3)\")\n",
        "      print(region_scores[2].item())"
      ],
      "metadata": {
        "id": "L_c-dbmIocIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1628a9ec-183f-469d-e88e-ba42452e8a2e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "\n",
            "...................................\n",
            "3\n",
            "[tensor(0.4000), tensor(0.4000), tensor(0.4000)]\n",
            "1 NECROTIC TUMOUR CORE (NCR — label 1)\n",
            "0.4000225067138672\n",
            "\n",
            "2 GD-ENHANCING TUMOUR (ET — label 2)\n",
            "0.4000225067138672\n",
            "\n",
            "3 PERITUMORAL EDEMATOUS/INVADED TISSUE (ED — label 3)\n",
            "0.4000225067138672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    }
  ]
}