{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P_MbWzdtmbBU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import conv\n",
        "class Layer1(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    # nn.Sequential doesn't work\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2) # concatenate this before 2nd ReLU in res block\n",
        "    # print(x3.shape)\n",
        "\n",
        "    # block 2\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) # concatenate this before 2nd ReLU in res block\n",
        "    # print(x8.shape)\n",
        "\n",
        "    # element wise add\n",
        "    x9 = torch.add(x3, x8)\n",
        "\n",
        "    # relu\n",
        "    x10 = self.relu(x9)\n",
        "    return x10"
      ],
      "metadata": {
        "id": "7kCncP7WoiQN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer2(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) # concat\n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "    return x10"
      ],
      "metadata": {
        "id": "uXDYAroiRJ8z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer3(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) # concat\n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14) # concat\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "    return x17"
      ],
      "metadata": {
        "id": "cjt8QRpBO80p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer4(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) \n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14)\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "\n",
        "    # block 2 x3\n",
        "    x18 = self.conv(x17)\n",
        "    x19 = self.instnorm(x18)\n",
        "    x20 = self.relu(x19)\n",
        "    x21 = self.conv(x20)\n",
        "    x22 = self.instnorm(x21)\n",
        "    x23 = torch.add(x17, x22)\n",
        "    x24 = self.relu(x23)\n",
        "    return x24"
      ],
      "metadata": {
        "id": "wyTA3AkiPopy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer5(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) \n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14)\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "\n",
        "    # block 2 x3\n",
        "    x18 = self.conv(x17)\n",
        "    x19 = self.instnorm(x18)\n",
        "    x20 = self.relu(x19)\n",
        "    x21 = self.conv(x20)\n",
        "    x22 = self.instnorm(x21)\n",
        "    x23 = torch.add(x17, x22)\n",
        "    x24 = self.relu(x23)\n",
        "\n",
        "    # block 2 x4\n",
        "    x25 = self.conv(x24)\n",
        "    x26 = self.instnorm(x25)\n",
        "    x27 = self.relu(x26)\n",
        "    x28 = self.conv(x27)\n",
        "    x29 = self.instnorm(x28)\n",
        "    x30 = torch.add(x24, x29)\n",
        "    x31 = self.relu(x30)\n",
        "    return x31"
      ],
      "metadata": {
        "id": "B0BUmRhkRshf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer6(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) \n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14)\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "\n",
        "    # block 2 x3\n",
        "    x18 = self.conv(x17)\n",
        "    x19 = self.instnorm(x18)\n",
        "    x20 = self.relu(x19)\n",
        "    x21 = self.conv(x20)\n",
        "    x22 = self.instnorm(x21)\n",
        "    x23 = torch.add(x17, x22)\n",
        "    x24 = self.relu(x23)\n",
        "\n",
        "    # block 2 x4\n",
        "    x25 = self.conv(x24)\n",
        "    x26 = self.instnorm(x25)\n",
        "    x27 = self.relu(x26)\n",
        "    x28 = self.conv(x27)\n",
        "    x29 = self.instnorm(x28)\n",
        "    x30 = torch.add(x24, x29)\n",
        "    x31 = self.relu(x30)\n",
        "\n",
        "    # block 2 x5\n",
        "    x32 = self.conv(x31)\n",
        "    x33 = self.instnorm(x32)\n",
        "    x34 = self.relu(x33)\n",
        "    x35 = self.conv(x34)\n",
        "    x36 = self.instnorm(x35)\n",
        "    x37 = torch.add(x31, x36)\n",
        "    x38 = self.relu(x37)\n",
        "    return x38"
      ],
      "metadata": {
        "id": "B8f_Cz09Sxdg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlockDecoder(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.transcov = nn.ConvTranspose3d(in_channels=input_channels, out_channels=input_channels, kernel_size=2, stride=2)\n",
        "    self.conv = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x, concat_tensor):\n",
        "    x1 = self.transcov(x)\n",
        "    x2 = torch.add(x1, concat_tensor)\n",
        "    x3 = self.conv(x2)\n",
        "    x4 = self.instnorm(x3)\n",
        "    x5 = self.relu(x4)\n",
        "    return x5"
      ],
      "metadata": {
        "id": "6njHhcTiT8kk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualUNet(nn.Module):\n",
        "  def __init__(self, input_shape):\n",
        "    super().__init__()\n",
        "    self.layer1 = Layer1(4, 24)\n",
        "    self.layer2 = Layer2(24, 48)\n",
        "    self.layer3 = Layer3(48, 96)\n",
        "    self.layer4 = Layer4(96, 192)\n",
        "    self.layer5 = Layer5(192, 320)\n",
        "    self.layer6 = Layer6(320, 320)\n",
        "    self.layer7 = ConvBlockDecoder(320, 192)\n",
        "    self.layer8 = ConvBlockDecoder(192, 96)\n",
        "    self.layer9 = ConvBlockDecoder(96, 48)\n",
        "    self.layer10 = ConvBlockDecoder(48, 24)\n",
        "    self.layer11 = ConvBlockDecoder(24, 24)\n",
        "    self.conv1x1x1 = nn.Conv3d(in_channels=24, out_channels=3, kernel_size=1, stride=1)\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    print(x.shape) # [1, 4, 128, 128, 128]\n",
        "    x1 = self.layer1(x)\n",
        "    print(x1.shape) # [1, 24, 128, 128, 128]\n",
        "\n",
        "    x2 = self.layer2(x1)\n",
        "    print(x2.shape) # [1, 48, 64, 64, 64]\n",
        "\n",
        "    x3 = self.layer3(x2)\n",
        "    print(x3.shape) # [1, 96, 32, 32, 32]\n",
        "\n",
        "    x4 = self.layer4(x3)\n",
        "    print(x4.shape) # [1, 192, 16, 16, 16])\n",
        "\n",
        "    x5 = self.layer5(x4)\n",
        "    print(x5.shape) # [1, 320, 8, 8, 8]\n",
        "\n",
        "    x6 = self.layer6(x5)\n",
        "    print(x6.shape) # [1, 320, 4, 4, 4]\n",
        "\n",
        "    x7 = self.layer7(x6, x5)\n",
        "    print(x7.shape) # [1, 320, 4, 4, 4]\n",
        "\n",
        "    x8 = self.layer8(x7, x4)\n",
        "    print(x8.shape) # [1, 192, 8, 8, 8]\n",
        "\n",
        "    x9 = self.layer9(x8, x3)\n",
        "    print(x9.shape) # [1, 96, 16, 16, 16]\n",
        "\n",
        "    x10 = self.layer10(x9, x2)\n",
        "    print(x10.shape) # [1, 48, 32, 32, 32]\n",
        "\n",
        "    x11 = self.layer11(x10, x1)\n",
        "    print(x11.shape) # [1, 24, 64, 64, 64]\n",
        "\n",
        "    x12 = self.conv1x1x1(x11)\n",
        "    print(x12.shape) # [1, 24, 128, 128, 128]\n",
        "\n",
        "    prob = self.softmax(x12) # [1, 3, 128, 128, 128]\n",
        "\n",
        "    return x12, prob"
      ],
      "metadata": {
        "id": "tJpWDC3oRgAU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(size=(2, 4, 128, 128, 128), dtype=torch.float32)\n",
        "# print(x.shape)\n",
        "\n",
        "model = ResidualUNet(x.shape)\n",
        "print(model)\n",
        "print()\n",
        "\n",
        "out = model(x)\n",
        "# print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7zsz3sFtFMZ",
        "outputId": "c7fb1ed2-1248-4405-dace-815992acf91a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResidualUNet(\n",
            "  (layer1): Layer1(\n",
            "    (conv1): Conv3d(4, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer2): Layer2(\n",
            "    (conv1): Conv3d(24, 48, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer3): Layer3(\n",
            "    (conv1): Conv3d(48, 96, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer4): Layer4(\n",
            "    (conv1): Conv3d(96, 192, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer5): Layer5(\n",
            "    (conv1): Conv3d(192, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer6): Layer6(\n",
            "    (conv1): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer7): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(320, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer8): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(192, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer9): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(96, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer10): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(48, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(48, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer11): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(24, 24, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (conv1x1x1): Conv3d(24, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output, probability = out"
      ],
      "metadata": {
        "id": "6uIe4jxJd_40"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spnR9HzFelYy",
        "outputId": "d0980d36-185d-4cf2-8ba4-e114d7bf22b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 128, 128, 128])\n",
            "tensor([[[[[ 3.8036e-01,  5.6360e-01,  5.2399e-01,  ...,  7.9193e-02,\n",
            "             4.3436e-01,  5.9636e-01],\n",
            "           [ 6.1829e-01,  1.7708e-01,  2.6073e-01,  ...,  6.4192e-01,\n",
            "             3.1353e-01,  3.8916e-01],\n",
            "           [ 3.6692e-01,  2.4827e-02,  1.1387e-01,  ...,  3.8452e-01,\n",
            "            -9.6049e-03,  2.3506e-01],\n",
            "           ...,\n",
            "           [ 6.5502e-01,  2.6126e-01,  4.3063e-01,  ...,  2.4737e-01,\n",
            "             2.4685e-01,  1.8544e-01],\n",
            "           [ 3.4946e-01,  3.0761e-01,  5.3952e-01,  ...,  2.6600e-01,\n",
            "             3.2800e-01,  4.1759e-01],\n",
            "           [ 3.7658e-01,  2.2018e-01,  5.1320e-01,  ...,  3.6918e-01,\n",
            "             2.5048e-01,  1.8049e-01]],\n",
            "\n",
            "          [[ 1.9677e-01,  5.5467e-01, -4.0175e-01,  ...,  6.6503e-01,\n",
            "             3.7104e-01,  2.3171e-01],\n",
            "           [ 6.9093e-01,  3.3562e-01,  2.7548e-01,  ...,  5.6249e-01,\n",
            "            -7.4299e-02,  1.7369e-01],\n",
            "           [ 7.5971e-01, -7.9540e-01,  3.6294e-01,  ...,  2.9384e-01,\n",
            "             2.5939e-01,  1.2971e-01],\n",
            "           ...,\n",
            "           [ 2.3341e-01,  1.5628e-01, -1.3145e-01,  ...,  8.0504e-01,\n",
            "             1.0280e-01,  3.6278e-01],\n",
            "           [ 4.8765e-01,  5.5000e-01, -1.1032e-01,  ...,  8.0623e-01,\n",
            "             8.2001e-02,  5.0627e-01],\n",
            "           [ 5.3280e-01,  6.3797e-01,  5.4172e-01,  ...,  7.8467e-01,\n",
            "             2.6134e-01,  5.7459e-01]],\n",
            "\n",
            "          [[ 3.5700e-01,  1.9157e-02, -3.8813e-03,  ...,  6.1500e-01,\n",
            "             3.2118e-01,  3.7628e-01],\n",
            "           [ 2.9636e-01,  7.3532e-02,  5.3806e-01,  ...,  6.2428e-02,\n",
            "            -2.2022e-01,  1.3192e-01],\n",
            "           [ 7.4115e-01, -7.0650e-02,  9.1552e-02,  ...,  4.7545e-01,\n",
            "             4.7858e-01,  3.7374e-01],\n",
            "           ...,\n",
            "           [ 3.5950e-01,  3.6996e-01,  1.0385e-01,  ..., -9.3456e-02,\n",
            "            -2.3581e-01,  2.0221e-01],\n",
            "           [ 6.5587e-01, -1.1685e-01, -5.3261e-01,  ...,  8.8642e-02,\n",
            "             3.2369e-01, -1.6675e-01],\n",
            "           [ 2.0212e-01,  9.6011e-01,  3.9028e-01,  ..., -9.7708e-02,\n",
            "            -2.5941e-02,  4.2151e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 8.5023e-01,  8.7518e-02,  5.1977e-02,  ...,  4.1316e-01,\n",
            "             5.5362e-01,  3.6515e-01],\n",
            "           [ 4.6177e-01,  6.9775e-02, -1.6554e-01,  ...,  2.0915e-01,\n",
            "            -5.1286e-02,  3.5125e-01],\n",
            "           [ 5.8549e-01, -2.8386e-01,  8.0749e-01,  ..., -2.5647e-01,\n",
            "             5.0330e-02,  1.9368e-01],\n",
            "           ...,\n",
            "           [ 5.6026e-01, -8.2942e-03, -2.5040e-01,  ..., -1.5577e-01,\n",
            "             3.4740e-01, -9.3776e-02],\n",
            "           [ 7.3529e-01, -2.2599e-01,  2.6266e-01,  ..., -3.0166e-01,\n",
            "             5.0201e-01,  1.6357e-01],\n",
            "           [ 4.3486e-01,  6.1316e-01, -4.2055e-01,  ...,  3.3625e-01,\n",
            "            -2.8617e-01,  1.5645e-01]],\n",
            "\n",
            "          [[ 3.8037e-01, -7.8763e-02, -1.7627e-01,  ..., -2.4153e-02,\n",
            "             1.4263e-01,  1.8845e-01],\n",
            "           [-2.2318e-03, -1.3982e-02, -5.0571e-01,  ..., -3.0590e-01,\n",
            "             2.9231e-01,  2.3245e-01],\n",
            "           [ 9.0764e-02, -1.0675e-01, -2.6877e-02,  ...,  2.3191e-01,\n",
            "            -6.3499e-01,  6.4408e-01],\n",
            "           ...,\n",
            "           [ 3.5549e-01, -4.7035e-01,  1.1781e-02,  ...,  5.2164e-02,\n",
            "            -3.3724e-01,  3.4399e-01],\n",
            "           [ 1.3370e-01, -2.4147e-01, -4.2926e-01,  ...,  2.7594e-01,\n",
            "             1.2581e-01,  3.3480e-01],\n",
            "           [ 1.0018e-01,  4.9590e-01, -1.7285e-01,  ...,  2.0459e-01,\n",
            "             2.8986e-01,  6.1281e-01]],\n",
            "\n",
            "          [[ 3.8874e-01,  2.3591e-01,  1.2100e-01,  ...,  2.2992e-01,\n",
            "             4.9511e-01,  5.5853e-01],\n",
            "           [ 5.7434e-01,  3.7213e-01,  2.2733e-01,  ...,  2.6688e-01,\n",
            "             1.7646e-01,  2.9832e-01],\n",
            "           [ 4.5749e-01,  2.9256e-01, -7.9722e-02,  ..., -6.0178e-02,\n",
            "             1.3797e-01,  1.2578e-01],\n",
            "           ...,\n",
            "           [ 3.0147e-01,  8.3402e-01,  4.1623e-01,  ...,  1.7072e-01,\n",
            "             2.5726e-01,  2.1449e-01],\n",
            "           [ 3.7956e-01, -7.2430e-02,  6.4173e-01,  ...,  5.3633e-01,\n",
            "             9.3887e-02,  4.8360e-01],\n",
            "           [ 1.7872e-01, -1.0221e-02,  5.6454e-01,  ...,  3.9580e-01,\n",
            "             2.4945e-01,  2.5264e-01]]],\n",
            "\n",
            "\n",
            "         [[[-1.8014e-01, -1.9226e-01, -1.3593e-01,  ..., -2.5435e-01,\n",
            "            -3.7998e-01,  2.4851e-01],\n",
            "           [ 3.0731e-02, -1.8932e-01, -7.2063e-02,  ..., -3.5804e-02,\n",
            "             5.3654e-01,  1.7803e-01],\n",
            "           [-5.0234e-01, -4.4195e-01, -9.5817e-02,  ..., -3.7188e-01,\n",
            "            -3.9076e-01,  4.5003e-02],\n",
            "           ...,\n",
            "           [-7.3889e-02,  5.2035e-01,  1.5305e-01,  ..., -1.6929e-01,\n",
            "            -8.1928e-03,  1.4082e-01],\n",
            "           [-4.7802e-02, -1.0670e+00, -5.1025e-01,  ..., -1.5898e-01,\n",
            "            -6.2257e-01, -1.7205e-01],\n",
            "           [-2.4963e-01,  3.3930e-02, -1.8619e-01,  ..., -4.7046e-01,\n",
            "            -3.4036e-01,  3.0750e-02]],\n",
            "\n",
            "          [[ 2.5106e-01,  1.4047e-01,  1.3559e-01,  ...,  2.6459e-01,\n",
            "             2.7963e-01,  2.6133e-01],\n",
            "           [ 7.3631e-01,  8.2188e-01,  2.0011e-01,  ...,  3.6013e-01,\n",
            "             6.2183e-01,  2.2538e-01],\n",
            "           [ 5.3494e-01,  8.1300e-01,  3.5241e-01,  ...,  6.6812e-01,\n",
            "             4.8632e-01,  2.7512e-01],\n",
            "           ...,\n",
            "           [ 2.3104e-01,  1.8479e-01,  1.8493e-01,  ...,  1.3169e-01,\n",
            "             4.0527e-01,  1.4979e-01],\n",
            "           [ 5.4465e-02,  4.1906e-01,  2.3612e-01,  ...,  3.0861e-01,\n",
            "             4.5100e-01,  2.8316e-01],\n",
            "           [-1.8642e-01,  2.6187e-01, -2.4188e-01,  ..., -2.0807e-01,\n",
            "             5.0716e-01, -1.8580e-01]],\n",
            "\n",
            "          [[ 2.6419e-01,  5.5237e-01, -2.5248e-01,  ...,  2.2064e-01,\n",
            "            -8.2101e-02, -1.1242e-01],\n",
            "           [ 7.0643e-01,  6.8944e-01,  2.0759e-01,  ...,  3.2575e-01,\n",
            "             2.6109e-01,  6.1097e-01],\n",
            "           [ 3.6711e-01,  7.6175e-01,  3.1393e-01,  ...,  2.4650e-01,\n",
            "            -1.1611e-01,  1.4181e-02],\n",
            "           ...,\n",
            "           [ 7.1705e-01,  1.2005e+00, -2.1188e-01,  ..., -4.0742e-02,\n",
            "             7.5152e-01,  8.7863e-01],\n",
            "           [ 8.9887e-01,  5.8290e-02, -7.7834e-01,  ...,  2.5403e-01,\n",
            "             4.2773e-01, -2.6825e-01],\n",
            "           [-3.8544e-01, -2.9563e-01, -1.1213e-01,  ..., -5.9483e-02,\n",
            "            -4.7810e-02, -5.0195e-02]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 2.9743e-01,  3.4990e-01,  4.8947e-01,  ...,  8.5178e-01,\n",
            "             4.9046e-01,  3.7784e-01],\n",
            "           [ 7.4112e-01,  6.7530e-01,  1.0414e+00,  ..., -2.4174e-01,\n",
            "             4.6035e-01,  2.1392e-01],\n",
            "           [ 1.1997e+00,  7.1443e-01,  3.7520e-01,  ..., -1.0238e-01,\n",
            "             8.4386e-01,  4.9282e-01],\n",
            "           ...,\n",
            "           [ 1.3256e+00,  5.4249e-01,  3.1606e-01,  ...,  6.3055e-01,\n",
            "            -1.8139e-01,  3.1533e-01],\n",
            "           [ 5.9388e-01,  4.9280e-01,  2.7532e-01,  ...,  2.8880e-01,\n",
            "             5.6025e-01,  4.0184e-01],\n",
            "           [-4.3131e-01, -3.7084e-01,  2.7614e-01,  ..., -7.2855e-02,\n",
            "            -7.3153e-01,  1.5998e-02]],\n",
            "\n",
            "          [[ 6.3663e-01, -6.2686e-02, -1.7314e-01,  ...,  4.5230e-01,\n",
            "             2.0483e-01, -3.0558e-01],\n",
            "           [ 2.7569e-01,  3.2094e-01, -1.0621e-02,  ...,  2.7126e-01,\n",
            "             1.2890e-01,  1.7663e-01],\n",
            "           [ 4.0739e-01,  6.5064e-01,  5.5603e-01,  ...,  2.7731e-01,\n",
            "             7.0761e-01,  6.3702e-02],\n",
            "           ...,\n",
            "           [ 2.7491e-01,  2.7311e-01, -1.0548e-01,  ...,  1.1727e-01,\n",
            "             8.7088e-02, -2.5267e-01],\n",
            "           [ 9.8796e-02,  2.4209e-01, -7.1056e-02,  ..., -2.3068e-01,\n",
            "             9.4353e-02,  3.8550e-02],\n",
            "           [-1.3434e-01, -2.8834e-01, -7.8233e-01,  ..., -2.7304e-01,\n",
            "            -3.5246e-01, -7.5707e-01]],\n",
            "\n",
            "          [[ 3.4891e-01,  7.1093e-01,  8.3238e-01,  ...,  1.0326e+00,\n",
            "             7.9211e-02,  5.0390e-01],\n",
            "           [ 8.5325e-01,  8.0213e-01,  9.0481e-01,  ...,  8.0485e-01,\n",
            "             1.4641e+00,  7.1297e-01],\n",
            "           [ 7.9055e-01,  5.0162e-01,  4.9271e-01,  ...,  1.4717e+00,\n",
            "             9.3365e-01,  5.1669e-02],\n",
            "           ...,\n",
            "           [ 8.5068e-01,  4.5873e-01,  4.5884e-01,  ...,  5.3785e-01,\n",
            "             5.9880e-01,  6.6809e-01],\n",
            "           [ 6.4628e-01,  7.5827e-01,  4.7898e-01,  ...,  2.6631e-01,\n",
            "             7.7161e-01,  8.9342e-01],\n",
            "           [ 2.7405e-01,  3.4415e-01,  1.9978e-01,  ...,  3.0173e-01,\n",
            "             2.3847e-01,  1.5709e-01]]],\n",
            "\n",
            "\n",
            "         [[[ 5.5804e-01, -4.2279e-02, -1.2838e-01,  ...,  2.0576e-01,\n",
            "            -2.9456e-02, -5.0384e-01],\n",
            "           [-1.2383e-01, -2.3606e-02, -2.3249e-01,  ..., -3.0574e-01,\n",
            "            -1.4109e-01, -3.6752e-01],\n",
            "           [ 1.8623e-02,  2.2800e-01, -3.9844e-02,  ..., -2.5027e-01,\n",
            "            -4.9304e-01, -3.6290e-01],\n",
            "           ...,\n",
            "           [ 1.7562e-01,  1.5090e-01, -2.4411e-01,  ..., -3.2264e-01,\n",
            "            -3.3623e-01, -8.6032e-02],\n",
            "           [-2.0087e-01, -7.9136e-01, -7.6839e-02,  ..., -7.3418e-01,\n",
            "            -3.9797e-04, -4.3036e-01],\n",
            "           [ 2.7364e-02, -1.8569e-01, -3.8365e-01,  ..., -1.8971e-01,\n",
            "            -1.8570e-01, -1.3002e-01]],\n",
            "\n",
            "          [[ 6.1825e-01,  1.4726e-01,  3.6002e-02,  ..., -9.6310e-02,\n",
            "             7.1447e-01,  3.4718e-01],\n",
            "           [ 5.1929e-01, -3.1834e-01, -2.4103e-01,  ..., -1.4548e-01,\n",
            "            -2.7323e-01,  4.8402e-01],\n",
            "           [ 2.9357e-01, -2.0229e-01,  5.0172e-01,  ...,  6.1924e-01,\n",
            "             4.5433e-01,  2.4657e-01],\n",
            "           ...,\n",
            "           [ 9.5269e-01,  3.3264e-01,  8.9340e-01,  ...,  1.8698e-01,\n",
            "             5.7707e-01,  1.8627e-01],\n",
            "           [ 6.4916e-01,  2.5514e-01,  3.6007e-03,  ...,  1.5187e-01,\n",
            "             1.0400e-01,  2.9176e-01],\n",
            "           [ 2.9853e-01,  2.4818e-01,  9.2658e-01,  ...,  3.1877e-01,\n",
            "             3.1120e-01, -4.8890e-02]],\n",
            "\n",
            "          [[ 3.5839e-01,  5.0259e-01, -2.8903e-01,  ..., -9.3643e-01,\n",
            "             3.3533e-01, -2.0112e-01],\n",
            "           [ 1.9287e-01, -5.0010e-01, -2.9252e-01,  ...,  8.3779e-02,\n",
            "            -4.3313e-01, -7.9565e-01],\n",
            "           [ 4.1312e-01, -1.6475e-01, -1.8590e-01,  ..., -6.0623e-01,\n",
            "            -1.3658e-01, -2.0247e-01],\n",
            "           ...,\n",
            "           [ 2.5250e-01, -5.0043e-01,  5.9312e-01,  ..., -1.9973e-01,\n",
            "             3.1424e-01, -2.4494e-01],\n",
            "           [ 5.4487e-01, -4.9928e-01,  3.3211e-01,  ..., -4.3403e-01,\n",
            "             6.1181e-03,  1.9297e-01],\n",
            "           [ 4.6917e-01, -3.8380e-01, -2.0373e-01,  ..., -7.4636e-02,\n",
            "             1.9853e-02, -1.5528e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 1.9401e-01,  6.8035e-02, -8.5367e-02,  ..., -3.2042e-01,\n",
            "            -1.2739e-01, -2.4676e-01],\n",
            "           [ 4.0360e-01, -4.0226e-01, -1.1638e-01,  ..., -6.0587e-01,\n",
            "            -1.2613e-01, -1.2342e-01],\n",
            "           [ 1.8482e-01, -2.6535e-01,  1.2132e-01,  ..., -3.2265e-02,\n",
            "             6.0154e-01, -3.4832e-01],\n",
            "           ...,\n",
            "           [ 2.4022e-01,  1.9060e-01,  5.1902e-02,  ..., -6.3849e-01,\n",
            "            -3.8711e-01, -6.0152e-02],\n",
            "           [ 5.8470e-01,  1.2228e-01, -6.7574e-02,  ..., -5.5381e-01,\n",
            "             2.5092e-01, -5.3161e-01],\n",
            "           [ 7.0108e-01, -3.8079e-03,  1.6730e-01,  ..., -4.5284e-01,\n",
            "             1.7527e-01, -3.5007e-01]],\n",
            "\n",
            "          [[-1.9042e-02,  2.2291e-01,  4.9048e-01,  ...,  6.4793e-01,\n",
            "             3.9031e-01,  3.8260e-02],\n",
            "           [ 1.8529e-01, -1.5512e-01, -8.9654e-02,  ..., -2.3684e-01,\n",
            "            -1.7348e-01, -4.3475e-01],\n",
            "           [ 2.9755e-01, -2.6529e-01, -6.9684e-01,  ...,  1.6186e-01,\n",
            "             1.7177e-01,  4.0921e-01],\n",
            "           ...,\n",
            "           [ 2.4872e-01, -5.5677e-01, -1.3587e-01,  ...,  6.3980e-02,\n",
            "            -2.8662e-01, -9.9657e-02],\n",
            "           [ 5.2482e-01,  1.8607e-01,  3.0274e-01,  ...,  4.1186e-01,\n",
            "            -3.2556e-01,  2.3029e-01],\n",
            "           [ 2.6434e-01, -1.2745e-01, -6.4667e-02,  ..., -2.4549e-01,\n",
            "             1.0105e-01, -2.7011e-01]],\n",
            "\n",
            "          [[ 3.8771e-02, -3.8537e-01, -2.5459e-01,  ...,  1.7879e-02,\n",
            "            -1.2875e-01, -1.8790e-01],\n",
            "           [-6.6072e-02, -4.0083e-01, -1.8994e-01,  ...,  1.0440e-01,\n",
            "             3.9588e-02, -2.2745e-01],\n",
            "           [-1.6048e-01, -3.9462e-01, -4.6547e-01,  ...,  1.6676e-01,\n",
            "             2.5448e-01,  1.2984e-01],\n",
            "           ...,\n",
            "           [ 1.1745e-01, -1.0631e-01,  2.8279e-01,  ..., -2.3852e-01,\n",
            "            -2.0388e-02, -2.0722e-01],\n",
            "           [ 1.0965e-01,  4.1273e-01,  1.1308e-01,  ...,  1.6636e-01,\n",
            "            -2.1555e-01, -4.3699e-01],\n",
            "           [ 3.5665e-03, -2.5260e-01,  1.7073e-02,  ..., -3.0828e-01,\n",
            "            -3.4208e-01, -5.7741e-01]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 3.0427e-01,  2.8352e-01,  2.1667e-01,  ...,  2.0214e-01,\n",
            "             5.4658e-01,  4.4911e-01],\n",
            "           [ 6.0963e-01,  1.1981e-01,  2.3085e-01,  ...,  9.6343e-01,\n",
            "             3.2595e-02,  1.0778e-01],\n",
            "           [ 6.9009e-01, -1.1314e-01,  6.7864e-01,  ...,  3.0367e-01,\n",
            "             5.1890e-01, -8.4411e-03],\n",
            "           ...,\n",
            "           [ 4.1028e-01,  3.9164e-01,  2.0344e-01,  ...,  3.8468e-01,\n",
            "             3.9023e-01,  2.9125e-01],\n",
            "           [ 6.7973e-01,  2.8138e-03,  5.9124e-01,  ...,  1.6299e-01,\n",
            "             4.1217e-01,  1.7762e-01],\n",
            "           [ 5.6328e-01,  1.2852e-01,  3.2277e-01,  ...,  2.2997e-01,\n",
            "             3.5251e-02,  3.0957e-01]],\n",
            "\n",
            "          [[ 3.9029e-01,  1.2543e-01,  3.4927e-01,  ...,  1.6970e-01,\n",
            "             3.2608e-01,  3.1320e-01],\n",
            "           [ 5.1177e-01,  2.7882e-03,  5.3904e-01,  ...,  3.8381e-01,\n",
            "             1.6411e-01,  4.0763e-01],\n",
            "           [ 5.7503e-01,  5.3125e-02,  6.4001e-01,  ...,  1.1404e-01,\n",
            "            -1.2196e-01,  1.3099e-01],\n",
            "           ...,\n",
            "           [ 3.3289e-01,  1.5686e+00, -5.8892e-01,  ..., -8.7609e-02,\n",
            "             2.9398e-01,  3.9603e-01],\n",
            "           [ 2.6870e-01,  3.8752e-01, -3.0370e-02,  ..., -2.9543e-02,\n",
            "             2.8891e-01,  5.0510e-01],\n",
            "           [ 2.7055e-01,  3.1165e-01,  3.3975e-01,  ...,  3.0602e-01,\n",
            "             1.9701e-01,  4.9650e-01]],\n",
            "\n",
            "          [[ 6.5730e-01,  4.1736e-01, -6.5233e-02,  ...,  3.4591e-01,\n",
            "             4.4647e-01,  3.9445e-01],\n",
            "           [ 7.0929e-01, -2.1409e-01,  4.9035e-02,  ...,  2.0717e-01,\n",
            "            -2.1360e-01,  4.7964e-01],\n",
            "           [ 2.1225e-02, -1.7872e-01,  2.3002e-01,  ..., -2.8244e-01,\n",
            "            -1.3942e-01,  1.8843e-01],\n",
            "           ...,\n",
            "           [ 7.6146e-02,  2.2632e-01,  1.2416e-01,  ...,  4.1881e-01,\n",
            "            -4.0617e-01,  5.5855e-01],\n",
            "           [ 8.3482e-01,  2.8976e-01,  9.2833e-02,  ...,  4.7849e-02,\n",
            "             3.9388e-01,  1.5575e-01],\n",
            "           [ 1.8147e-01,  1.8268e-01,  5.9221e-02,  ...,  1.7787e-01,\n",
            "            -1.3060e-01,  2.9188e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 3.0289e-01,  7.8660e-02,  5.9022e-01,  ...,  4.7438e-01,\n",
            "             5.3832e-01,  2.9051e-01],\n",
            "           [ 2.7046e-01, -2.8738e-01,  4.1452e-02,  ..., -4.9162e-02,\n",
            "            -7.6624e-02,  1.7162e-01],\n",
            "           [-2.6694e-01,  1.3924e-02,  7.2764e-02,  ...,  1.9229e-01,\n",
            "             5.7772e-01,  3.2499e-01],\n",
            "           ...,\n",
            "           [ 6.3362e-03, -3.0373e-01,  1.4889e-01,  ..., -4.0012e-01,\n",
            "            -1.0801e-01, -2.9223e-02],\n",
            "           [ 6.9341e-01, -1.1786e-01, -8.6842e-01,  ...,  2.8934e-01,\n",
            "             7.2149e-02, -1.1753e-01],\n",
            "           [ 3.9936e-01,  8.5724e-02, -1.0194e-01,  ...,  3.1050e-02,\n",
            "             3.9696e-01, -1.4203e-01]],\n",
            "\n",
            "          [[ 1.6351e-01,  1.9581e-01, -1.4693e-01,  ...,  1.4592e-01,\n",
            "             1.2779e-01,  3.2728e-01],\n",
            "           [-1.5339e-01, -2.6719e-01, -4.2111e-04,  ...,  9.3046e-02,\n",
            "            -3.4445e-01,  1.6980e-01],\n",
            "           [-6.6674e-02, -2.8804e-01, -5.0092e-01,  ..., -2.8137e-01,\n",
            "             8.1589e-02,  2.6210e-01],\n",
            "           ...,\n",
            "           [ 3.0743e-01, -3.2481e-01, -5.8978e-02,  ..., -4.3083e-02,\n",
            "             4.9058e-01,  2.5334e-02],\n",
            "           [ 3.5136e-01,  4.8478e-02, -2.0052e-01,  ...,  3.4147e-01,\n",
            "            -3.6893e-02,  9.8324e-02],\n",
            "           [-2.4187e-03,  2.5978e-01,  6.7850e-02,  ..., -3.6588e-02,\n",
            "            -2.6867e-01,  2.7722e-01]],\n",
            "\n",
            "          [[ 8.1971e-01,  2.8983e-01,  5.2767e-01,  ..., -1.3343e-02,\n",
            "             5.2908e-01,  4.6524e-01],\n",
            "           [ 3.3686e-01,  3.3017e-01,  8.9758e-02,  ...,  2.8742e-01,\n",
            "             3.3073e-01,  3.0518e-01],\n",
            "           [ 6.9451e-01,  3.6789e-01, -1.1383e-02,  ...,  7.2297e-02,\n",
            "             7.1117e-02, -1.3395e-02],\n",
            "           ...,\n",
            "           [ 2.3394e-01,  3.3989e-01,  1.4102e-01,  ...,  3.8807e-01,\n",
            "             1.8290e-01,  6.1411e-01],\n",
            "           [ 4.8920e-01, -7.0168e-02,  7.9828e-01,  ...,  4.3364e-01,\n",
            "             1.4833e-01,  3.4287e-01],\n",
            "           [ 4.4943e-01,  2.7161e-01,  3.4851e-01,  ...,  2.1692e-01,\n",
            "             4.7521e-01,  2.4244e-01]]],\n",
            "\n",
            "\n",
            "         [[[ 3.3639e-01,  3.7945e-01, -2.0190e-01,  ..., -5.4795e-01,\n",
            "            -8.4001e-02,  2.0249e-01],\n",
            "           [ 9.0587e-03,  2.4092e-01,  1.7250e-01,  ...,  4.2263e-02,\n",
            "             2.6636e-01,  3.4846e-01],\n",
            "           [-1.6979e-01,  1.1946e-01,  7.5844e-01,  ..., -7.4791e-01,\n",
            "             5.4031e-01, -2.7116e-01],\n",
            "           ...,\n",
            "           [-1.1044e-01,  5.5637e-01,  3.5871e-01,  ...,  4.4758e-02,\n",
            "            -3.6298e-01,  3.2905e-01],\n",
            "           [ 3.2230e-01, -4.0959e-01, -6.2509e-02,  ..., -1.7565e-01,\n",
            "            -1.4433e-01, -2.4631e-01],\n",
            "           [-1.1439e-01, -1.3467e-01, -1.0016e-01,  ..., -2.5198e-01,\n",
            "            -3.1274e-01, -5.1092e-02]],\n",
            "\n",
            "          [[ 1.8259e-01,  5.8114e-01,  1.0183e+00,  ...,  4.1491e-01,\n",
            "             1.0974e-01,  1.8827e-01],\n",
            "           [ 4.4789e-01,  6.1607e-01,  2.7659e-01,  ...,  6.1605e-01,\n",
            "             2.6235e-01,  2.2600e-01],\n",
            "           [ 7.4480e-01,  5.4675e-01,  8.3497e-01,  ...,  2.6990e-02,\n",
            "             4.2379e-01,  2.3194e-01],\n",
            "           ...,\n",
            "           [ 2.1808e-02, -1.5475e-01,  2.5819e-01,  ...,  9.7932e-01,\n",
            "             3.4326e-01,  3.3180e-01],\n",
            "           [ 4.8932e-01, -4.6214e-02,  9.3970e-01,  ...,  1.4626e-01,\n",
            "             4.0373e-01,  2.1249e-01],\n",
            "           [-3.9104e-01,  1.1845e-01, -6.0302e-01,  ..., -9.7494e-02,\n",
            "            -5.6667e-02, -3.1737e-02]],\n",
            "\n",
            "          [[ 1.7717e-01,  3.6765e-01,  8.6801e-01,  ...,  1.1017e-01,\n",
            "             3.1782e-01,  7.1505e-01],\n",
            "           [ 6.6106e-01,  6.8319e-01,  2.8241e-01,  ...,  6.3082e-01,\n",
            "             5.7537e-01,  3.3469e-01],\n",
            "           [ 8.9771e-01,  4.2436e-01,  4.7683e-01,  ..., -1.3098e-01,\n",
            "             6.5620e-01,  4.4443e-01],\n",
            "           ...,\n",
            "           [ 7.0611e-01, -4.6627e-01,  8.9123e-02,  ..., -1.2261e-01,\n",
            "             5.0194e-01,  8.0025e-02],\n",
            "           [ 6.5413e-01,  7.3807e-01, -9.3724e-02,  ...,  3.2517e-01,\n",
            "             1.0779e-01, -8.1435e-03],\n",
            "           [-2.7767e-01,  1.5903e-01, -3.4515e-01,  ..., -2.8623e-02,\n",
            "             4.2732e-02, -1.4261e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 1.4407e-01,  6.1363e-01,  3.4940e-01,  ...,  4.4167e-01,\n",
            "             5.0327e-01,  4.8797e-01],\n",
            "           [ 4.9814e-01,  6.2625e-01,  5.4032e-01,  ...,  7.5017e-01,\n",
            "             7.2029e-01,  1.1940e-01],\n",
            "           [ 3.2447e-01,  6.0597e-01,  1.4479e+00,  ...,  6.1771e-02,\n",
            "             2.6539e-01,  1.4961e-01],\n",
            "           ...,\n",
            "           [-4.0539e-02,  8.0788e-01,  5.1377e-01,  ...,  3.5615e-01,\n",
            "             5.5160e-01, -1.6284e-01],\n",
            "           [ 7.8648e-01,  3.0055e-01, -3.2569e-01,  ...,  5.7892e-03,\n",
            "            -7.9105e-02,  6.2263e-01],\n",
            "           [-1.0428e-01, -3.9239e-01,  4.8452e-02,  ..., -7.5386e-01,\n",
            "            -4.8064e-01, -2.5755e-02]],\n",
            "\n",
            "          [[ 1.9647e-01,  5.9401e-03, -2.3286e-01,  ...,  1.5039e-01,\n",
            "             9.9652e-02, -2.4246e-01],\n",
            "           [ 3.9731e-01,  3.2849e-01,  2.8684e-01,  ..., -1.3302e-01,\n",
            "             1.7809e-01, -2.7813e-02],\n",
            "           [ 5.8586e-01,  1.4014e-01, -2.2559e-01,  ...,  4.1977e-01,\n",
            "             4.4746e-01, -4.1885e-01],\n",
            "           ...,\n",
            "           [ 6.1058e-02,  6.9724e-01,  1.6367e-01,  ..., -1.4884e-01,\n",
            "            -1.1756e-01,  1.6710e-01],\n",
            "           [ 6.1838e-01,  4.3996e-03, -3.5994e-01,  ...,  4.1932e-01,\n",
            "             2.5799e-02, -3.4346e-01],\n",
            "           [-3.5543e-01, -1.8277e-01, -7.0603e-01,  ..., -3.7677e-02,\n",
            "            -8.3898e-01, -1.1242e-01]],\n",
            "\n",
            "          [[ 5.8197e-01,  7.3363e-01,  2.7634e-01,  ...,  6.0379e-01,\n",
            "             6.4197e-01,  6.1165e-01],\n",
            "           [ 8.1758e-01,  5.1347e-01,  4.1971e-01,  ...,  8.0942e-01,\n",
            "             7.4515e-01,  8.5486e-01],\n",
            "           [ 1.0169e+00,  5.4404e-01,  3.1121e-01,  ...,  3.7458e-01,\n",
            "             9.7533e-01,  1.1847e+00],\n",
            "           ...,\n",
            "           [ 9.1106e-01,  9.4990e-01,  3.6212e-01,  ...,  4.9241e-01,\n",
            "             8.3862e-01,  4.4245e-01],\n",
            "           [ 8.0824e-01,  1.0536e+00,  4.0859e-01,  ...,  8.1162e-01,\n",
            "             6.7923e-01,  5.3577e-01],\n",
            "           [ 1.5854e-01,  3.3277e-01,  5.3694e-01,  ...,  8.0650e-02,\n",
            "             5.0470e-01,  1.2013e-02]]],\n",
            "\n",
            "\n",
            "         [[[ 5.1814e-01,  9.2438e-02,  8.6021e-02,  ...,  5.9233e-02,\n",
            "            -7.4151e-02, -2.7347e-01],\n",
            "           [-2.0511e-01, -4.8002e-02, -4.8639e-01,  ..., -2.1116e-01,\n",
            "            -3.4170e-01, -4.6826e-01],\n",
            "           [-8.7462e-02, -2.8715e-02, -3.3252e-02,  ..., -2.1017e-01,\n",
            "            -4.7388e-01, -8.4044e-03],\n",
            "           ...,\n",
            "           [ 4.7198e-02, -1.5060e-01,  2.1788e-01,  ..., -3.3159e-01,\n",
            "            -5.0809e-01, -2.4028e-01],\n",
            "           [-1.1323e-01, -3.3668e-01, -6.1421e-01,  ..., -4.8001e-01,\n",
            "            -6.0134e-02, -2.5124e-01],\n",
            "           [-2.4303e-01, -3.3268e-01, -4.2802e-02,  ..., -2.6970e-01,\n",
            "            -2.7137e-01, -2.3179e-01]],\n",
            "\n",
            "          [[ 8.4917e-01,  1.4129e-01,  4.5545e-01,  ..., -3.5486e-01,\n",
            "             4.0941e-01,  1.9032e-02],\n",
            "           [ 1.6944e-01, -1.4492e-01,  3.4040e-01,  ..., -4.3900e-01,\n",
            "             2.6915e-01, -1.0888e-01],\n",
            "           [ 4.5848e-01,  9.8566e-03,  9.8565e-01,  ..., -2.3123e-01,\n",
            "             4.3488e-01,  2.4364e-01],\n",
            "           ...,\n",
            "           [ 8.3464e-01,  8.4135e-02,  7.6816e-02,  ..., -3.6174e-01,\n",
            "             4.7038e-01, -6.4495e-02],\n",
            "           [ 8.3060e-01,  1.2587e-01, -2.1048e-01,  ..., -6.5563e-01,\n",
            "             4.1714e-01,  4.3291e-02],\n",
            "           [ 5.6011e-01,  1.4168e-01,  3.9936e-01,  ..., -1.9876e-02,\n",
            "             2.6697e-01, -1.6804e-01]],\n",
            "\n",
            "          [[ 2.5761e-01,  2.8752e-01, -4.8768e-01,  ...,  1.4394e-01,\n",
            "            -4.4986e-01, -6.3214e-01],\n",
            "           [ 1.6522e-01, -3.9493e-01, -8.3012e-02,  ..., -5.3726e-01,\n",
            "            -9.0751e-01, -6.6950e-02],\n",
            "           [ 1.3012e-01,  1.9951e-01,  3.8987e-01,  ...,  5.8066e-02,\n",
            "             2.7552e-01, -7.5614e-02],\n",
            "           ...,\n",
            "           [ 5.3568e-02,  1.4817e-01, -2.6490e-01,  ..., -5.1139e-01,\n",
            "             5.1000e-02,  6.6336e-03],\n",
            "           [ 4.8978e-01, -2.1460e-01,  2.2671e-02,  ..., -6.6209e-02,\n",
            "            -6.0732e-01,  3.1519e-02],\n",
            "           [ 5.0313e-01, -3.8252e-01,  2.4147e-01,  ..., -2.9994e-01,\n",
            "            -1.1861e-01, -1.3403e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 2.6977e-01,  5.3900e-02, -3.5074e-03,  ...,  5.7233e-02,\n",
            "            -1.7492e-02, -1.7256e-01],\n",
            "           [ 3.9935e-01,  1.1729e-01, -4.6665e-01,  ..., -6.5622e-01,\n",
            "            -1.8363e-01, -2.7226e-01],\n",
            "           [-1.8356e-01, -6.5341e-01, -1.3407e-01,  ..., -6.8547e-01,\n",
            "             2.2785e-01, -1.1072e-01],\n",
            "           ...,\n",
            "           [ 8.6361e-01, -4.3739e-03,  1.1022e-01,  ..., -2.4812e-01,\n",
            "            -1.1930e-01, -1.0728e-01],\n",
            "           [ 3.3533e-01,  6.1361e-02,  7.0208e-01,  ..., -2.0627e-02,\n",
            "             3.7018e-01, -4.3367e-01],\n",
            "           [ 4.7960e-01, -4.8109e-01,  2.8066e-01,  ...,  2.2177e-01,\n",
            "            -8.3410e-02,  7.7963e-02]],\n",
            "\n",
            "          [[ 3.1520e-01, -5.5764e-02,  3.0414e-01,  ...,  4.0773e-01,\n",
            "            -9.6355e-03, -8.0306e-02],\n",
            "           [ 2.8125e-01,  1.8542e-01, -1.9926e-01,  ...,  1.6300e-01,\n",
            "            -8.9424e-02,  1.7560e-01],\n",
            "           [ 6.3084e-02,  1.0457e-01,  2.4587e-01,  ...,  2.2840e-01,\n",
            "            -5.4281e-01,  1.1092e-01],\n",
            "           ...,\n",
            "           [ 9.5134e-01, -5.6197e-01,  1.8442e-01,  ...,  1.7858e-01,\n",
            "             1.3357e-01,  1.3870e-01],\n",
            "           [ 3.9834e-01, -1.9182e-01,  6.6694e-02,  ..., -7.4758e-01,\n",
            "             2.1818e-01,  2.2123e-01],\n",
            "           [ 2.6842e-02, -6.0153e-01, -2.7542e-01,  ...,  3.1051e-01,\n",
            "             1.8883e-01, -2.9289e-01]],\n",
            "\n",
            "          [[-1.1572e-01, -3.7817e-01, -2.2002e-01,  ..., -4.4508e-01,\n",
            "             2.5529e-02, -1.0585e-01],\n",
            "           [ 2.6848e-02, -7.4334e-01, -2.4087e-01,  ..., -4.1382e-01,\n",
            "             8.7001e-02, -5.9883e-02],\n",
            "           [-2.1604e-01, -4.5938e-02, -2.2497e-01,  ..., -2.8860e-01,\n",
            "             6.6251e-02, -2.0362e-01],\n",
            "           ...,\n",
            "           [ 4.9790e-01, -2.1367e-01, -2.2847e-01,  ..., -4.9441e-01,\n",
            "            -1.5790e-02, -1.9834e-01],\n",
            "           [ 3.1763e-01, -2.9190e-01,  3.1728e-01,  ..., -5.5280e-01,\n",
            "             2.1631e-01, -2.9335e-01],\n",
            "           [ 1.9304e-01, -8.6548e-01, -9.9526e-02,  ..., -9.0309e-01,\n",
            "            -2.0665e-01, -2.5999e-01]]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(probability.shape)\n",
        "print(probability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxBKEIIneFRa",
        "outputId": "9f686b55-1e84-443d-ec8c-31f21b999820"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 128, 128, 128])\n",
            "tensor([[[[[0.3616, 0.4962, 0.4907,  ..., 0.3507, 0.4827, 0.4904],\n",
            "           [0.4922, 0.3982, 0.4296,  ..., 0.5276, 0.3467, 0.4388],\n",
            "           [0.4706, 0.3506, 0.3748,  ..., 0.5002, 0.4348, 0.4207],\n",
            "           ...,\n",
            "           [0.4758, 0.3134, 0.4411,  ..., 0.4495, 0.4286, 0.3678],\n",
            "           [0.4447, 0.6305, 0.5291,  ..., 0.4947, 0.4747, 0.5043],\n",
            "           [0.4465, 0.4006, 0.5250,  ..., 0.4991, 0.4545, 0.3855]],\n",
            "\n",
            "          [[0.2793, 0.4299, 0.2347,  ..., 0.4679, 0.3010, 0.3172],\n",
            "           [0.3462, 0.3178, 0.3962,  ..., 0.4330, 0.2614, 0.2927],\n",
            "           [0.4122, 0.1281, 0.3186,  ..., 0.2605, 0.2882, 0.3048],\n",
            "           ...,\n",
            "           [0.2469, 0.3104, 0.1938,  ..., 0.4880, 0.2525, 0.3779],\n",
            "           [0.3541, 0.3814, 0.2829,  ..., 0.4700, 0.2883, 0.3836],\n",
            "           [0.4389, 0.4231, 0.3417,  ..., 0.5005, 0.3003, 0.4991]],\n",
            "\n",
            "          [[0.3433, 0.2312, 0.3950,  ..., 0.5302, 0.3728, 0.4598],\n",
            "           [0.2934, 0.2928, 0.4642,  ..., 0.3009, 0.2918, 0.3322],\n",
            "           [0.4152, 0.2376, 0.3326,  ..., 0.4685, 0.4779, 0.4425],\n",
            "           ...,\n",
            "           [0.3004, 0.2693, 0.2976,  ..., 0.3386, 0.1846, 0.2773],\n",
            "           [0.3155, 0.3480, 0.2406,  ..., 0.3606, 0.3524, 0.2997],\n",
            "           [0.3494, 0.6470, 0.4636,  ..., 0.3266, 0.3306, 0.4575]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.4775, 0.3048, 0.2924,  ..., 0.3300, 0.4090, 0.3914],\n",
            "           [0.3062, 0.2894, 0.1854,  ..., 0.4808, 0.2781, 0.4010],\n",
            "           [0.2842, 0.2113, 0.4646,  ..., 0.2926, 0.2022, 0.3413],\n",
            "           ...,\n",
            "           [0.2580, 0.2529, 0.2430,  ..., 0.2623, 0.4833, 0.2825],\n",
            "           [0.3665, 0.2238, 0.3661,  ..., 0.2792, 0.3524, 0.3613],\n",
            "           [0.3669, 0.5226, 0.2080,  ..., 0.4720, 0.3099, 0.4046]],\n",
            "\n",
            "          [[0.3375, 0.2969, 0.2531,  ..., 0.2189, 0.2989, 0.4047],\n",
            "           [0.2836, 0.3062, 0.2406,  ..., 0.2596, 0.4037, 0.4067],\n",
            "           [0.2776, 0.2509, 0.3028,  ..., 0.3357, 0.1415, 0.4255],\n",
            "           ...,\n",
            "           [0.3544, 0.2487, 0.3634,  ..., 0.3248, 0.2793, 0.4561],\n",
            "           [0.2903, 0.2407, 0.2217,  ..., 0.3639, 0.3838, 0.3782],\n",
            "           [0.3368, 0.5019, 0.3762,  ..., 0.4429, 0.4248, 0.5996]],\n",
            "\n",
            "          [[0.3751, 0.3179, 0.2686,  ..., 0.2475, 0.4555, 0.4131],\n",
            "           [0.3510, 0.3335, 0.2757,  ..., 0.2807, 0.1819, 0.3221],\n",
            "           [0.3408, 0.3656, 0.2896,  ..., 0.1453, 0.2304, 0.3410],\n",
            "           ...,\n",
            "           [0.2806, 0.4813, 0.3426,  ..., 0.3218, 0.3160, 0.3096],\n",
            "           [0.3258, 0.2033, 0.4100,  ..., 0.4075, 0.2700, 0.3443],\n",
            "           [0.3402, 0.3115, 0.4400,  ..., 0.4158, 0.3933, 0.4265]]],\n",
            "\n",
            "\n",
            "         [[[0.2065, 0.2330, 0.2537,  ..., 0.2512, 0.2138, 0.3463],\n",
            "           [0.2735, 0.2760, 0.3080,  ..., 0.2679, 0.4333, 0.3553],\n",
            "           [0.1973, 0.2198, 0.3039,  ..., 0.2347, 0.2970, 0.3479],\n",
            "           ...,\n",
            "           [0.2296, 0.4060, 0.3342,  ..., 0.2963, 0.3321, 0.3518],\n",
            "           [0.2989, 0.1595, 0.1852,  ..., 0.3234, 0.1835, 0.2797],\n",
            "           [0.2387, 0.3325, 0.2609,  ..., 0.2155, 0.2517, 0.3319]],\n",
            "\n",
            "          [[0.2949, 0.2841, 0.4017,  ..., 0.3135, 0.2747, 0.3267],\n",
            "           [0.3622, 0.5169, 0.3674,  ..., 0.3537, 0.5244, 0.3082],\n",
            "           [0.3292, 0.6400, 0.3153,  ..., 0.3788, 0.3616, 0.3525],\n",
            "           ...,\n",
            "           [0.2463, 0.3194, 0.2660,  ..., 0.2489, 0.3417, 0.3054],\n",
            "           [0.2296, 0.3346, 0.4000,  ..., 0.2857, 0.4170, 0.3069],\n",
            "           [0.2138, 0.2904, 0.1561,  ..., 0.1855, 0.3840, 0.2333]],\n",
            "\n",
            "          [[0.3129, 0.3940, 0.3080,  ..., 0.3574, 0.2491, 0.2821],\n",
            "           [0.4421, 0.5421, 0.3335,  ..., 0.3916, 0.4723, 0.5364],\n",
            "           [0.2857, 0.5462, 0.4154,  ..., 0.3726, 0.2637, 0.3088],\n",
            "           ...,\n",
            "           [0.4296, 0.6179, 0.2170,  ..., 0.3569, 0.4955, 0.5454],\n",
            "           [0.4022, 0.4146, 0.1882,  ..., 0.4255, 0.3911, 0.2708],\n",
            "           [0.1942, 0.1843, 0.2805,  ..., 0.3393, 0.3234, 0.2855]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.2747, 0.3963, 0.4528,  ..., 0.5116, 0.3840, 0.3964],\n",
            "           [0.4049, 0.5302, 0.6199,  ..., 0.3063, 0.4639, 0.3495],\n",
            "           [0.5253, 0.5734, 0.3015,  ..., 0.3413, 0.4470, 0.4603],\n",
            "           ...,\n",
            "           [0.5546, 0.4386, 0.4282,  ..., 0.5758, 0.2848, 0.4253],\n",
            "           [0.3182, 0.4592, 0.3708,  ..., 0.5039, 0.3735, 0.4585],\n",
            "           [0.1543, 0.1954, 0.4175,  ..., 0.3135, 0.1985, 0.3516]],\n",
            "\n",
            "          [[0.4361, 0.3017, 0.2539,  ..., 0.3525, 0.3181, 0.2470],\n",
            "           [0.3744, 0.4280, 0.3947,  ..., 0.4623, 0.3429, 0.3846],\n",
            "           [0.3810, 0.5350, 0.5423,  ..., 0.3513, 0.5416, 0.2381],\n",
            "           ...,\n",
            "           [0.3270, 0.5231, 0.3232,  ..., 0.3466, 0.4269, 0.2512],\n",
            "           [0.2804, 0.3903, 0.3172,  ..., 0.2193, 0.3719, 0.2812],\n",
            "           [0.2664, 0.2291, 0.2045,  ..., 0.2747, 0.2235, 0.1524]],\n",
            "\n",
            "          [[0.3605, 0.5113, 0.5470,  ..., 0.5523, 0.3005, 0.3911],\n",
            "           [0.4640, 0.5126, 0.5427,  ..., 0.4807, 0.6594, 0.4876],\n",
            "           [0.4755, 0.4506, 0.5134,  ..., 0.6723, 0.5106, 0.3166],\n",
            "           ...,\n",
            "           [0.4860, 0.3307, 0.3575,  ..., 0.4645, 0.4446, 0.4873],\n",
            "           [0.4254, 0.4665, 0.3484,  ..., 0.3111, 0.5318, 0.5186],\n",
            "           [0.3742, 0.4440, 0.3055,  ..., 0.3785, 0.3890, 0.3876]]],\n",
            "\n",
            "\n",
            "         [[[0.4319, 0.2707, 0.2556,  ..., 0.3980, 0.3035, 0.1632],\n",
            "           [0.2343, 0.3258, 0.2624,  ..., 0.2045, 0.2200, 0.2059],\n",
            "           [0.3322, 0.4296, 0.3214,  ..., 0.2651, 0.2681, 0.2314],\n",
            "           ...,\n",
            "           [0.2946, 0.2806, 0.2247,  ..., 0.2542, 0.2392, 0.2804],\n",
            "           [0.2565, 0.2101, 0.2857,  ..., 0.1819, 0.3418, 0.2160],\n",
            "           [0.3149, 0.2669, 0.2141,  ..., 0.2854, 0.2938, 0.2826]],\n",
            "\n",
            "          [[0.4258, 0.2860, 0.3636,  ..., 0.2185, 0.4243, 0.3560],\n",
            "           [0.2916, 0.1653, 0.2364,  ..., 0.2133, 0.2142, 0.3992],\n",
            "           [0.2586, 0.2319, 0.3661,  ..., 0.3607, 0.3502, 0.3426],\n",
            "           ...,\n",
            "           [0.5068, 0.3703, 0.5402,  ..., 0.2631, 0.4058, 0.3167],\n",
            "           [0.4162, 0.2840, 0.3170,  ..., 0.2443, 0.2947, 0.3095],\n",
            "           [0.3473, 0.2865, 0.5022,  ..., 0.3141, 0.3157, 0.2676]],\n",
            "\n",
            "          [[0.3438, 0.3749, 0.2970,  ..., 0.1124, 0.3781, 0.2581],\n",
            "           [0.2645, 0.1650, 0.2023,  ..., 0.3074, 0.2359, 0.1314],\n",
            "           [0.2991, 0.2162, 0.2520,  ..., 0.1588, 0.2584, 0.2487],\n",
            "           ...,\n",
            "           [0.2700, 0.1128, 0.4854,  ..., 0.3045, 0.3200, 0.1773],\n",
            "           [0.2823, 0.2374, 0.5712,  ..., 0.2138, 0.2565, 0.4295],\n",
            "           [0.4564, 0.1687, 0.2559,  ..., 0.3342, 0.3460, 0.2570]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.2477, 0.2989, 0.2548,  ..., 0.1584, 0.2070, 0.2122],\n",
            "           [0.2889, 0.1805, 0.1947,  ..., 0.2128, 0.2580, 0.2495],\n",
            "           [0.1904, 0.2153, 0.2339,  ..., 0.3661, 0.3508, 0.1985],\n",
            "           ...,\n",
            "           [0.1874, 0.3085, 0.3288,  ..., 0.1619, 0.2319, 0.2922],\n",
            "           [0.3153, 0.3170, 0.2631,  ..., 0.2170, 0.2741, 0.1803],\n",
            "           [0.4788, 0.2820, 0.3745,  ..., 0.2144, 0.4916, 0.2438]],\n",
            "\n",
            "          [[0.2264, 0.4014, 0.4930,  ..., 0.4286, 0.3829, 0.3483],\n",
            "           [0.3420, 0.2659, 0.3647,  ..., 0.2781, 0.2534, 0.2087],\n",
            "           [0.3414, 0.2141, 0.1549,  ..., 0.3130, 0.3169, 0.3364],\n",
            "           ...,\n",
            "           [0.3186, 0.2281, 0.3135,  ..., 0.3286, 0.2938, 0.2927],\n",
            "           [0.4293, 0.3690, 0.4610,  ..., 0.4169, 0.2444, 0.3406],\n",
            "           [0.3969, 0.2691, 0.4192,  ..., 0.2824, 0.3517, 0.2480]],\n",
            "\n",
            "          [[0.2644, 0.1708, 0.1845,  ..., 0.2002, 0.2441, 0.1958],\n",
            "           [0.1850, 0.1539, 0.1816,  ..., 0.2386, 0.1587, 0.1904],\n",
            "           [0.1837, 0.1839, 0.1969,  ..., 0.1823, 0.2589, 0.3424],\n",
            "           ...,\n",
            "           [0.2334, 0.1880, 0.2998,  ..., 0.2137, 0.2394, 0.2031],\n",
            "           [0.2488, 0.3302, 0.2416,  ..., 0.2815, 0.1982, 0.1371],\n",
            "           [0.2855, 0.2445, 0.2545,  ..., 0.2057, 0.2177, 0.1859]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0.3057, 0.3417, 0.3944,  ..., 0.4275, 0.4831, 0.4411],\n",
            "           [0.5022, 0.3362, 0.4113,  ..., 0.5858, 0.3389, 0.3528],\n",
            "           [0.5311, 0.2985, 0.3885,  ..., 0.5135, 0.4180, 0.3611],\n",
            "           ...,\n",
            "           [0.4368, 0.3623, 0.3142,  ..., 0.4545, 0.5325, 0.3808],\n",
            "           [0.4647, 0.4212, 0.5496,  ..., 0.4467, 0.4552, 0.4337],\n",
            "           [0.5117, 0.4168, 0.4257,  ..., 0.4496, 0.4095, 0.4388]],\n",
            "\n",
            "          [[0.2946, 0.2783, 0.2460,  ..., 0.3485, 0.3457, 0.3806],\n",
            "           [0.3776, 0.2696, 0.3862,  ..., 0.3703, 0.3111, 0.4114],\n",
            "           [0.3252, 0.2781, 0.2756,  ..., 0.3810, 0.2237, 0.3100],\n",
            "           ...,\n",
            "           [0.2955, 0.7117, 0.1894,  ..., 0.2143, 0.3083, 0.3893],\n",
            "           [0.2499, 0.4136, 0.2235,  ..., 0.3667, 0.3069, 0.4208],\n",
            "           [0.3506, 0.3748, 0.4080,  ..., 0.4184, 0.3511, 0.4753]],\n",
            "\n",
            "          [[0.4368, 0.3534, 0.2382,  ..., 0.3836, 0.4372, 0.3655],\n",
            "           [0.3947, 0.2332, 0.3186,  ..., 0.3331, 0.2702, 0.4092],\n",
            "           [0.2214, 0.2332, 0.2896,  ..., 0.2802, 0.2114, 0.3268],\n",
            "           ...,\n",
            "           [0.2594, 0.4124, 0.3783,  ..., 0.5060, 0.1977, 0.4555],\n",
            "           [0.3933, 0.3155, 0.3620,  ..., 0.3114, 0.4720, 0.3660],\n",
            "           [0.3321, 0.3929, 0.3488,  ..., 0.4109, 0.3124, 0.4346]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.3545, 0.2715, 0.4277,  ..., 0.3807, 0.3938, 0.3512],\n",
            "           [0.2947, 0.2003, 0.3078,  ..., 0.2653, 0.2429, 0.3860],\n",
            "           [0.2568, 0.3011, 0.1733,  ..., 0.4360, 0.4104, 0.4023],\n",
            "           ...,\n",
            "           [0.2320, 0.1856, 0.2939,  ..., 0.2329, 0.2549, 0.3572],\n",
            "           [0.3576, 0.2691, 0.1328,  ..., 0.4022, 0.3118, 0.2614],\n",
            "           [0.3720, 0.4572, 0.2756,  ..., 0.3751, 0.4916, 0.2968]],\n",
            "\n",
            "          [[0.3128, 0.3839, 0.2867,  ..., 0.3027, 0.3516, 0.4482],\n",
            "           [0.2337, 0.2280, 0.3172,  ..., 0.3484, 0.2515, 0.3538],\n",
            "           [0.2464, 0.2490, 0.2259,  ..., 0.2136, 0.3359, 0.4227],\n",
            "           ...,\n",
            "           [0.2713, 0.2189, 0.2837,  ..., 0.3177, 0.4456, 0.3056],\n",
            "           [0.2981, 0.3645, 0.3166,  ..., 0.4137, 0.2980, 0.3605],\n",
            "           [0.3660, 0.4843, 0.4607,  ..., 0.2929, 0.3179, 0.4459]],\n",
            "\n",
            "          [[0.4585, 0.3256, 0.4442,  ..., 0.2855, 0.3671, 0.3673],\n",
            "           [0.2984, 0.3932, 0.3216,  ..., 0.3143, 0.3033, 0.2918],\n",
            "           [0.3594, 0.3504, 0.3136,  ..., 0.3279, 0.2240, 0.1945],\n",
            "           ...,\n",
            "           [0.2342, 0.2928, 0.3403,  ..., 0.3962, 0.2669, 0.4374],\n",
            "           [0.3107, 0.2050, 0.4356,  ..., 0.3531, 0.2652, 0.3647],\n",
            "           [0.3966, 0.4195, 0.3513,  ..., 0.4548, 0.3944, 0.4168]]],\n",
            "\n",
            "\n",
            "         [[[0.3157, 0.3761, 0.2595,  ..., 0.2019, 0.2572, 0.3447],\n",
            "           [0.2755, 0.3795, 0.3880,  ..., 0.2332, 0.4281, 0.4488],\n",
            "           [0.2248, 0.3767, 0.4208,  ..., 0.1794, 0.4271, 0.2777],\n",
            "           ...,\n",
            "           [0.2595, 0.4271, 0.3670,  ..., 0.3235, 0.2507, 0.3954],\n",
            "           [0.3250, 0.2789, 0.2858,  ..., 0.3184, 0.2609, 0.2838],\n",
            "           [0.2598, 0.3204, 0.2789,  ..., 0.2776, 0.2891, 0.3059]],\n",
            "\n",
            "          [[0.2393, 0.4390, 0.4804,  ..., 0.4453, 0.2785, 0.3359],\n",
            "           [0.3542, 0.4978, 0.2971,  ..., 0.4671, 0.3433, 0.3431],\n",
            "           [0.3854, 0.4556, 0.3350,  ..., 0.3492, 0.3860, 0.3430],\n",
            "           ...,\n",
            "           [0.2165, 0.1270, 0.4419,  ..., 0.6228, 0.3239, 0.3651],\n",
            "           [0.3116, 0.2680, 0.5898,  ..., 0.4372, 0.3442, 0.3140],\n",
            "           [0.1809, 0.3090, 0.1589,  ..., 0.2795, 0.2724, 0.2802]],\n",
            "\n",
            "          [[0.2703, 0.3363, 0.6057,  ..., 0.3030, 0.3844, 0.5036],\n",
            "           [0.3762, 0.5721, 0.4023,  ..., 0.5087, 0.5948, 0.3540],\n",
            "           [0.5318, 0.4263, 0.3706,  ..., 0.3260, 0.4685, 0.4222],\n",
            "           ...,\n",
            "           [0.4870, 0.2063, 0.3653,  ..., 0.2944, 0.4901, 0.2823],\n",
            "           [0.3282, 0.4940, 0.3004,  ..., 0.4109, 0.3546, 0.3107],\n",
            "           [0.2098, 0.3838, 0.2328,  ..., 0.3343, 0.3715, 0.2815]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.3025, 0.4636, 0.3361,  ..., 0.3684, 0.3803, 0.4278],\n",
            "           [0.3701, 0.4995, 0.5070,  ..., 0.5901, 0.5389, 0.3664],\n",
            "           [0.4640, 0.5444, 0.6857,  ..., 0.3827, 0.3003, 0.3376],\n",
            "           ...,\n",
            "           [0.2213, 0.5641, 0.4233,  ..., 0.4961, 0.4930, 0.3125],\n",
            "           [0.3925, 0.4089, 0.2285,  ..., 0.3029, 0.2681, 0.5480],\n",
            "           [0.2248, 0.2834, 0.3203,  ..., 0.1711, 0.2044, 0.3334]],\n",
            "\n",
            "          [[0.3232, 0.3175, 0.2631,  ..., 0.3040, 0.3419, 0.2536],\n",
            "           [0.4054, 0.4136, 0.4228,  ..., 0.2779, 0.4240, 0.2904],\n",
            "           [0.4731, 0.3822, 0.2975,  ..., 0.4307, 0.4842, 0.2139],\n",
            "           ...,\n",
            "           [0.2121, 0.6084, 0.3544,  ..., 0.2858, 0.2426, 0.3521],\n",
            "           [0.3894, 0.3488, 0.2699,  ..., 0.4471, 0.3173, 0.2318],\n",
            "           [0.2571, 0.3111, 0.2125,  ..., 0.2926, 0.1797, 0.3020]],\n",
            "\n",
            "          [[0.3615, 0.5075, 0.3455,  ..., 0.5292, 0.4110, 0.4252],\n",
            "           [0.4827, 0.4723, 0.4473,  ..., 0.5298, 0.4590, 0.5056],\n",
            "           [0.4961, 0.4179, 0.4330,  ..., 0.4436, 0.5532, 0.6446],\n",
            "           ...,\n",
            "           [0.4609, 0.5389, 0.4245,  ..., 0.4398, 0.5142, 0.3684],\n",
            "           [0.4275, 0.6307, 0.2950,  ..., 0.5153, 0.4510, 0.4423],\n",
            "           [0.2965, 0.4460, 0.4242,  ..., 0.3968, 0.4062, 0.3310]]],\n",
            "\n",
            "\n",
            "         [[[0.3786, 0.2822, 0.3461,  ..., 0.3706, 0.2597, 0.2142],\n",
            "           [0.2223, 0.2843, 0.2007,  ..., 0.1810, 0.2331, 0.1983],\n",
            "           [0.2441, 0.3248, 0.1907,  ..., 0.3071, 0.1549, 0.3612],\n",
            "           ...,\n",
            "           [0.3038, 0.2106, 0.3188,  ..., 0.2220, 0.2168, 0.2238],\n",
            "           [0.2103, 0.2999, 0.1646,  ..., 0.2349, 0.2839, 0.2824],\n",
            "           [0.2285, 0.2628, 0.2954,  ..., 0.2728, 0.3014, 0.2553]],\n",
            "\n",
            "          [[0.4661, 0.2827, 0.2736,  ..., 0.2062, 0.3758, 0.2836],\n",
            "           [0.2681, 0.2326, 0.3167,  ..., 0.1626, 0.3456, 0.2455],\n",
            "           [0.2894, 0.2663, 0.3894,  ..., 0.2698, 0.3903, 0.3470],\n",
            "           ...,\n",
            "           [0.4880, 0.1613, 0.3686,  ..., 0.1629, 0.3678, 0.2456],\n",
            "           [0.4384, 0.3184, 0.1867,  ..., 0.1961, 0.3489, 0.2652],\n",
            "           [0.4684, 0.3162, 0.4331,  ..., 0.3021, 0.3765, 0.2445]],\n",
            "\n",
            "          [[0.2929, 0.3104, 0.1561,  ..., 0.3134, 0.1784, 0.1309],\n",
            "           [0.2291, 0.1947, 0.2792,  ..., 0.1582, 0.1350, 0.2369],\n",
            "           [0.2468, 0.3405, 0.3398,  ..., 0.3938, 0.3201, 0.2510],\n",
            "           ...,\n",
            "           [0.2536, 0.3814, 0.2564,  ..., 0.1996, 0.3122, 0.2623],\n",
            "           [0.2785, 0.1905, 0.3375,  ..., 0.2778, 0.1734, 0.3233],\n",
            "           [0.4581, 0.2233, 0.4185,  ..., 0.2548, 0.3161, 0.2839]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.3430, 0.2649, 0.2362,  ..., 0.2509, 0.2259, 0.2210],\n",
            "           [0.3352, 0.3002, 0.1852,  ..., 0.1446, 0.2182, 0.2476],\n",
            "           [0.2792, 0.1545, 0.1410,  ..., 0.1813, 0.2893, 0.2602],\n",
            "           ...,\n",
            "           [0.5467, 0.2504, 0.2828,  ..., 0.2711, 0.2521, 0.3303],\n",
            "           [0.2500, 0.3219, 0.6387,  ..., 0.2950, 0.4201, 0.1906],\n",
            "           [0.4031, 0.2594, 0.4041,  ..., 0.4539, 0.3041, 0.3698]],\n",
            "\n",
            "          [[0.3640, 0.2985, 0.4502,  ..., 0.3933, 0.3065, 0.2982],\n",
            "           [0.3609, 0.3585, 0.2600,  ..., 0.3737, 0.3245, 0.3559],\n",
            "           [0.2805, 0.3688, 0.4766,  ..., 0.3557, 0.1799, 0.3634],\n",
            "           ...,\n",
            "           [0.5166, 0.1727, 0.3619,  ..., 0.3965, 0.3118, 0.3423],\n",
            "           [0.3125, 0.2867, 0.4135,  ..., 0.1392, 0.3846, 0.4077],\n",
            "           [0.3769, 0.2047, 0.3268,  ..., 0.4145, 0.5023, 0.2521]],\n",
            "\n",
            "          [[0.1799, 0.1669, 0.2103,  ..., 0.1854, 0.2219, 0.2075],\n",
            "           [0.2189, 0.1344, 0.2311,  ..., 0.1559, 0.2377, 0.2026],\n",
            "           [0.1446, 0.2317, 0.2533,  ..., 0.2285, 0.2229, 0.1608],\n",
            "           ...,\n",
            "           [0.3049, 0.1683, 0.2352,  ..., 0.1639, 0.2188, 0.1941],\n",
            "           [0.2617, 0.1642, 0.2693,  ..., 0.1317, 0.2838, 0.1930],\n",
            "           [0.3069, 0.1346, 0.2245,  ..., 0.1484, 0.1994, 0.2522]]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pouMPGESf3DW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_images = []\n",
        "for i in range(2):\n",
        "  newx = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_training_images.append(newx)"
      ],
      "metadata": {
        "id": "0ohSRRMqgJPg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_training_images))\n",
        "print(randomized_training_images[0].shape)\n",
        "# print(randomized_training_images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEM8Z-YVgK4L",
        "outputId": "a88f4213-0a82-41cb-90a9-ae67c5d16778"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 3, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_training_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "je2iTekggR-n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_training_segmentations))\n",
        "print(randomized_training_segmentations[0].shape)\n",
        "# print(randomized_training_segmentations[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQI7ldbUgTss",
        "outputId": "0b6dbd1b-c439-4fbb-a632-5c3523b097a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 3, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_data = list(zip(randomized_training_images, randomized_training_segmentations))"
      ],
      "metadata": {
        "id": "0Id-C28UgVDx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(dataset=randomized_training_data, batch_size=2, shuffle=True) # batch size should be 2"
      ],
      "metadata": {
        "id": "0kXxYbR2gWRx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1mBIdOfgXms",
        "outputId": "b4aadb56-f763-4755-9223-0dfe311a71e4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_images = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_validation_images.append(newy)"
      ],
      "metadata": {
        "id": "jH0ms_qigaKE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_validation_images))\n",
        "print(randomized_validation_images[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDOlu_IigcUt",
        "outputId": "88e88563-4781-48d2-8b05-f44adedfc48c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 3, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_validation_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "S9ruiz4Tgevx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_validation_segmentations))\n",
        "print(randomized_validation_segmentations[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOYt_EuigfYa",
        "outputId": "6d57a0c1-9319-446e-953d-d641ecab7afa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 3, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_data = list(zip(randomized_validation_images, randomized_validation_segmentations))"
      ],
      "metadata": {
        "id": "zFPZkDakgz_p"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validationloader = torch.utils.data.DataLoader(dataset=randomized_validation_data, batch_size=2, shuffle=True) # batch size should be 2"
      ],
      "metadata": {
        "id": "6Wb-FS1Lg1jM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(validationloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fchjms5Gg2au",
        "outputId": "1bfbd3cf-039c-448f-ddd0-dba602d9cf1d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_images = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_testing_images.append(newy)"
      ],
      "metadata": {
        "id": "527yVWmyggOd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_testing_images))\n",
        "print(randomized_testing_images[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDHqy02vhBWt",
        "outputId": "f2db96bf-2342-4651-f0f3-07681611cc52"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 3, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_testing_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "y4Zu_IyFhC92"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_testing_segmentations))\n",
        "print(randomized_testing_segmentations[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1SbByMohEmA",
        "outputId": "e50bbe15-23ee-4351-ec83-907904a27cf4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 3, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_data = list(zip(randomized_testing_segmentations, randomized_testing_segmentations))"
      ],
      "metadata": {
        "id": "O0KeJ9qyhSIB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testingloader = torch.utils.data.DataLoader(dataset=randomized_testing_data, batch_size=2, shuffle=True) # batch size should be 2"
      ],
      "metadata": {
        "id": "Y-6FP0OmhTWX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training\n",
        "The sum of cross-entropy and dice loss \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9Rja_lBDtooP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "256021d8-1abd-4464-fff3-717faa2e8ed9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTraining\\nThe sum of cross-entropy and dice loss \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chose to make this a class because when you call dice loss in criterion, you don't have anything to input, but when u run the prediction through inside the training, then you have params\n",
        "# also because most sources I saw used a class\n",
        "class DiceLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self, true, pred):\n",
        "    # flatten to easily do it pixel by pixel\n",
        "    true = true.view(-1)\n",
        "    pred = pred.view(-1)\n",
        "    numerator = 2*(true*pred).sum()\n",
        "    denominator = true.sum() + pred.sum()\n",
        "    dice_loss = 1 - (numerator) / (denominator)\n",
        "    return dice_loss"
      ],
      "metadata": {
        "id": "b8VIZVcJhTYT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim"
      ],
      "metadata": {
        "id": "6JfmRbwXiE-l"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# btw im not doing patches\n",
        "# epochs\n",
        "epochs = 2 # should be 1000\n",
        "# loss\n",
        "criterion1 = DiceLoss()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01, momentum= 0.99)"
      ],
      "metadata": {
        "id": "kay03ku8iGA0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  training_loss = 0\n",
        "  validation_loss = 0\n",
        "  print(\"training time\")\n",
        "  for images, segs in trainloader:\n",
        "    optimizer.zero_grad()\n",
        "    print(len(images), len(segs))\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "    # segs = segs.long() - no\n",
        "    segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    outputs, softmax_outputs = model(images)\n",
        "    print(outputs.shape)\n",
        "    print(softmax_outputs.shape)\n",
        "\n",
        "    # arg_outputs = outputs.argmax(dim=1)\n",
        "    # print(arg_outputs.shape)\n",
        "    # print(arg_outputs)\n",
        "    # print(segs.shape)\n",
        "    print()\n",
        "    diceloss = criterion1(softmax_outputs.float(), segs)\n",
        "    celoss = criterion2(outputs.float(), segs)\n",
        "    loss = diceloss + celoss\n",
        "    print(loss) # loss with random tensors will be really high because none of the tensors are related to each other\n",
        "    # loss can be > 1 - https://ai.stackexchange.com/questions/24685/can-the-sparse-categorical-cross-entropy-be-greater-than-one, https://stats.stackexchange.com/questions/392681/cross-entropy-loss-max-value\n",
        "    loss.backward()\n",
        "    training_loss += loss.item()\n",
        "    print()\n",
        "  print(\"validation time\")\n",
        "  for images, segs in validationloader:\n",
        "    optimizer.zero_grad()\n",
        "    print(len(images), len(segs))\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "    # segs = segs.long() - no\n",
        "    segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    outputs, softmax_outputs = model(images)\n",
        "    print(outputs.shape)\n",
        "    print(softmax_outputs.shape)\n",
        "    diceloss = criterion1(softmax_outputs.float(), segs)\n",
        "    celoss = criterion2(outputs.float(), segs)\n",
        "    loss = diceloss + celoss\n",
        "    print(loss) # loss with random tensors will be really high because none of the tensors are related to each other\n",
        "    # loss can be > 1 - https://ai.stackexchange.com/questions/24685/can-the-sparse-categorical-cross-entropy-be-greater-than-one, https://stats.stackexchange.com/questions/392681/cross-entropy-loss-max-value\n",
        "    loss.backward()\n",
        "    validation_loss += loss.item()\n",
        "  training_losses.append(training_loss/len(trainloader))\n",
        "  validation_losses.append(validation_loss/len(validationloader))\n",
        "  print(\"Epoch: {}/{}... Training Loss: {}... Validation Loss: {}...\".format(i+1,epochs, training_losses[-1], validation_losses[-1]))\n",
        "  if validation_loss < min(validation_losses):\n",
        "    print(\"Validation loss has decreased...saving model\")\n",
        "    torch.save(model.state_dict(), \"fcn.pth\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqJhvR3HpfQC",
        "outputId": "d5008567-c18c-49cb-de25-a1d940ba5733"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3361, grad_fn=<AddBackward0>)\n",
            "\n",
            "validation time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "tensor(2.3357, grad_fn=<AddBackward0>)\n",
            "Epoch: 1/2... Training Loss: 2.3360934257507324... Validation Loss: 2.3357226848602295...\n",
            "\n",
            "training time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "\n",
            "tensor(2.3361, grad_fn=<AddBackward0>)\n",
            "\n",
            "validation time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "tensor(2.3357, grad_fn=<AddBackward0>)\n",
            "Epoch: 2/2... Training Loss: 2.3360934257507324... Validation Loss: 2.3357226848602295...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_losses)\n",
        "print(validation_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de7ESYI8rt6b",
        "outputId": "eff9a476-b3d3-41eb-c96e-0f945371b395"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.3360934257507324, 2.3360934257507324]\n",
            "[2.3357226848602295, 2.3357226848602295]\n"
          ]
        }
      ]
    }
  ]
}