{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P_MbWzdtmbBU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import conv\n",
        "class Layer1(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    # nn.Sequential doesn't work\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2) # concatenate this before 2nd ReLU in res block\n",
        "    # print(x3.shape)\n",
        "\n",
        "    # block 2\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) # concatenate this before 2nd ReLU in res block\n",
        "    # print(x8.shape)\n",
        "\n",
        "    # element wise add\n",
        "    x9 = torch.add(x3, x8)\n",
        "\n",
        "    # relu\n",
        "    x10 = self.relu(x9)\n",
        "    return x10"
      ],
      "metadata": {
        "id": "7kCncP7WoiQN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer2(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) # concat\n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "    return x10"
      ],
      "metadata": {
        "id": "uXDYAroiRJ8z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer3(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) # concat\n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14) # concat\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "    return x17"
      ],
      "metadata": {
        "id": "cjt8QRpBO80p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer4(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) \n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14)\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "\n",
        "    # block 2 x3\n",
        "    x18 = self.conv(x17)\n",
        "    x19 = self.instnorm(x18)\n",
        "    x20 = self.relu(x19)\n",
        "    x21 = self.conv(x20)\n",
        "    x22 = self.instnorm(x21)\n",
        "    x23 = torch.add(x17, x22)\n",
        "    x24 = self.relu(x23)\n",
        "    return x24"
      ],
      "metadata": {
        "id": "wyTA3AkiPopy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer5(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) \n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14)\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "\n",
        "    # block 2 x3\n",
        "    x18 = self.conv(x17)\n",
        "    x19 = self.instnorm(x18)\n",
        "    x20 = self.relu(x19)\n",
        "    x21 = self.conv(x20)\n",
        "    x22 = self.instnorm(x21)\n",
        "    x23 = torch.add(x17, x22)\n",
        "    x24 = self.relu(x23)\n",
        "\n",
        "    # block 2 x4\n",
        "    x25 = self.conv(x24)\n",
        "    x26 = self.instnorm(x25)\n",
        "    x27 = self.relu(x26)\n",
        "    x28 = self.conv(x27)\n",
        "    x29 = self.instnorm(x28)\n",
        "    x30 = torch.add(x24, x29)\n",
        "    x31 = self.relu(x30)\n",
        "    return x31"
      ],
      "metadata": {
        "id": "B0BUmRhkRshf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer6(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=2, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv3d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block 1\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.instnorm(x1)\n",
        "    x3 = self.relu(x2)\n",
        "\n",
        "    # block 2 x1\n",
        "    x4 = self.conv(x3)\n",
        "    x5 = self.instnorm(x4)\n",
        "    x6 = self.relu(x5)\n",
        "    x7 = self.conv(x6)\n",
        "    x8 = self.instnorm(x7) \n",
        "    x9 = torch.add(x8, x3)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    # block 2 x2\n",
        "    x11 = self.conv(x10)\n",
        "    x12 = self.instnorm(x11)\n",
        "    x13 = self.relu(x12)\n",
        "    x14 = self.conv(x13)\n",
        "    x15 = self.instnorm(x14)\n",
        "    x16 = torch.add(x10, x15)\n",
        "    x17 = self.relu(x16)\n",
        "\n",
        "    # block 2 x3\n",
        "    x18 = self.conv(x17)\n",
        "    x19 = self.instnorm(x18)\n",
        "    x20 = self.relu(x19)\n",
        "    x21 = self.conv(x20)\n",
        "    x22 = self.instnorm(x21)\n",
        "    x23 = torch.add(x17, x22)\n",
        "    x24 = self.relu(x23)\n",
        "\n",
        "    # block 2 x4\n",
        "    x25 = self.conv(x24)\n",
        "    x26 = self.instnorm(x25)\n",
        "    x27 = self.relu(x26)\n",
        "    x28 = self.conv(x27)\n",
        "    x29 = self.instnorm(x28)\n",
        "    x30 = torch.add(x24, x29)\n",
        "    x31 = self.relu(x30)\n",
        "\n",
        "    # block 2 x5\n",
        "    x32 = self.conv(x31)\n",
        "    x33 = self.instnorm(x32)\n",
        "    x34 = self.relu(x33)\n",
        "    x35 = self.conv(x34)\n",
        "    x36 = self.instnorm(x35)\n",
        "    x37 = torch.add(x31, x36)\n",
        "    x38 = self.relu(x37)\n",
        "    return x38"
      ],
      "metadata": {
        "id": "B8f_Cz09Sxdg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlockDecoder(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels):\n",
        "    super().__init__()\n",
        "    self.transcov = nn.ConvTranspose3d(in_channels=input_channels, out_channels=input_channels, kernel_size=2, stride=2)\n",
        "    self.conv = nn.Conv3d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
        "    self.instnorm = nn.InstanceNorm3d(output_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x, concat_tensor):\n",
        "    x1 = self.transcov(x)\n",
        "    x2 = torch.add(x1, concat_tensor)\n",
        "    x3 = self.conv(x2)\n",
        "    x4 = self.instnorm(x3)\n",
        "    x5 = self.relu(x4)\n",
        "    return x5"
      ],
      "metadata": {
        "id": "6njHhcTiT8kk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualUNet(nn.Module):\n",
        "  def __init__(self, input_shape):\n",
        "    super().__init__()\n",
        "    self.layer1 = Layer1(4, 24)\n",
        "    self.layer2 = Layer2(24, 48)\n",
        "    self.layer3 = Layer3(48, 96)\n",
        "    self.layer4 = Layer4(96, 192)\n",
        "    self.layer5 = Layer5(192, 320)\n",
        "    self.layer6 = Layer6(320, 320)\n",
        "    self.layer7 = ConvBlockDecoder(320, 192)\n",
        "    self.layer8 = ConvBlockDecoder(192, 96)\n",
        "    self.layer9 = ConvBlockDecoder(96, 48)\n",
        "    self.layer10 = ConvBlockDecoder(48, 24)\n",
        "    self.layer11 = ConvBlockDecoder(24, 24)\n",
        "    self.conv1x1x1 = nn.Conv3d(in_channels=24, out_channels=4, kernel_size=1, stride=1)\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    print(x.shape) # [1, 4, 128, 128, 128]\n",
        "    x1 = self.layer1(x)\n",
        "    print(x1.shape) # [1, 24, 128, 128, 128]\n",
        "\n",
        "    x2 = self.layer2(x1)\n",
        "    print(x2.shape) # [1, 48, 64, 64, 64]\n",
        "\n",
        "    x3 = self.layer3(x2)\n",
        "    print(x3.shape) # [1, 96, 32, 32, 32]\n",
        "\n",
        "    x4 = self.layer4(x3)\n",
        "    print(x4.shape) # [1, 192, 16, 16, 16])\n",
        "\n",
        "    x5 = self.layer5(x4)\n",
        "    print(x5.shape) # [1, 320, 8, 8, 8]\n",
        "\n",
        "    x6 = self.layer6(x5)\n",
        "    print(x6.shape) # [1, 320, 4, 4, 4]\n",
        "\n",
        "    x7 = self.layer7(x6, x5)\n",
        "    print(x7.shape) # [1, 320, 4, 4, 4]\n",
        "\n",
        "    x8 = self.layer8(x7, x4)\n",
        "    print(x8.shape) # [1, 192, 8, 8, 8]\n",
        "\n",
        "    x9 = self.layer9(x8, x3)\n",
        "    print(x9.shape) # [1, 96, 16, 16, 16]\n",
        "\n",
        "    x10 = self.layer10(x9, x2)\n",
        "    print(x10.shape) # [1, 48, 32, 32, 32]\n",
        "\n",
        "    x11 = self.layer11(x10, x1)\n",
        "    print(x11.shape) # [1, 24, 64, 64, 64]\n",
        "\n",
        "    x12 = self.conv1x1x1(x11)\n",
        "    print(x12.shape) # [1, 24, 128, 128, 128]\n",
        "\n",
        "    prob = self.softmax(x12) # [1, 4, 128, 128, 128]\n",
        "\n",
        "    return x12, prob"
      ],
      "metadata": {
        "id": "tJpWDC3oRgAU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(size=(2, 4, 128, 128, 128), dtype=torch.float32)\n",
        "# print(x.shape)\n",
        "\n",
        "model = ResidualUNet(x.shape)\n",
        "print(model)\n",
        "print()\n",
        "\n",
        "out = model(x)\n",
        "# print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7zsz3sFtFMZ",
        "outputId": "b42e7449-ec4f-4575-c6e0-734eb1573f97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResidualUNet(\n",
            "  (layer1): Layer1(\n",
            "    (conv1): Conv3d(4, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer2): Layer2(\n",
            "    (conv1): Conv3d(24, 48, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer3): Layer3(\n",
            "    (conv1): Conv3d(48, 96, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer4): Layer4(\n",
            "    (conv1): Conv3d(96, 192, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer5): Layer5(\n",
            "    (conv1): Conv3d(192, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer6): Layer6(\n",
            "    (conv1): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "    (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  )\n",
            "  (layer7): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(320, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer8): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(192, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer9): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(96, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer10): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(48, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(48, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer11): ConvBlockDecoder(\n",
            "    (transcov): ConvTranspose3d(24, 24, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (conv): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (instnorm): InstanceNorm3d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (conv1x1x1): Conv3d(24, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output, probability = out"
      ],
      "metadata": {
        "id": "6uIe4jxJd_40"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spnR9HzFelYy",
        "outputId": "3bd30d95-7ae1-4878-bb61-526360d8ed36"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 128, 128, 128])\n",
            "tensor([[[[[ 9.8355e-02,  8.4364e-02, -1.1735e-01,  ...,  3.5398e-01,\n",
            "             8.4577e-02,  2.2950e-01],\n",
            "           [ 4.7926e-01,  2.9260e-01, -4.5907e-02,  ..., -6.9443e-02,\n",
            "             6.0083e-02,  3.7119e-01],\n",
            "           [-3.1483e-01, -1.8319e-01,  2.0164e-02,  ..., -1.1144e-01,\n",
            "            -6.2461e-02, -8.7484e-02],\n",
            "           ...,\n",
            "           [ 4.3557e-03,  2.6417e-01, -6.8776e-02,  ...,  1.9701e-01,\n",
            "             1.5996e-01,  1.9354e-02],\n",
            "           [-3.1506e-01, -1.4187e-01, -3.6458e-01,  ..., -1.5512e-01,\n",
            "            -2.9781e-01,  2.0675e-02],\n",
            "           [-3.8352e-01, -1.4258e-01,  8.2822e-02,  ..., -3.4391e-01,\n",
            "            -9.0388e-02,  8.8806e-02]],\n",
            "\n",
            "          [[ 1.1789e-01,  2.6575e-01,  3.8306e-01,  ..., -2.0077e-01,\n",
            "            -6.7902e-02, -9.9870e-02],\n",
            "           [ 4.8985e-01,  9.3815e-02, -1.8866e-01,  ..., -3.3745e-01,\n",
            "            -2.2370e-01, -3.4868e-01],\n",
            "           [ 2.3134e-01, -1.0210e-01,  1.8121e-01,  ..., -3.3807e-01,\n",
            "            -2.7616e-02, -1.0721e-01],\n",
            "           ...,\n",
            "           [-1.9339e-01, -8.8007e-02,  1.3909e-01,  ..., -4.0363e-02,\n",
            "            -2.5505e-01, -2.4701e-02],\n",
            "           [-2.0073e-01, -1.1945e-01, -6.2925e-01,  ..., -3.1858e-01,\n",
            "            -6.3933e-01, -2.4791e-01],\n",
            "           [ 1.5166e-01,  6.6443e-02,  1.4543e-01,  ...,  2.6708e-01,\n",
            "            -4.3810e-01,  4.2466e-02]],\n",
            "\n",
            "          [[ 2.6828e-01,  4.6999e-01,  1.4471e-01,  ...,  2.4056e-01,\n",
            "             4.4962e-01, -2.3848e-01],\n",
            "           [-2.7029e-01, -1.8952e-01,  2.9912e-01,  ..., -4.3997e-02,\n",
            "             4.0729e-01, -1.0525e-01],\n",
            "           [ 8.5582e-02,  1.9530e-01,  1.5243e-01,  ...,  1.1841e-02,\n",
            "             3.1580e-02, -3.5362e-01],\n",
            "           ...,\n",
            "           [ 1.3324e-01, -1.5137e-01, -3.1158e-01,  ...,  1.6183e-01,\n",
            "            -2.5003e-01,  6.6975e-02],\n",
            "           [-1.7851e-01, -1.4720e-01, -5.9645e-01,  ..., -8.4835e-01,\n",
            "            -1.4985e-01, -3.9858e-01],\n",
            "           [-1.7243e-02, -4.9727e-01, -2.6024e-01,  ..., -5.9463e-01,\n",
            "            -8.1345e-01, -3.8867e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 7.2129e-02, -1.2516e-01, -3.4551e-02,  ..., -4.7927e-02,\n",
            "            -1.3613e-01,  2.2591e-02],\n",
            "           [-2.0282e-01, -4.2265e-01, -1.1705e-01,  ...,  6.8502e-02,\n",
            "            -4.3815e-01, -2.3805e-02],\n",
            "           [ 3.2180e-01,  4.4746e-01,  8.4853e-02,  ..., -7.6339e-02,\n",
            "            -3.1129e-02, -5.0912e-03],\n",
            "           ...,\n",
            "           [-4.3284e-02,  2.2620e-01, -2.3963e-01,  ..., -2.7392e-01,\n",
            "            -1.8097e-01, -4.6140e-01],\n",
            "           [-4.1737e-02, -6.4555e-01, -4.5194e-01,  ..., -6.2166e-01,\n",
            "            -6.8652e-01, -4.4771e-01],\n",
            "           [-1.0735e-01, -6.0318e-01, -7.9035e-01,  ..., -4.3845e-01,\n",
            "            -4.5006e-01,  1.3460e-01]],\n",
            "\n",
            "          [[ 4.9655e-01,  1.1109e-01, -2.5830e-01,  ...,  1.3647e-01,\n",
            "            -3.1311e-02,  1.6340e-01],\n",
            "           [-3.0725e-01,  1.5171e-01,  1.6030e-01,  ..., -4.9774e-02,\n",
            "            -1.0710e-01, -1.9685e-01],\n",
            "           [ 5.0982e-02, -9.4301e-02, -2.7999e-01,  ..., -6.0921e-02,\n",
            "            -3.9919e-01, -1.3194e-01],\n",
            "           ...,\n",
            "           [-5.2673e-02,  1.6947e-01,  1.6365e-01,  ..., -9.5898e-02,\n",
            "            -2.4896e-01, -2.4037e-01],\n",
            "           [-7.1895e-02, -8.7662e-01, -6.7129e-01,  ..., -2.7700e-01,\n",
            "            -5.3402e-01, -2.9396e-01],\n",
            "           [-2.1583e-01, -8.4108e-01, -2.6915e-01,  ..., -6.6045e-01,\n",
            "            -7.7026e-01, -1.1820e-01]],\n",
            "\n",
            "          [[ 1.9621e-01, -8.8353e-02, -3.9450e-02,  ..., -1.0385e-01,\n",
            "            -3.1586e-01, -2.4211e-01],\n",
            "           [ 1.0557e-01,  2.0265e-01,  1.4949e-01,  ..., -1.9726e-01,\n",
            "             1.8045e-01, -2.1521e-01],\n",
            "           [-2.7049e-02,  2.0228e-01, -2.1014e-02,  ...,  1.9501e-01,\n",
            "             1.2451e-03, -7.1170e-02],\n",
            "           ...,\n",
            "           [ 1.6574e-02,  1.9683e-01, -1.2671e-01,  ...,  2.7413e-01,\n",
            "            -7.5867e-02, -1.4514e-01],\n",
            "           [-6.1622e-03,  3.2133e-02, -6.8989e-01,  ..., -5.2233e-01,\n",
            "            -2.8328e-01, -1.8829e-01],\n",
            "           [ 4.7629e-02, -9.2601e-02, -5.0672e-02,  ...,  6.7633e-02,\n",
            "            -3.3871e-02,  5.3559e-02]]],\n",
            "\n",
            "\n",
            "         [[[ 3.2668e-01, -1.1853e-01, -3.3201e-01,  ..., -6.0190e-01,\n",
            "            -6.3399e-01, -1.2756e-01],\n",
            "           [-4.0505e-02, -7.3105e-01,  2.8197e-02,  ..., -1.4788e-02,\n",
            "            -5.6190e-01,  3.3830e-01],\n",
            "           [-2.4590e-01, -1.2764e-01, -5.7130e-02,  ..., -3.3327e-01,\n",
            "            -7.7387e-01,  5.2606e-01],\n",
            "           ...,\n",
            "           [-1.9938e-01, -5.3298e-01, -9.8410e-02,  ...,  5.2449e-02,\n",
            "            -4.8353e-01,  1.6772e-01],\n",
            "           [-3.1590e-01, -5.1598e-01, -1.1848e+00,  ..., -6.3476e-01,\n",
            "            -8.2116e-01,  5.8638e-02],\n",
            "           [-3.6698e-01, -6.3817e-02, -3.4402e-01,  ..., -6.5400e-01,\n",
            "            -3.9264e-01, -4.4840e-01]],\n",
            "\n",
            "          [[ 1.0575e-01, -9.1863e-01, -4.3137e-01,  ...,  1.0523e-01,\n",
            "            -6.1204e-01,  1.1240e-01],\n",
            "           [ 6.9240e-01,  4.5238e-01, -9.4493e-02,  ...,  3.1666e-01,\n",
            "             1.9643e-01,  6.4125e-01],\n",
            "           [ 4.1088e-01,  5.4336e-01,  5.4711e-01,  ...,  3.2522e-01,\n",
            "             1.7820e-01,  3.8001e-01],\n",
            "           ...,\n",
            "           [ 2.4431e-01,  2.4218e-01, -2.5340e-01,  ...,  4.0954e-01,\n",
            "             1.6028e-02,  3.4746e-01],\n",
            "           [ 1.3783e-01, -9.9328e-01, -2.3200e-02,  ..., -3.2137e-01,\n",
            "            -5.1242e-01, -5.2194e-01],\n",
            "           [ 3.8420e-02,  6.5503e-01,  4.8436e-01,  ...,  2.5159e-01,\n",
            "            -1.2180e-01,  4.8383e-01]],\n",
            "\n",
            "          [[-1.2252e-01, -6.8160e-01, -5.2870e-01,  ..., -3.2723e-01,\n",
            "            -5.6515e-01, -3.6954e-01],\n",
            "           [ 2.5993e-01, -3.6368e-01,  7.7634e-01,  ..., -5.8003e-01,\n",
            "             7.1290e-01,  1.1899e-01],\n",
            "           [ 3.3523e-01,  3.9560e-01,  4.7627e-01,  ..., -4.3579e-01,\n",
            "             1.9690e-01,  7.3566e-02],\n",
            "           ...,\n",
            "           [-2.7526e-01,  2.2848e-01,  3.1442e-01,  ..., -2.0654e-01,\n",
            "            -1.8136e-01,  3.2332e-01],\n",
            "           [-1.2050e-01, -2.8076e-01,  1.9651e-01,  ..., -6.9284e-01,\n",
            "            -4.4594e-01, -2.8790e-01],\n",
            "           [ 3.7502e-01,  5.1175e-01,  3.5042e-01,  ...,  4.9897e-01,\n",
            "            -2.3854e-01,  6.7324e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-5.2232e-01, -4.1683e-02, -5.9436e-01,  ..., -1.3726e-02,\n",
            "            -3.5793e-01,  5.0641e-01],\n",
            "           [ 2.2671e-01,  5.1909e-01,  4.3801e-01,  ...,  4.2683e-01,\n",
            "            -1.7162e-01,  1.3895e-01],\n",
            "           [ 3.5590e-01,  2.8144e-02,  7.3281e-01,  ...,  1.4790e-01,\n",
            "             2.2259e-01, -3.2696e-01],\n",
            "           ...,\n",
            "           [ 6.5349e-01, -4.3788e-01,  1.9412e-01,  ...,  5.4634e-01,\n",
            "             4.2164e-01,  1.6026e-01],\n",
            "           [ 1.7318e-01, -1.5257e+00, -2.9674e-01,  ..., -1.0609e+00,\n",
            "             5.4360e-02,  2.1554e-01],\n",
            "           [ 3.7010e-01,  7.9463e-01, -3.3826e-01,  ..., -1.8099e-01,\n",
            "             1.1045e-01,  3.6283e-01]],\n",
            "\n",
            "          [[ 3.1563e-01, -1.2061e-01, -4.8974e-01,  ...,  1.0272e-02,\n",
            "            -1.8563e-01,  1.2976e-01],\n",
            "           [ 1.7588e-01,  6.5791e-01,  7.4890e-01,  ..., -9.7238e-02,\n",
            "             3.3533e-01,  5.9198e-01],\n",
            "           [ 3.4849e-01, -4.5493e-02,  1.2728e+00,  ...,  4.4388e-01,\n",
            "             2.5225e-01,  1.7520e-01],\n",
            "           ...,\n",
            "           [ 1.2900e+00,  4.1941e-01,  8.8535e-01,  ...,  1.0990e+00,\n",
            "             7.1829e-01,  7.0430e-01],\n",
            "           [ 7.6608e-02, -3.4415e-01,  2.5982e-02,  ..., -6.5729e-01,\n",
            "            -5.7383e-02,  2.8081e-01],\n",
            "           [ 5.6509e-01,  8.5934e-01,  7.1360e-01,  ...,  1.0870e+00,\n",
            "             5.7861e-01,  7.3952e-01]],\n",
            "\n",
            "          [[-4.1517e-01, -4.5952e-01, -7.8361e-01,  ..., -1.1386e-01,\n",
            "            -4.3528e-01, -3.1856e-01],\n",
            "           [ 2.0830e-01, -2.6249e-02,  2.9007e-01,  ..., -2.2317e-01,\n",
            "             1.1912e-01,  2.6378e-01],\n",
            "           [-1.4094e-02, -1.0789e-01,  4.1564e-01,  ..., -3.0936e-01,\n",
            "             4.2098e-02,  3.7611e-02],\n",
            "           ...,\n",
            "           [ 9.7530e-02,  3.2093e-01,  2.3034e-01,  ..., -3.6336e-01,\n",
            "             3.1332e-01,  2.5289e-02],\n",
            "           [ 2.1515e-01,  3.0079e-01, -1.7676e-01,  ..., -1.6024e-01,\n",
            "            -1.1148e-01, -9.5580e-02],\n",
            "           [ 7.8455e-01,  9.2217e-01,  3.3517e-01,  ...,  9.0659e-01,\n",
            "             7.3114e-01,  7.5379e-01]]],\n",
            "\n",
            "\n",
            "         [[[ 6.5525e-02,  3.0471e-01, -9.3366e-02,  ...,  3.3883e-01,\n",
            "            -3.0297e-01,  3.1603e-01],\n",
            "           [ 2.3191e-01,  1.2439e-01,  4.9634e-01,  ...,  6.2554e-01,\n",
            "             4.2371e-01,  3.7676e-01],\n",
            "           [ 8.3178e-01,  4.3876e-01,  1.9499e-01,  ...,  9.4838e-02,\n",
            "             1.0004e-01,  1.0026e+00],\n",
            "           ...,\n",
            "           [ 7.0027e-01, -1.5372e-01,  1.9624e-01,  ...,  1.9589e-01,\n",
            "            -1.8304e-01,  1.7632e-01],\n",
            "           [ 7.1940e-01,  8.3353e-01,  6.9916e-01,  ...,  8.3718e-01,\n",
            "             6.0851e-01,  5.9157e-01],\n",
            "           [ 5.9246e-01,  4.0073e-01,  6.2306e-01,  ...,  1.0797e-01,\n",
            "             4.1687e-01,  1.9434e-01]],\n",
            "\n",
            "          [[-1.6669e-01, -4.2008e-01, -7.0998e-01,  ..., -2.6305e-01,\n",
            "            -7.4351e-01,  2.8075e-01],\n",
            "           [-1.3060e-01,  3.3945e-01, -4.5157e-01,  ...,  4.1930e-01,\n",
            "             1.1021e-01,  3.0798e-01],\n",
            "           [ 3.2321e-01,  3.7653e-01,  1.0957e-01,  ..., -2.3449e-01,\n",
            "            -9.0597e-02,  9.9750e-02],\n",
            "           ...,\n",
            "           [ 1.3220e-01,  1.1948e-01, -3.3038e-01,  ...,  6.4293e-02,\n",
            "             5.2173e-02,  8.3930e-01],\n",
            "           [ 3.1408e-02,  2.8762e-01, -6.0794e-02,  ...,  5.5019e-01,\n",
            "             5.5781e-01,  6.4956e-01],\n",
            "           [ 2.5050e-01,  2.7420e-01,  1.2143e-01,  ...,  1.3178e-01,\n",
            "             4.1786e-01,  4.2987e-01]],\n",
            "\n",
            "          [[-2.7371e-01, -1.5539e-01, -5.5930e-01,  ..., -4.6249e-01,\n",
            "             8.1438e-02,  5.3499e-01],\n",
            "           [ 7.8532e-02, -1.5547e-01,  4.3503e-01,  ..., -4.5000e-02,\n",
            "             5.0629e-01,  2.3021e-01],\n",
            "           [ 3.8590e-01,  3.0921e-01,  4.2191e-01,  ...,  7.1464e-02,\n",
            "             1.0891e-02,  4.9302e-01],\n",
            "           ...,\n",
            "           [ 2.6918e-01,  4.8892e-01,  3.7289e-01,  ..., -4.8395e-01,\n",
            "             2.8418e-02,  4.2189e-01],\n",
            "           [ 1.8238e-01,  2.4383e-02,  4.7532e-01,  ..., -1.2464e-01,\n",
            "             3.0199e-01,  7.3829e-01],\n",
            "           [ 6.1543e-01,  3.8999e-01,  4.9787e-01,  ...,  1.2899e-01,\n",
            "             4.2659e-01,  5.4992e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-6.3892e-01, -1.5008e-01, -5.6256e-01,  ..., -3.8424e-01,\n",
            "            -3.9264e-01,  3.2970e-01],\n",
            "           [ 2.7325e-02,  3.3161e-01,  2.0463e-01,  ...,  4.4762e-01,\n",
            "            -1.4666e-02,  1.6531e-01],\n",
            "           [-6.9473e-02, -4.0479e-01,  8.1965e-01,  ...,  2.0934e-01,\n",
            "             2.9947e-01,  4.0936e-01],\n",
            "           ...,\n",
            "           [ 2.3587e-02, -2.6840e-01,  1.1780e-01,  ...,  3.5947e-01,\n",
            "             4.5734e-01,  3.4419e-01],\n",
            "           [ 2.1991e-01, -2.2185e-01, -3.0476e-02,  ...,  3.0076e-01,\n",
            "             3.3297e-01,  5.7041e-01],\n",
            "           [ 4.2295e-01,  3.0893e-01,  1.5441e-01,  ...,  2.6144e-01,\n",
            "            -1.2935e-01,  5.5916e-01]],\n",
            "\n",
            "          [[-1.0073e-01, -1.1515e-01, -1.2841e-01,  ..., -3.3000e-01,\n",
            "            -4.8964e-01,  4.1542e-01],\n",
            "           [ 5.9710e-02,  4.8640e-01,  1.0605e-01,  ...,  9.6162e-02,\n",
            "            -1.5958e-01,  1.9697e-01],\n",
            "           [ 2.9513e-01,  3.6280e-01,  6.5082e-01,  ...,  5.6386e-01,\n",
            "             1.3706e-01,  4.9771e-01],\n",
            "           ...,\n",
            "           [ 9.0546e-01,  1.6216e-01,  4.7953e-01,  ...,  5.5042e-01,\n",
            "             4.8871e-02,  5.2240e-01],\n",
            "           [ 4.5446e-01, -2.6583e-01, -2.7210e-01,  ...,  3.3700e-01,\n",
            "             3.5203e-01,  6.2689e-01],\n",
            "           [ 4.0458e-01,  3.4332e-01,  8.0110e-01,  ...,  8.0090e-01,\n",
            "             7.9393e-01,  4.8945e-01]],\n",
            "\n",
            "          [[-3.9390e-01, -7.2152e-01, -8.7116e-01,  ..., -4.5160e-01,\n",
            "            -5.0297e-01, -2.1167e-01],\n",
            "           [ 7.1012e-02,  2.7738e-01, -9.3900e-02,  ...,  1.8772e-02,\n",
            "            -2.4729e-02, -1.0190e-01],\n",
            "           [ 3.2131e-01, -1.9498e-01, -1.1655e-01,  ..., -2.1655e-01,\n",
            "            -1.3026e-01, -1.2543e-01],\n",
            "           ...,\n",
            "           [-8.7848e-02,  1.3483e-01,  1.8636e-02,  ..., -7.3322e-02,\n",
            "            -1.0547e-01, -1.3577e-01],\n",
            "           [-4.2166e-01, -1.2219e-01, -5.9169e-01,  ..., -4.6295e-01,\n",
            "            -9.0205e-02,  3.3410e-01],\n",
            "           [ 5.3648e-01,  3.9081e-01,  1.9807e-01,  ...,  5.3812e-01,\n",
            "             6.3091e-01,  3.8298e-01]]],\n",
            "\n",
            "\n",
            "         [[[ 1.1072e-01,  6.5780e-01,  8.8133e-01,  ...,  6.4820e-01,\n",
            "             9.8427e-01,  4.8377e-01],\n",
            "           [ 1.4043e-01,  8.2624e-01,  3.2015e-01,  ...,  1.9564e-01,\n",
            "             4.4651e-01,  3.1877e-02],\n",
            "           [ 3.2690e-01,  6.3912e-01, -3.5460e-02,  ...,  1.0486e-01,\n",
            "             7.5611e-01,  2.8712e-01],\n",
            "           ...,\n",
            "           [-1.5286e-01,  6.1394e-01,  8.2224e-01,  ...,  1.5648e-01,\n",
            "             4.8068e-01,  1.9528e-01],\n",
            "           [ 3.9597e-01,  3.8145e-01,  7.5070e-01,  ...,  4.7907e-01,\n",
            "             7.2110e-01, -3.0987e-02],\n",
            "           [ 8.0521e-01,  1.5155e-01,  3.0019e-01,  ...,  6.0843e-01,\n",
            "             7.5550e-01,  3.8929e-01]],\n",
            "\n",
            "          [[ 2.6699e-01,  1.1089e+00,  6.5827e-01,  ...,  5.1296e-01,\n",
            "             1.2989e+00,  4.6734e-01],\n",
            "           [-1.3742e-01,  2.0835e-01,  5.8631e-01,  ..., -2.1819e-01,\n",
            "             1.8395e-01, -2.0796e-01],\n",
            "           [-4.7080e-01, -4.1495e-01,  2.7467e-01,  ...,  4.7428e-01,\n",
            "             4.2490e-02,  2.5770e-02],\n",
            "           ...,\n",
            "           [ 5.7673e-01,  3.5503e-02,  4.7369e-01,  ...,  2.4414e-01,\n",
            "             3.0439e-01, -4.8609e-01],\n",
            "           [ 1.4961e-01,  8.4689e-01,  2.8222e-01,  ...,  5.9412e-01,\n",
            "             6.5537e-01,  2.7416e-01],\n",
            "           [ 6.6434e-02, -9.3482e-02, -2.3676e-01,  ...,  3.1286e-01,\n",
            "             2.5057e-01,  1.3290e-01]],\n",
            "\n",
            "          [[ 3.1767e-01,  1.1383e+00,  9.1012e-01,  ...,  6.4547e-01,\n",
            "             4.7024e-01,  2.7947e-01],\n",
            "           [ 3.4054e-01,  8.6341e-02, -5.6706e-04,  ...,  1.0809e-01,\n",
            "             2.1363e-01,  2.5503e-01],\n",
            "           [ 4.2487e-01,  6.1435e-01,  1.3848e-01,  ...,  6.5008e-01,\n",
            "             7.9441e-01,  4.3482e-01],\n",
            "           ...,\n",
            "           [ 1.5334e-01, -2.2561e-01,  5.8910e-01,  ...,  9.2834e-01,\n",
            "             9.0856e-02, -2.1559e-01],\n",
            "           [ 3.9123e-01,  7.6695e-01,  1.1523e+00,  ...,  4.0503e-01,\n",
            "             1.0879e-01,  2.2825e-01],\n",
            "           [ 8.1278e-02,  2.4076e-01,  4.2555e-01,  ...,  5.0879e-01,\n",
            "             2.6783e-01, -3.1113e-02]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 9.6370e-01,  4.7093e-01,  1.1033e+00,  ...,  4.8037e-01,\n",
            "             7.5750e-01, -1.1071e-01],\n",
            "           [ 1.8163e-01, -4.4722e-01,  3.5287e-01,  ...,  2.7380e-01,\n",
            "             2.1755e-01,  3.0608e-01],\n",
            "           [-9.1875e-02,  6.6163e-01, -1.3064e-01,  ...,  4.8111e-02,\n",
            "             1.0707e-01,  3.1319e-01],\n",
            "           ...,\n",
            "           [ 1.1512e-01,  7.0450e-01,  1.6414e-01,  ...,  1.6671e-01,\n",
            "             3.2729e-01,  6.3917e-01],\n",
            "           [ 3.7024e-01,  1.3678e+00,  8.4015e-01,  ...,  6.0894e-01,\n",
            "             7.6000e-01,  1.2359e-01],\n",
            "           [-1.6674e-02,  4.0411e-01,  1.5223e-01,  ...,  3.3926e-01,\n",
            "             2.3620e-01,  1.1755e-01]],\n",
            "\n",
            "          [[ 4.0723e-01,  8.0787e-01,  7.7310e-01,  ...,  6.5280e-01,\n",
            "             6.6264e-01,  8.2086e-02],\n",
            "           [-2.1347e-01,  1.7245e-01,  1.3132e-01,  ...,  1.1449e-01,\n",
            "             5.7875e-01, -3.1019e-01],\n",
            "           [ 1.9246e-01,  3.0137e-01,  3.6155e-01,  ...,  2.4749e-01,\n",
            "             8.0361e-01,  1.5136e-01],\n",
            "           ...,\n",
            "           [-3.3577e-01,  3.6877e-01, -1.9172e-01,  ..., -9.9765e-02,\n",
            "             1.8061e-01, -3.7392e-02],\n",
            "           [ 5.6379e-01,  8.0598e-01,  7.5839e-01,  ...,  2.0208e-01,\n",
            "             5.8592e-01, -2.9734e-01],\n",
            "           [ 8.5109e-02, -2.2400e-01,  8.6136e-03,  ...,  4.1643e-01,\n",
            "             6.7597e-01, -7.3309e-02]],\n",
            "\n",
            "          [[ 8.5728e-01,  9.7131e-01,  9.4030e-01,  ...,  8.4157e-01,\n",
            "             5.3857e-01,  3.1699e-01],\n",
            "           [ 3.4030e-01,  5.3109e-01,  3.2911e-01,  ...,  8.3602e-01,\n",
            "             3.0209e-01,  2.1055e-01],\n",
            "           [ 4.1883e-01,  5.0697e-01,  3.1604e-01,  ...,  7.8687e-01,\n",
            "             5.9821e-01,  1.3441e-01],\n",
            "           ...,\n",
            "           [ 2.5881e-01,  3.7767e-01,  4.7890e-02,  ...,  5.9295e-01,\n",
            "             6.2294e-02,  3.4275e-01],\n",
            "           [ 4.3355e-01,  7.9796e-01,  7.8135e-01,  ...,  5.9252e-01,\n",
            "             2.1474e-02,  8.4783e-02],\n",
            "           [-3.1459e-01,  1.8505e-01,  2.0511e-01,  ...,  2.1733e-01,\n",
            "            -4.1897e-03,  1.1177e-01]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 2.9039e-01,  3.3474e-01,  1.7614e-01,  ..., -1.8165e-01,\n",
            "            -6.8135e-02,  2.7353e-01],\n",
            "           [-1.0910e-01,  6.6059e-02, -2.7967e-01,  ..., -7.6811e-02,\n",
            "             9.0410e-02,  6.4673e-02],\n",
            "           [ 2.7789e-01,  3.3568e-02, -8.3219e-03,  ..., -5.6242e-01,\n",
            "            -2.1465e-01,  1.8072e-01],\n",
            "           ...,\n",
            "           [-3.8148e-01, -1.3781e-01,  6.2613e-03,  ...,  3.4572e-01,\n",
            "            -9.6801e-02, -3.7251e-01],\n",
            "           [-3.9262e-02, -3.1994e-03, -3.9002e-01,  ..., -2.2699e-01,\n",
            "            -4.1565e-01, -2.7233e-01],\n",
            "           [ 3.2651e-02,  6.8470e-02, -4.5147e-01,  ..., -8.3797e-02,\n",
            "            -2.8076e-01, -1.8510e-01]],\n",
            "\n",
            "          [[ 4.3805e-01,  3.2962e-01, -5.2204e-01,  ..., -1.7051e-01,\n",
            "            -1.1113e-01, -2.4187e-01],\n",
            "           [-8.9180e-02, -8.6183e-02, -2.5596e-03,  ...,  1.4757e-01,\n",
            "             2.9923e-02, -1.5617e-01],\n",
            "           [ 3.3910e-01,  2.9018e-01,  2.6243e-01,  ..., -3.0741e-01,\n",
            "            -2.4578e-01, -2.2942e-01],\n",
            "           ...,\n",
            "           [ 1.0000e-01,  8.5747e-02,  2.7005e-02,  ...,  1.8970e-01,\n",
            "            -5.7901e-01,  6.1882e-02],\n",
            "           [-2.2753e-01, -6.2433e-01, -7.8500e-01,  ..., -5.3062e-01,\n",
            "            -7.0563e-01,  3.8093e-02],\n",
            "           [-1.4739e-01, -2.4930e-02, -7.2356e-02,  ..., -5.4359e-02,\n",
            "            -4.3648e-01,  9.4210e-02]],\n",
            "\n",
            "          [[ 2.1796e-01,  1.9169e-01, -2.1751e-01,  ..., -3.0423e-01,\n",
            "             1.6987e-02, -2.0176e-02],\n",
            "           [ 2.6318e-01,  8.0301e-01,  5.5759e-01,  ..., -5.7094e-01,\n",
            "            -2.6661e-01, -2.8191e-01],\n",
            "           [-2.4376e-01,  5.5482e-02, -3.4315e-01,  ...,  1.7816e-01,\n",
            "             2.0110e-01, -7.7349e-02],\n",
            "           ...,\n",
            "           [-1.6651e-01, -1.7874e-01, -1.3079e-01,  ...,  1.1979e-01,\n",
            "            -3.9114e-01, -1.6866e-01],\n",
            "           [-1.8773e-01, -8.7401e-01, -7.2353e-01,  ..., -6.3716e-01,\n",
            "            -3.8267e-01, -4.0576e-01],\n",
            "           [-3.0031e-01, -5.5155e-01, -5.8119e-01,  ..., -3.9872e-01,\n",
            "            -1.5456e-01, -2.3744e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 4.6205e-02,  2.6236e-01,  3.5309e-02,  ...,  2.0419e-01,\n",
            "            -7.0966e-02, -1.1127e-01],\n",
            "           [-3.3093e-01,  2.8459e-01, -3.0340e-02,  ..., -1.5378e-01,\n",
            "            -1.8717e-02,  1.4351e-01],\n",
            "           [ 1.8748e-01,  2.5624e-01,  2.0304e-01,  ..., -2.5771e-01,\n",
            "             9.4763e-02, -4.3057e-01],\n",
            "           ...,\n",
            "           [ 5.8979e-02, -4.4809e-02, -1.4925e-01,  ..., -1.0385e-01,\n",
            "             4.5785e-02, -4.1690e-02],\n",
            "           [-4.2086e-01, -3.7837e-01, -5.4048e-01,  ..., -8.1103e-01,\n",
            "            -5.1018e-01, -2.9956e-01],\n",
            "           [ 8.1282e-02, -4.6633e-01, -4.9195e-01,  ..., -2.1768e-01,\n",
            "             9.2387e-02, -9.4545e-02]],\n",
            "\n",
            "          [[ 2.0383e-01,  2.1297e-03,  4.9972e-02,  ...,  1.4447e-01,\n",
            "            -9.9267e-02, -1.0857e-01],\n",
            "           [ 1.1800e-01,  7.7937e-01,  6.0402e-02,  ..., -1.9511e-01,\n",
            "            -4.1362e-01, -2.6015e-01],\n",
            "           [-3.2657e-01, -3.5280e-01,  3.0961e-01,  ...,  1.0942e-02,\n",
            "            -1.8919e-01, -2.2512e-01],\n",
            "           ...,\n",
            "           [ 2.8827e-01,  8.2066e-02,  1.4725e-01,  ..., -5.9820e-01,\n",
            "            -6.5134e-01, -1.1469e-02],\n",
            "           [-3.3600e-01, -1.2951e-01, -2.7397e-01,  ..., -3.2329e-01,\n",
            "            -2.3789e-01, -4.3625e-01],\n",
            "           [-1.6082e-01, -4.3017e-01, -1.6125e-01,  ..., -2.4788e-01,\n",
            "            -2.4371e-01, -2.2421e-01]],\n",
            "\n",
            "          [[ 2.3426e-01,  1.6032e-01, -4.6749e-01,  ..., -3.3936e-01,\n",
            "             1.5793e-01, -9.0318e-02],\n",
            "           [-2.1768e-02,  1.7634e-01, -1.0315e-01,  ..., -3.2562e-02,\n",
            "             1.0532e-02,  2.7757e-02],\n",
            "           [ 2.1391e-02, -8.5629e-02, -1.3935e-01,  ..., -3.7263e-02,\n",
            "             6.1604e-03, -5.7150e-02],\n",
            "           ...,\n",
            "           [ 2.2267e-01, -3.2575e-01, -5.7129e-01,  ..., -6.8385e-02,\n",
            "            -1.1135e-01,  3.1009e-02],\n",
            "           [ 2.2038e-01,  6.1835e-02, -1.6801e-01,  ..., -5.1334e-01,\n",
            "            -1.4412e-01, -7.7252e-02],\n",
            "           [ 1.1703e-01, -3.7469e-01, -2.5049e-02,  ..., -8.2352e-02,\n",
            "            -2.4666e-02,  5.7863e-02]]],\n",
            "\n",
            "\n",
            "         [[[ 1.1704e-01, -4.9553e-01, -2.3206e-01,  ..., -2.6806e-01,\n",
            "            -5.0957e-01,  4.5270e-01],\n",
            "           [-3.9602e-01, -6.7929e-01, -1.6147e-01,  ..., -1.6276e-01,\n",
            "             2.7626e-01, -3.2276e-01],\n",
            "           [-4.1040e-01,  1.8063e-01, -2.7173e-01,  ..., -3.7934e-02,\n",
            "            -2.1923e-01,  4.0017e-01],\n",
            "           ...,\n",
            "           [-2.4251e-01, -3.1535e-01,  5.5033e-01,  ...,  1.9756e-01,\n",
            "             2.0957e-01,  4.6359e-01],\n",
            "           [-8.6060e-01, -1.0820e+00, -1.2026e+00,  ..., -4.2006e-01,\n",
            "            -5.0509e-01,  2.0530e-01],\n",
            "           [-6.4029e-01,  2.2305e-02, -4.9535e-01,  ..., -3.0269e-01,\n",
            "            -6.7175e-01, -3.4576e-01]],\n",
            "\n",
            "          [[ 3.7904e-02, -6.6730e-01, -2.3656e-01,  ..., -4.6129e-01,\n",
            "            -3.7158e-01,  1.8860e-01],\n",
            "           [ 4.2298e-01,  1.8434e-02,  2.1042e-01,  ...,  4.1555e-01,\n",
            "            -3.4314e-01,  1.7369e-01],\n",
            "           [ 3.3671e-01, -3.7142e-03,  2.8512e-01,  ...,  5.2466e-01,\n",
            "            -3.8039e-01,  3.0452e-01],\n",
            "           ...,\n",
            "           [ 9.5984e-01, -5.6718e-01, -2.3390e-01,  ...,  4.4134e-01,\n",
            "            -1.0594e-01,  2.9486e-01],\n",
            "           [-3.6170e-01, -6.0707e-01, -1.1174e+00,  ..., -7.2094e-01,\n",
            "            -1.5680e+00,  5.5997e-01],\n",
            "           [ 1.5043e-01,  2.8029e-01,  3.4470e-01,  ...,  4.7186e-01,\n",
            "            -3.4098e-01,  1.7019e-01]],\n",
            "\n",
            "          [[ 3.0638e-02, -5.9946e-01, -8.0414e-01,  ..., -4.4723e-01,\n",
            "             2.6303e-02,  3.4027e-01],\n",
            "           [ 5.4052e-02, -7.0623e-01, -2.2748e-01,  ...,  5.3900e-01,\n",
            "            -5.8346e-01, -2.3278e-01],\n",
            "           [-1.2013e-01,  7.2953e-01, -8.1157e-02,  ..., -4.7725e-03,\n",
            "            -1.8311e-01,  6.1236e-01],\n",
            "           ...,\n",
            "           [ 7.8204e-01, -8.2472e-02,  5.8522e-01,  ...,  4.3761e-01,\n",
            "             5.5399e-01,  6.7365e-01],\n",
            "           [-4.5200e-01, -6.2732e-01, -6.6322e-01,  ..., -2.7812e-02,\n",
            "            -5.5575e-01,  3.3287e-01],\n",
            "           [ 2.8601e-01,  9.6526e-01,  4.4599e-01,  ...,  6.7400e-01,\n",
            "            -2.3814e-01,  4.2229e-02]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 1.8872e-01, -2.5419e-01, -6.2233e-02,  ..., -1.0402e-01,\n",
            "            -7.3089e-01,  4.0950e-01],\n",
            "           [ 3.0638e-01,  1.6665e-01,  2.5963e-01,  ...,  6.0677e-01,\n",
            "             2.2937e-01,  4.5695e-01],\n",
            "           [ 4.2021e-01,  1.5778e-01,  4.9075e-01,  ...,  1.9889e-02,\n",
            "             4.0432e-01,  2.6938e-01],\n",
            "           ...,\n",
            "           [ 1.9885e-01,  6.1774e-01,  4.3508e-01,  ...,  4.6554e-01,\n",
            "             1.1738e-01,  8.2023e-01],\n",
            "           [ 8.2598e-02, -4.8196e-01,  2.8205e-01,  ..., -4.1540e-01,\n",
            "            -2.8820e-01,  1.4250e-01],\n",
            "           [ 2.9983e-01,  2.0158e-01,  3.4200e-01,  ...,  3.3526e-01,\n",
            "             7.5351e-01, -8.9135e-02]],\n",
            "\n",
            "          [[-2.0249e-01, -6.5161e-01,  1.4632e-01,  ..., -1.3014e-01,\n",
            "            -5.1004e-01,  1.9513e-01],\n",
            "           [-8.2592e-02,  1.7317e-01,  6.2855e-01,  ...,  5.6516e-01,\n",
            "            -7.1042e-02,  4.7667e-01],\n",
            "           [ 1.1537e+00,  8.1924e-01,  1.0115e+00,  ...,  6.7538e-01,\n",
            "             1.1183e+00,  4.2867e-01],\n",
            "           ...,\n",
            "           [ 8.9058e-01,  7.2708e-01,  6.1307e-01,  ...,  1.3852e+00,\n",
            "             2.3604e-01,  5.7171e-01],\n",
            "           [ 1.4216e-01,  3.5059e-01,  6.8057e-01,  ...,  5.0582e-01,\n",
            "             5.5698e-01,  7.0497e-02],\n",
            "           [ 5.7079e-01,  2.5268e-01,  5.9068e-01,  ...,  7.1919e-01,\n",
            "             5.1089e-01,  2.7735e-01]],\n",
            "\n",
            "          [[-5.5111e-01, -5.1568e-01, -6.7445e-01,  ..., -6.6678e-01,\n",
            "            -2.9067e-01, -2.3530e-01],\n",
            "           [-4.3161e-01,  6.8850e-01, -6.1773e-02,  ..., -1.2946e-01,\n",
            "             2.8214e-01,  2.5411e-01],\n",
            "           [ 3.2628e-01,  4.2612e-02,  1.4326e-01,  ..., -4.4405e-01,\n",
            "             4.6841e-01,  4.5979e-02],\n",
            "           ...,\n",
            "           [ 7.9027e-02, -3.3928e-01, -4.2470e-01,  ..., -1.7192e-01,\n",
            "             7.7006e-02,  1.4686e-01],\n",
            "           [ 3.5601e-01,  1.3390e-01,  4.9412e-02,  ..., -7.5866e-02,\n",
            "             1.2152e-01, -4.8483e-02],\n",
            "           [ 9.5385e-01,  4.7307e-01,  7.4272e-01,  ...,  3.8298e-01,\n",
            "             6.3929e-01,  6.7066e-01]]],\n",
            "\n",
            "\n",
            "         [[[ 2.5031e-01,  2.9952e-01, -3.2134e-01,  ..., -3.3406e-01,\n",
            "            -3.6205e-01,  2.4380e-01],\n",
            "           [ 5.4713e-01,  3.6250e-01, -3.5471e-02,  ...,  5.0214e-01,\n",
            "             1.4080e-01,  1.7410e-01],\n",
            "           [ 4.6232e-01,  4.6380e-01,  2.3300e-01,  ...,  9.3139e-02,\n",
            "            -2.8369e-01,  4.6738e-01],\n",
            "           ...,\n",
            "           [ 5.6272e-02,  4.1970e-01,  4.7997e-01,  ..., -7.0574e-02,\n",
            "             6.6873e-03,  4.9182e-01],\n",
            "           [ 3.6510e-01,  1.6045e-01,  5.2505e-01,  ...,  4.8662e-01,\n",
            "             8.4733e-01,  7.7844e-01],\n",
            "           [ 9.0116e-02,  6.5156e-01,  2.6760e-01,  ...,  2.3592e-01,\n",
            "             6.2994e-01, -1.3945e-02]],\n",
            "\n",
            "          [[-2.0912e-01, -4.9419e-01, -4.5472e-01,  ..., -8.0120e-01,\n",
            "            -1.8941e-01,  5.8194e-03],\n",
            "           [ 2.5977e-04,  1.0949e-01,  6.0444e-01,  ...,  4.6046e-01,\n",
            "            -3.3780e-02,  5.6508e-01],\n",
            "           [-2.0097e-01, -3.2386e-01, -2.8450e-01,  ..., -2.7462e-02,\n",
            "            -7.2691e-01,  2.2460e-01],\n",
            "           ...,\n",
            "           [ 3.1910e-01,  1.3908e-02,  3.0990e-01,  ...,  3.7824e-01,\n",
            "            -1.2670e-01,  6.4904e-01],\n",
            "           [ 8.1138e-01,  1.5575e-01, -4.7052e-01,  ...,  4.3596e-01,\n",
            "             1.0246e-01,  4.8888e-01],\n",
            "           [ 2.6134e-01,  1.6479e-01,  3.7169e-02,  ...,  4.4314e-01,\n",
            "             1.3673e-01,  1.7310e-01]],\n",
            "\n",
            "          [[-6.0033e-01, -3.7629e-01, -5.3516e-01,  ..., -3.7719e-01,\n",
            "            -1.9797e-01,  1.1135e-01],\n",
            "           [-6.3280e-02, -4.2101e-02, -5.7748e-02,  ..., -2.0777e-01,\n",
            "             3.7086e-02,  4.2173e-01],\n",
            "           [ 4.7163e-01,  3.4449e-01,  4.1338e-01,  ..., -8.3496e-02,\n",
            "             8.5185e-03,  7.6139e-01],\n",
            "           ...,\n",
            "           [ 2.4621e-01,  5.2349e-01,  7.5527e-01,  ...,  2.7039e-01,\n",
            "             3.9873e-01,  3.1378e-01],\n",
            "           [ 3.4608e-01, -1.3760e-01,  4.8072e-01,  ...,  7.8429e-01,\n",
            "             3.5098e-01,  5.9524e-01],\n",
            "           [ 7.5682e-01,  3.3227e-01,  1.1474e+00,  ...,  1.6082e-01,\n",
            "             1.3758e-01,  1.5972e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-4.1972e-01, -3.5669e-01, -1.0852e-01,  ..., -1.9412e-01,\n",
            "            -1.3064e-01,  4.2586e-01],\n",
            "           [ 1.4668e-01,  1.0930e+00,  6.4842e-01,  ..., -8.4724e-03,\n",
            "             4.3533e-01,  4.5233e-01],\n",
            "           [ 4.7433e-01, -1.9735e-01,  1.9233e-01,  ..., -6.7355e-02,\n",
            "             1.6744e-01,  3.6098e-01],\n",
            "           ...,\n",
            "           [-1.6864e-01,  3.8594e-01,  3.2594e-01,  ...,  9.5069e-02,\n",
            "             2.3977e-02,  5.3893e-01],\n",
            "           [ 9.0443e-02,  2.4168e-01,  1.4032e-01,  ...,  9.9511e-01,\n",
            "             7.8010e-01,  7.5150e-01],\n",
            "           [ 3.0996e-01,  1.3662e-01,  1.3986e-01,  ...,  4.8583e-01,\n",
            "             3.9083e-01,  4.9669e-01]],\n",
            "\n",
            "          [[-1.3934e-01, -6.1480e-01,  2.1060e-01,  ...,  5.9904e-04,\n",
            "            -6.5603e-01,  9.6756e-02],\n",
            "           [-4.4605e-02,  1.8943e-01,  6.7506e-01,  ...,  4.4617e-01,\n",
            "            -1.0168e-01,  3.0536e-01],\n",
            "           [ 2.3383e-01,  3.4342e-01,  7.8296e-01,  ...,  3.7114e-01,\n",
            "             7.7144e-01,  5.3336e-01],\n",
            "           ...,\n",
            "           [-1.4340e-02,  1.8065e-01,  1.5055e-01,  ...,  1.0076e+00,\n",
            "            -2.0809e-02,  4.5308e-01],\n",
            "           [ 6.9688e-01,  1.8562e-01,  7.0004e-01,  ...,  7.2425e-01,\n",
            "             1.0982e+00,  2.6220e-01],\n",
            "           [ 5.3528e-01,  2.1936e-01,  1.1038e+00,  ...,  6.8359e-01,\n",
            "             3.6976e-01,  5.2437e-01]],\n",
            "\n",
            "          [[-4.9264e-01, -3.9420e-01, -8.4883e-01,  ..., -8.9456e-01,\n",
            "            -7.2863e-01, -1.5387e-01],\n",
            "           [ 1.7761e-01,  3.7775e-01,  2.8528e-01,  ..., -3.6971e-01,\n",
            "            -2.1794e-01,  1.1874e-01],\n",
            "           [ 9.2294e-02,  2.3930e-02, -4.1084e-01,  ..., -4.2234e-01,\n",
            "            -9.0710e-02,  1.4514e-02],\n",
            "           ...,\n",
            "           [ 2.1136e-01, -2.3304e-01, -3.7429e-01,  ...,  1.3742e-02,\n",
            "            -1.0910e-01,  2.6459e-01],\n",
            "           [-3.1052e-01,  4.6064e-02, -2.8550e-01,  ..., -4.6262e-01,\n",
            "            -3.5312e-02,  2.3504e-01],\n",
            "           [ 2.3602e-01,  3.7636e-01,  5.1548e-01,  ...,  6.5005e-01,\n",
            "             6.9094e-01,  2.8525e-01]]],\n",
            "\n",
            "\n",
            "         [[[ 6.7190e-02,  5.9412e-01,  5.8622e-01,  ...,  7.5963e-01,\n",
            "             9.1346e-01,  2.3786e-01],\n",
            "           [ 4.5080e-01,  3.1510e-01,  2.8819e-01,  ...,  1.7937e-01,\n",
            "             3.4369e-01,  2.3387e-01],\n",
            "           [ 1.1328e-01,  7.4738e-02,  5.3716e-01,  ...,  4.1134e-01,\n",
            "             4.8684e-01, -2.0235e-02],\n",
            "           ...,\n",
            "           [ 3.2880e-01,  7.1771e-01,  4.3567e-02,  ...,  2.4768e-02,\n",
            "             4.4680e-01, -2.2971e-01],\n",
            "           [ 5.8981e-01,  8.7661e-01,  5.6426e-01,  ...,  4.2991e-01,\n",
            "             1.0182e+00,  1.0411e-01],\n",
            "           [ 3.7021e-01,  3.0811e-01,  1.0388e+00,  ...,  2.6217e-01,\n",
            "             4.9227e-01,  2.7253e-01]],\n",
            "\n",
            "          [[ 1.6960e-01,  8.9684e-01,  8.9226e-01,  ...,  7.9579e-01,\n",
            "             4.2667e-01,  3.7890e-01],\n",
            "           [ 4.2496e-01,  2.2200e-01,  3.5845e-01,  ...,  3.7157e-01,\n",
            "             2.0800e-01,  2.7156e-02],\n",
            "           [ 4.3928e-02,  4.5363e-01,  4.4415e-01,  ..., -1.1006e-01,\n",
            "             6.7335e-01,  1.2333e-01],\n",
            "           ...,\n",
            "           [-1.3884e-01,  8.7091e-01,  1.0810e-01,  ...,  3.9009e-01,\n",
            "            -8.3574e-02, -2.6862e-01],\n",
            "           [ 2.5268e-01,  1.0328e+00,  9.0120e-01,  ...,  9.6445e-01,\n",
            "             9.4057e-01,  3.2005e-02],\n",
            "           [ 4.5624e-01,  3.3710e-01, -4.3177e-02,  ...,  4.6572e-01,\n",
            "             3.7833e-01, -2.1643e-01]],\n",
            "\n",
            "          [[ 4.6273e-01,  7.4930e-01,  1.0859e+00,  ...,  6.6563e-01,\n",
            "             7.6774e-01,  2.7337e-01],\n",
            "           [ 3.3982e-01,  7.3056e-01,  4.2099e-01,  ...,  3.5420e-01,\n",
            "             3.0521e-01,  4.9056e-02],\n",
            "           [ 3.3362e-01,  8.0837e-02, -1.7839e-01,  ...,  2.3688e-01,\n",
            "             6.2048e-01,  3.6755e-01],\n",
            "           ...,\n",
            "           [ 3.4003e-01,  6.4834e-01,  1.2421e-01,  ...,  5.3866e-01,\n",
            "             2.5110e-01,  2.1541e-01],\n",
            "           [ 7.0748e-02,  5.7573e-01,  7.9793e-01,  ...,  5.3446e-01,\n",
            "             4.2946e-01,  1.5790e-01],\n",
            "           [ 8.6057e-02,  4.6596e-01,  7.7500e-01,  ...,  7.2149e-02,\n",
            "             2.7255e-01, -2.6808e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 4.2217e-01,  7.8683e-01,  3.6260e-01,  ...,  5.3691e-01,\n",
            "             8.5614e-01,  2.5727e-01],\n",
            "           [ 3.4185e-01, -1.7358e-01,  2.7951e-01,  ...,  2.1183e-01,\n",
            "             2.3216e-01, -8.8282e-02],\n",
            "           [ 3.2802e-01,  3.4879e-01,  1.1940e-01,  ...,  2.8173e-01,\n",
            "             3.2958e-01, -6.9248e-02],\n",
            "           ...,\n",
            "           [ 1.9014e-01,  1.4177e-01,  8.4149e-02,  ...,  3.0685e-01,\n",
            "             4.1291e-01, -3.1005e-01],\n",
            "           [ 3.6416e-01,  9.2827e-01,  4.7839e-01,  ...,  7.5314e-01,\n",
            "             1.0524e+00, -1.6378e-01],\n",
            "           [-1.5850e-01,  5.7743e-01, -2.0334e-01,  ...,  5.4011e-01,\n",
            "             8.6499e-02, -5.6628e-02]],\n",
            "\n",
            "          [[ 5.1153e-01,  9.3503e-01,  3.6057e-01,  ...,  5.2756e-01,\n",
            "             1.0715e+00,  2.6688e-01],\n",
            "           [ 3.4648e-01,  2.3584e-01, -2.6291e-01,  ..., -1.0079e-01,\n",
            "             1.8273e-01, -2.2752e-01],\n",
            "           [ 3.3418e-02,  1.7414e-01, -3.3576e-01,  ...,  1.6849e-01,\n",
            "             2.9266e-01,  2.2391e-01],\n",
            "           ...,\n",
            "           [-3.0140e-01,  3.9603e-01,  2.5034e-01,  ..., -6.0146e-02,\n",
            "             5.9155e-01, -1.8736e-01],\n",
            "           [ 4.6911e-02,  3.2076e-01,  5.4866e-01,  ...,  2.4572e-02,\n",
            "             6.3510e-01,  1.4728e-01],\n",
            "           [-1.4287e-01,  1.4171e-01, -1.8190e-01,  ...,  1.8881e-01,\n",
            "             1.6108e-01, -1.1668e-01]],\n",
            "\n",
            "          [[ 9.2322e-01,  7.4904e-01,  9.4559e-01,  ...,  9.1901e-01,\n",
            "             6.2681e-01,  4.3194e-01],\n",
            "           [ 5.7773e-01,  3.7014e-01,  5.1448e-01,  ...,  1.8627e-01,\n",
            "             3.8138e-02, -2.0940e-04],\n",
            "           [ 1.9953e-01,  4.6437e-01,  3.1283e-01,  ...,  8.6085e-01,\n",
            "             2.7458e-01,  2.8730e-01],\n",
            "           ...,\n",
            "           [ 2.7901e-01,  5.0097e-01,  6.9192e-01,  ...,  5.5513e-01,\n",
            "            -1.3880e-01,  1.1495e-01],\n",
            "           [ 2.3979e-01,  3.1290e-01,  4.3253e-01,  ...,  6.0306e-01,\n",
            "             8.2077e-02,  2.0357e-01],\n",
            "           [-4.6697e-02,  5.4978e-01, -4.8322e-02,  ...,  1.9619e-01,\n",
            "             1.5229e-01, -2.6301e-02]]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(probability.shape)\n",
        "print(probability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxBKEIIneFRa",
        "outputId": "42cbe875-bf06-45a9-9689-b4fc9ceea270"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 128, 128, 128])\n",
            "tensor([[[[[0.2360, 0.2067, 0.1803,  ..., 0.2694, 0.2162, 0.2451],\n",
            "           [0.3238, 0.2558, 0.1909,  ..., 0.1864, 0.2249, 0.2713],\n",
            "           [0.1405, 0.1614, 0.2462,  ..., 0.2341, 0.2026, 0.1374],\n",
            "           ...,\n",
            "           [0.2139, 0.2835, 0.1751,  ..., 0.2615, 0.2768, 0.2211],\n",
            "           [0.1460, 0.1659, 0.1354,  ..., 0.1612, 0.1462, 0.2102],\n",
            "           [0.1257, 0.1943, 0.2168,  ..., 0.1696, 0.1745, 0.2471]],\n",
            "\n",
            "          [[0.2563, 0.2419, 0.3231,  ..., 0.1873, 0.1663, 0.1830],\n",
            "           [0.3034, 0.2070, 0.1985,  ..., 0.1618, 0.1844, 0.1477],\n",
            "           [0.2640, 0.1904, 0.2237,  ..., 0.1586, 0.2358, 0.2000],\n",
            "           ...,\n",
            "           [0.1641, 0.2104, 0.2704,  ..., 0.1997, 0.1845, 0.1834],\n",
            "           [0.1967, 0.1802, 0.1411,  ..., 0.1455, 0.1099, 0.1695],\n",
            "           [0.2554, 0.2047, 0.2462,  ..., 0.2561, 0.1489, 0.1952]],\n",
            "\n",
            "          [[0.3022, 0.2630, 0.2407,  ..., 0.2808, 0.3252, 0.1747],\n",
            "           [0.1678, 0.2385, 0.2223,  ..., 0.2668, 0.2333, 0.1967],\n",
            "           [0.1985, 0.2056, 0.2138,  ..., 0.2177, 0.1885, 0.1415],\n",
            "           ...,\n",
            "           [0.2611, 0.1891, 0.1367,  ..., 0.2289, 0.2084, 0.2236],\n",
            "           [0.1900, 0.1800, 0.0842,  ..., 0.1293, 0.2169, 0.1407],\n",
            "           [0.1829, 0.1210, 0.1437,  ..., 0.1104, 0.1089, 0.1269]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.2231, 0.2050, 0.1894,  ..., 0.2250, 0.1993, 0.2059],\n",
            "           [0.1900, 0.1500, 0.1748,  ..., 0.1953, 0.1736, 0.2094],\n",
            "           [0.2966, 0.3010, 0.1723,  ..., 0.2121, 0.2071, 0.2168],\n",
            "           ...,\n",
            "           [0.1905, 0.2675, 0.1828,  ..., 0.1491, 0.1567, 0.1234],\n",
            "           [0.1981, 0.0959, 0.1364,  ..., 0.1319, 0.0988, 0.1337],\n",
            "           [0.1850, 0.0973, 0.1297,  ..., 0.1542, 0.1635, 0.2097]],\n",
            "\n",
            "          [[0.3031, 0.2175, 0.1743,  ..., 0.2390, 0.2227, 0.2395],\n",
            "           [0.1937, 0.1970, 0.2119,  ..., 0.2331, 0.1821, 0.1793],\n",
            "           [0.2094, 0.1955, 0.0984,  ..., 0.1699, 0.1257, 0.1797],\n",
            "           ...,\n",
            "           [0.1221, 0.2224, 0.1949,  ..., 0.1387, 0.1535, 0.1441],\n",
            "           [0.1742, 0.1008, 0.1153,  ..., 0.1943, 0.1234, 0.1591],\n",
            "           [0.1564, 0.0862, 0.1265,  ..., 0.0715, 0.0721, 0.1603]],\n",
            "\n",
            "          [[0.2479, 0.1958, 0.2186,  ..., 0.1897, 0.1974, 0.2125],\n",
            "           [0.2305, 0.2346, 0.2420,  ..., 0.1660, 0.2574, 0.1899],\n",
            "           [0.2003, 0.2658, 0.2059,  ..., 0.2455, 0.2112, 0.2331],\n",
            "           ...,\n",
            "           [0.2348, 0.2342, 0.2094,  ..., 0.2770, 0.2176, 0.2073],\n",
            "           [0.2242, 0.1881, 0.1230,  ..., 0.1528, 0.2102, 0.1962],\n",
            "           [0.1846, 0.1492, 0.1982,  ..., 0.1646, 0.1633, 0.1830]]],\n",
            "\n",
            "\n",
            "         [[[0.2966, 0.1688, 0.1455,  ..., 0.1036, 0.1054, 0.1715],\n",
            "           [0.1926, 0.0919, 0.2056,  ..., 0.1969, 0.1207, 0.2626],\n",
            "           [0.1505, 0.1706, 0.2278,  ..., 0.1875, 0.0995, 0.2539],\n",
            "           ...,\n",
            "           [0.1744, 0.1277, 0.1700,  ..., 0.2263, 0.1454, 0.2565],\n",
            "           [0.1459, 0.1141, 0.0596,  ..., 0.0998, 0.0867, 0.2183],\n",
            "           [0.1278, 0.2103, 0.1415,  ..., 0.1244, 0.1290, 0.1444]],\n",
            "\n",
            "          [[0.2532, 0.0740, 0.1431,  ..., 0.2543, 0.0965, 0.2263],\n",
            "           [0.3715, 0.2963, 0.2181,  ..., 0.3112, 0.2807, 0.3975],\n",
            "           [0.3159, 0.3631, 0.3225,  ..., 0.3080, 0.2897, 0.3256],\n",
            "           ...,\n",
            "           [0.2542, 0.2927, 0.1826,  ..., 0.3132, 0.2419, 0.2660],\n",
            "           [0.2760, 0.0752, 0.2587,  ..., 0.1451, 0.1248, 0.1289],\n",
            "           [0.2281, 0.3688, 0.3455,  ..., 0.2522, 0.2043, 0.3035]],\n",
            "\n",
            "          [[0.2045, 0.0831, 0.1228,  ..., 0.1592, 0.1179, 0.1533],\n",
            "           [0.2852, 0.2004, 0.3583,  ..., 0.1561, 0.3168, 0.2462],\n",
            "           [0.2548, 0.2512, 0.2955,  ..., 0.1391, 0.2224, 0.2170],\n",
            "           ...,\n",
            "           [0.1735, 0.2765, 0.2557,  ..., 0.1584, 0.2232, 0.2889],\n",
            "           [0.2014, 0.1575, 0.1861,  ..., 0.1511, 0.1613, 0.1572],\n",
            "           [0.2708, 0.3319, 0.2646,  ..., 0.3294, 0.1935, 0.3671]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.1231, 0.2229, 0.1082,  ..., 0.2328, 0.1596, 0.3340],\n",
            "           [0.2919, 0.3847, 0.3045,  ..., 0.2795, 0.2267, 0.2464],\n",
            "           [0.3068, 0.1979, 0.3294,  ..., 0.2654, 0.2669, 0.1571],\n",
            "           ...,\n",
            "           [0.3825, 0.1377, 0.2821,  ..., 0.3385, 0.2862, 0.2297],\n",
            "           [0.2456, 0.0398, 0.1593,  ..., 0.0850, 0.2073, 0.2595],\n",
            "           [0.2982, 0.3938, 0.2038,  ..., 0.1995, 0.2864, 0.2635]],\n",
            "\n",
            "          [[0.2529, 0.1725, 0.1383,  ..., 0.2106, 0.1908, 0.2316],\n",
            "           [0.3140, 0.3267, 0.3817,  ..., 0.2223, 0.2835, 0.3947],\n",
            "           [0.2820, 0.2053, 0.4650,  ..., 0.2815, 0.2411, 0.2443],\n",
            "           ...,\n",
            "           [0.4676, 0.2855, 0.4011,  ..., 0.4583, 0.4038, 0.3705],\n",
            "           [0.2021, 0.1716, 0.2315,  ..., 0.1329, 0.1988, 0.2827],\n",
            "           [0.3415, 0.4722, 0.3379,  ..., 0.4104, 0.2776, 0.3779]],\n",
            "\n",
            "          [[0.1345, 0.1351, 0.1039,  ..., 0.1879, 0.1751, 0.1968],\n",
            "           [0.2554, 0.1866, 0.2786,  ..., 0.1617, 0.2421, 0.3066],\n",
            "           [0.2029, 0.1949, 0.3186,  ..., 0.1482, 0.2200, 0.2599],\n",
            "           ...,\n",
            "           [0.2546, 0.2651, 0.2992,  ..., 0.1464, 0.3212, 0.2458],\n",
            "           [0.2798, 0.2461, 0.2055,  ..., 0.2194, 0.2496, 0.2152],\n",
            "           [0.3858, 0.4117, 0.2916,  ..., 0.3808, 0.3510, 0.3686]]],\n",
            "\n",
            "\n",
            "         [[[0.2284, 0.2577, 0.1847,  ..., 0.2654, 0.1467, 0.2673],\n",
            "           [0.2529, 0.2162, 0.3283,  ..., 0.3736, 0.3235, 0.2728],\n",
            "           [0.4421, 0.3006, 0.2932,  ..., 0.2877, 0.2384, 0.4088],\n",
            "           ...,\n",
            "           [0.4289, 0.1866, 0.2282,  ..., 0.2612, 0.1964, 0.2587],\n",
            "           [0.4108, 0.4400, 0.3922,  ..., 0.4349, 0.3620, 0.3720],\n",
            "           [0.3337, 0.3346, 0.3722,  ..., 0.2665, 0.2898, 0.2747]],\n",
            "\n",
            "          [[0.1929, 0.1219, 0.1083,  ..., 0.1760, 0.0846, 0.2678],\n",
            "           [0.1631, 0.2646, 0.1526,  ..., 0.3448, 0.2576, 0.2848],\n",
            "           [0.2894, 0.3073, 0.2082,  ..., 0.1760, 0.2214, 0.2460],\n",
            "           ...,\n",
            "           [0.2272, 0.2589, 0.1691,  ..., 0.2217, 0.2508, 0.4350],\n",
            "           [0.2481, 0.2708, 0.2491,  ..., 0.3469, 0.3640, 0.4159],\n",
            "           [0.2820, 0.2520, 0.2403,  ..., 0.2237, 0.3504, 0.2876]],\n",
            "\n",
            "          [[0.1758, 0.1407, 0.1191,  ..., 0.1390, 0.2250, 0.3787],\n",
            "           [0.2379, 0.2468, 0.2547,  ..., 0.2665, 0.2576, 0.2751],\n",
            "           [0.2680, 0.2304, 0.2799,  ..., 0.2311, 0.1847, 0.3301],\n",
            "           ...,\n",
            "           [0.2991, 0.3588, 0.2711,  ..., 0.1200, 0.2753, 0.3189],\n",
            "           [0.2726, 0.2136, 0.2459,  ..., 0.2667, 0.3408, 0.4387],\n",
            "           [0.3444, 0.2939, 0.3066,  ..., 0.2275, 0.3764, 0.3245]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.1096, 0.2000, 0.1117,  ..., 0.1607, 0.1542, 0.2799],\n",
            "           [0.2391, 0.3189, 0.2411,  ..., 0.2854, 0.2652, 0.2530],\n",
            "           [0.2005, 0.1283, 0.3593,  ..., 0.2822, 0.2882, 0.3281],\n",
            "           ...,\n",
            "           [0.2037, 0.1631, 0.2614,  ..., 0.2808, 0.2966, 0.2761],\n",
            "           [0.2573, 0.1465, 0.2079,  ..., 0.3317, 0.2739, 0.3701],\n",
            "           [0.3143, 0.2423, 0.3336,  ..., 0.3106, 0.2253, 0.3206]],\n",
            "\n",
            "          [[0.1668, 0.1735, 0.1985,  ..., 0.1499, 0.1408, 0.3081],\n",
            "           [0.2796, 0.2752, 0.2007,  ..., 0.2698, 0.1728, 0.2659],\n",
            "           [0.2673, 0.3088, 0.2496,  ..., 0.3173, 0.2148, 0.3373],\n",
            "           ...,\n",
            "           [0.3183, 0.2207, 0.2673,  ..., 0.2648, 0.2068, 0.3089],\n",
            "           [0.2948, 0.1856, 0.1718,  ..., 0.3591, 0.2994, 0.3996],\n",
            "           [0.2908, 0.2818, 0.3688,  ..., 0.3083, 0.3443, 0.2943]],\n",
            "\n",
            "          [[0.1374, 0.1040, 0.0952,  ..., 0.1340, 0.1637, 0.2190],\n",
            "           [0.2226, 0.2528, 0.1897,  ..., 0.2060, 0.2097, 0.2127],\n",
            "           [0.2838, 0.1787, 0.1871,  ..., 0.1627, 0.1852, 0.2208],\n",
            "           ...,\n",
            "           [0.2115, 0.2201, 0.2421,  ..., 0.1957, 0.2113, 0.2092],\n",
            "           [0.1480, 0.1612, 0.1357,  ..., 0.1621, 0.2550, 0.3308],\n",
            "           [0.3010, 0.2420, 0.2542,  ..., 0.2635, 0.3175, 0.2544]]],\n",
            "\n",
            "\n",
            "         [[[0.2390, 0.3668, 0.4895,  ..., 0.3616, 0.5316, 0.3161],\n",
            "           [0.2308, 0.4361, 0.2753,  ..., 0.2430, 0.3309, 0.1933],\n",
            "           [0.2669, 0.3673, 0.2328,  ..., 0.2906, 0.4595, 0.1999],\n",
            "           ...,\n",
            "           [0.1828, 0.4022, 0.4268,  ..., 0.2511, 0.3814, 0.2637],\n",
            "           [0.2973, 0.2800, 0.4129,  ..., 0.3040, 0.4051, 0.1996],\n",
            "           [0.4128, 0.2608, 0.2695,  ..., 0.4395, 0.4066, 0.3338]],\n",
            "\n",
            "          [[0.2976, 0.5622, 0.4255,  ..., 0.3824, 0.6525, 0.3228],\n",
            "           [0.1620, 0.2321, 0.4308,  ..., 0.1823, 0.2773, 0.1700],\n",
            "           [0.1308, 0.1393, 0.2456,  ..., 0.3575, 0.2530, 0.2284],\n",
            "           ...,\n",
            "           [0.3544, 0.2380, 0.3779,  ..., 0.2654, 0.3228, 0.1156],\n",
            "           [0.2792, 0.4737, 0.3511,  ..., 0.3625, 0.4013, 0.2857],\n",
            "           [0.2346, 0.1745, 0.1680,  ..., 0.2681, 0.2964, 0.2137]],\n",
            "\n",
            "          [[0.3175, 0.5131, 0.5175,  ..., 0.4210, 0.3319, 0.2933],\n",
            "           [0.3091, 0.3143, 0.1647,  ..., 0.3106, 0.1923, 0.2820],\n",
            "           [0.2787, 0.3127, 0.2108,  ..., 0.4121, 0.4043, 0.3114],\n",
            "           ...,\n",
            "           [0.2664, 0.1756, 0.3365,  ..., 0.4927, 0.2931, 0.1686],\n",
            "           [0.3359, 0.4489, 0.4839,  ..., 0.4529, 0.2809, 0.2634],\n",
            "           [0.2019, 0.2531, 0.2852,  ..., 0.3327, 0.3211, 0.1815]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.5442, 0.3721, 0.5908,  ..., 0.3815, 0.4870, 0.1802],\n",
            "           [0.2790, 0.1464, 0.2796,  ..., 0.2398, 0.3345, 0.2912],\n",
            "           [0.1961, 0.3728, 0.1389,  ..., 0.2402, 0.2378, 0.2980],\n",
            "           ...,\n",
            "           [0.2233, 0.4316, 0.2738,  ..., 0.2316, 0.2605, 0.3708],\n",
            "           [0.2991, 0.7179, 0.4965,  ..., 0.4514, 0.4199, 0.2367],\n",
            "           [0.2025, 0.2665, 0.3329,  ..., 0.3357, 0.3248, 0.2062]],\n",
            "\n",
            "          [[0.2772, 0.4366, 0.4889,  ..., 0.4005, 0.4457, 0.2208],\n",
            "           [0.2127, 0.2011, 0.2058,  ..., 0.2748, 0.3616, 0.1601],\n",
            "           [0.2413, 0.2904, 0.1869,  ..., 0.2313, 0.4184, 0.2386],\n",
            "           ...,\n",
            "           [0.0920, 0.2714, 0.1366,  ..., 0.1382, 0.2359, 0.1765],\n",
            "           [0.3289, 0.5420, 0.4815,  ..., 0.3138, 0.3783, 0.1586],\n",
            "           [0.2113, 0.1598, 0.1669,  ..., 0.2099, 0.3060, 0.1676]],\n",
            "\n",
            "          [[0.4802, 0.5651, 0.5823,  ..., 0.4884, 0.4638, 0.3716],\n",
            "           [0.2915, 0.3259, 0.2897,  ..., 0.4664, 0.2907, 0.2907],\n",
            "           [0.3129, 0.3605, 0.2884,  ..., 0.4436, 0.3836, 0.2863],\n",
            "           ...,\n",
            "           [0.2991, 0.2806, 0.2493,  ..., 0.3810, 0.2499, 0.3377],\n",
            "           [0.3480, 0.4046, 0.5357,  ..., 0.4658, 0.2851, 0.2578],\n",
            "           [0.1285, 0.1970, 0.2560,  ..., 0.1912, 0.1682, 0.1940]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0.2777, 0.2705, 0.2646,  ..., 0.1873, 0.1977, 0.2420],\n",
            "           [0.1842, 0.2438, 0.1936,  ..., 0.2002, 0.2201, 0.2513],\n",
            "           [0.2814, 0.2111, 0.2098,  ..., 0.1377, 0.2022, 0.2275],\n",
            "           ...,\n",
            "           [0.1744, 0.1685, 0.1863,  ..., 0.3080, 0.1927, 0.1463],\n",
            "           [0.2077, 0.2029, 0.1530,  ..., 0.1726, 0.1037, 0.1443],\n",
            "           [0.2518, 0.1993, 0.1184,  ..., 0.2177, 0.1580, 0.2165]],\n",
            "\n",
            "          [[0.3380, 0.2800, 0.1331,  ..., 0.2037, 0.2269, 0.1761],\n",
            "           [0.1840, 0.2133, 0.1816,  ..., 0.2030, 0.2617, 0.1770],\n",
            "           [0.3008, 0.2887, 0.2631,  ..., 0.1713, 0.2000, 0.1753],\n",
            "           ...,\n",
            "           [0.1853, 0.2153, 0.2391,  ..., 0.2121, 0.1719, 0.2092],\n",
            "           [0.1583, 0.1059, 0.1178,  ..., 0.1122, 0.1130, 0.1905],\n",
            "           [0.1760, 0.1999, 0.2145,  ..., 0.1661, 0.1631, 0.2568]],\n",
            "\n",
            "          [[0.2819, 0.2655, 0.1676,  ..., 0.1840, 0.2027, 0.2034],\n",
            "           [0.2768, 0.3875, 0.3486,  ..., 0.1251, 0.2060, 0.1830],\n",
            "           [0.1678, 0.1879, 0.1783,  ..., 0.2730, 0.2483, 0.1456],\n",
            "           ...,\n",
            "           [0.1481, 0.1561, 0.1479,  ..., 0.1977, 0.1303, 0.1560],\n",
            "           [0.2097, 0.1159, 0.1002,  ..., 0.0979, 0.1619, 0.1321],\n",
            "           [0.1399, 0.0931, 0.0751,  ..., 0.1375, 0.2087, 0.2092]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.2360, 0.2615, 0.2404,  ..., 0.2631, 0.2006, 0.1712],\n",
            "           [0.1547, 0.2098, 0.1763,  ..., 0.1743, 0.1945, 0.2211],\n",
            "           [0.2107, 0.2748, 0.2357,  ..., 0.1907, 0.2126, 0.1502],\n",
            "           ...,\n",
            "           [0.2447, 0.1760, 0.1765,  ..., 0.1820, 0.2224, 0.1689],\n",
            "           [0.1535, 0.1342, 0.1247,  ..., 0.0749, 0.0939, 0.1524],\n",
            "           [0.2332, 0.1313, 0.1534,  ..., 0.1451, 0.1894, 0.2062]],\n",
            "\n",
            "          [[0.2677, 0.2173, 0.2155,  ..., 0.2443, 0.1831, 0.1985],\n",
            "           [0.2548, 0.3731, 0.1873,  ..., 0.1630, 0.1789, 0.1700],\n",
            "           [0.1166, 0.1261, 0.1943,  ..., 0.1803, 0.1120, 0.1510],\n",
            "           ...,\n",
            "           [0.2427, 0.1859, 0.2125,  ..., 0.0668, 0.1140, 0.1915],\n",
            "           [0.1452, 0.1800, 0.1173,  ..., 0.1323, 0.1062, 0.1547],\n",
            "           [0.1639, 0.1500, 0.1308,  ..., 0.1296, 0.1545, 0.1701]],\n",
            "\n",
            "          [[0.2544, 0.2574, 0.1514,  ..., 0.1720, 0.2741, 0.2228],\n",
            "           [0.2125, 0.1959, 0.1862,  ..., 0.2587, 0.2418, 0.2314],\n",
            "           [0.2162, 0.2006, 0.2146,  ..., 0.2083, 0.2082, 0.2177],\n",
            "           ...,\n",
            "           [0.2556, 0.1862, 0.1447,  ..., 0.2061, 0.2391, 0.2235],\n",
            "           [0.2665, 0.2302, 0.2018,  ..., 0.1503, 0.2140, 0.2119],\n",
            "           [0.1892, 0.1254, 0.1710,  ..., 0.1668, 0.1618, 0.1992]]],\n",
            "\n",
            "\n",
            "         [[[0.2335, 0.1179, 0.1759,  ..., 0.1718, 0.1272, 0.2895],\n",
            "           [0.1383, 0.1157, 0.2178,  ..., 0.1838, 0.2650, 0.1706],\n",
            "           [0.1414, 0.2445, 0.1612,  ..., 0.2326, 0.2013, 0.2833],\n",
            "           ...,\n",
            "           [0.2004, 0.1411, 0.3210,  ..., 0.2655, 0.2618, 0.3376],\n",
            "           [0.0914, 0.0690, 0.0679,  ..., 0.1423, 0.0948, 0.2327],\n",
            "           [0.1285, 0.1903, 0.1133,  ..., 0.1749, 0.1069, 0.1844]],\n",
            "\n",
            "          [[0.2266, 0.1033, 0.1771,  ..., 0.1523, 0.1749, 0.2708],\n",
            "           [0.3071, 0.2369, 0.2247,  ..., 0.2654, 0.1802, 0.2462],\n",
            "           [0.3001, 0.2152, 0.2691,  ..., 0.3936, 0.1748, 0.2991],\n",
            "           ...,\n",
            "           [0.4379, 0.1121, 0.1842,  ..., 0.2727, 0.2759, 0.2641],\n",
            "           [0.1384, 0.1077, 0.0845,  ..., 0.0927, 0.0477, 0.3211],\n",
            "           [0.2371, 0.2713, 0.3254,  ..., 0.2812, 0.1794, 0.2771]],\n",
            "\n",
            "          [[0.2337, 0.1204, 0.0932,  ..., 0.1595, 0.2046, 0.2917],\n",
            "           [0.2246, 0.0857, 0.1590,  ..., 0.3795, 0.1500, 0.1922],\n",
            "           [0.1899, 0.3686, 0.2317,  ..., 0.2274, 0.1691, 0.2903],\n",
            "           ...,\n",
            "           [0.3824, 0.1719, 0.3026,  ..., 0.2717, 0.3352, 0.3622],\n",
            "           [0.1610, 0.1483, 0.1065,  ..., 0.1801, 0.1362, 0.2765],\n",
            "           [0.2515, 0.4242, 0.2099,  ..., 0.4019, 0.1920, 0.2767]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.2722, 0.1560, 0.2180,  ..., 0.1933, 0.1037, 0.2882],\n",
            "           [0.2926, 0.1865, 0.2357,  ..., 0.3729, 0.2493, 0.3025],\n",
            "           [0.2660, 0.2491, 0.3143,  ..., 0.2517, 0.2898, 0.3025],\n",
            "           ...,\n",
            "           [0.2814, 0.3413, 0.3166,  ..., 0.3216, 0.2389, 0.4000],\n",
            "           [0.2540, 0.1209, 0.2838,  ..., 0.1113, 0.1172, 0.2371],\n",
            "           [0.2902, 0.2560, 0.3532,  ..., 0.2522, 0.3669, 0.2073]],\n",
            "\n",
            "          [[0.1783, 0.1130, 0.2373,  ..., 0.1857, 0.1214, 0.2689],\n",
            "           [0.2085, 0.2035, 0.3307,  ..., 0.3485, 0.2520, 0.3552],\n",
            "           [0.5122, 0.4072, 0.3919,  ..., 0.3503, 0.4140, 0.2903],\n",
            "           ...,\n",
            "           [0.4433, 0.3544, 0.3386,  ..., 0.4857, 0.2768, 0.3431],\n",
            "           [0.2342, 0.2909, 0.3048,  ..., 0.3032, 0.2352, 0.2568],\n",
            "           [0.3406, 0.2970, 0.2775,  ..., 0.3409, 0.3286, 0.2809]],\n",
            "\n",
            "          [[0.1160, 0.1309, 0.1231,  ..., 0.1240, 0.1750, 0.1927],\n",
            "           [0.1411, 0.3269, 0.1940,  ..., 0.2348, 0.3172, 0.2902],\n",
            "           [0.2933, 0.2280, 0.2846,  ..., 0.1387, 0.3305, 0.2413],\n",
            "           ...,\n",
            "           [0.2214, 0.1837, 0.1675,  ..., 0.1858, 0.2887, 0.2510],\n",
            "           [0.3052, 0.2474, 0.2508,  ..., 0.2327, 0.2791, 0.2181],\n",
            "           [0.4370, 0.2927, 0.3684,  ..., 0.2657, 0.3142, 0.3676]]],\n",
            "\n",
            "\n",
            "         [[[0.2668, 0.2611, 0.1609,  ..., 0.1608, 0.1474, 0.2349],\n",
            "           [0.3551, 0.3279, 0.2471,  ..., 0.3573, 0.2314, 0.2804],\n",
            "           [0.3384, 0.3245, 0.2670,  ..., 0.2652, 0.1887, 0.3030],\n",
            "           ...,\n",
            "           [0.2702, 0.2942, 0.2992,  ..., 0.2031, 0.2137, 0.3473],\n",
            "           [0.3112, 0.2390, 0.3819,  ..., 0.3523, 0.3666, 0.4127],\n",
            "           [0.2667, 0.3571, 0.2430,  ..., 0.2997, 0.3928, 0.2569]],\n",
            "\n",
            "          [[0.1770, 0.1229, 0.1424,  ..., 0.1084, 0.2098, 0.2256],\n",
            "           [0.2012, 0.2594, 0.3332,  ..., 0.2776, 0.2455, 0.3641],\n",
            "           [0.1753, 0.1562, 0.1523,  ..., 0.2266, 0.1236, 0.2761],\n",
            "           ...,\n",
            "           [0.2307, 0.2004, 0.3173,  ..., 0.2561, 0.2702, 0.3763],\n",
            "           [0.4474, 0.2310, 0.1614,  ..., 0.2949, 0.2534, 0.2990],\n",
            "           [0.2649, 0.2417, 0.2393,  ..., 0.2732, 0.2893, 0.2779]],\n",
            "\n",
            "          [[0.1244, 0.1505, 0.1220,  ..., 0.1711, 0.1635, 0.2320],\n",
            "           [0.1997, 0.1664, 0.1884,  ..., 0.1799, 0.2791, 0.3699],\n",
            "           [0.3432, 0.2508, 0.3799,  ..., 0.2101, 0.2048, 0.3369],\n",
            "           ...,\n",
            "           [0.2238, 0.3151, 0.3587,  ..., 0.2299, 0.2870, 0.2527],\n",
            "           [0.3577, 0.2420, 0.3342,  ..., 0.4058, 0.3372, 0.3594],\n",
            "           [0.4027, 0.2253, 0.4233,  ..., 0.2405, 0.2795, 0.3112]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.1481, 0.1408, 0.2082,  ..., 0.1767, 0.1889, 0.2930],\n",
            "           [0.2494, 0.4710, 0.3476,  ..., 0.2016, 0.3063, 0.3011],\n",
            "           [0.2808, 0.1746, 0.2332,  ..., 0.2306, 0.2287, 0.3316],\n",
            "           ...,\n",
            "           [0.1949, 0.2707, 0.2839,  ..., 0.2220, 0.2176, 0.3019],\n",
            "           [0.2560, 0.2494, 0.2463,  ..., 0.4559, 0.3411, 0.4359],\n",
            "           [0.2931, 0.2399, 0.2886,  ..., 0.2932, 0.2553, 0.3724]],\n",
            "\n",
            "          [[0.1899, 0.1173, 0.2531,  ..., 0.2116, 0.1049, 0.2437],\n",
            "           [0.2165, 0.2068, 0.3464,  ..., 0.3094, 0.2444, 0.2992],\n",
            "           [0.2041, 0.2530, 0.3119,  ..., 0.2584, 0.2927, 0.3223],\n",
            "           ...,\n",
            "           [0.1794, 0.2052, 0.2132,  ..., 0.3330, 0.2141, 0.3047],\n",
            "           [0.4078, 0.2467, 0.3108,  ..., 0.3772, 0.4042, 0.3111],\n",
            "           [0.3287, 0.2872, 0.4635,  ..., 0.3290, 0.2853, 0.3596]],\n",
            "\n",
            "          [[0.1230, 0.1479, 0.1034,  ..., 0.0987, 0.1129, 0.2090],\n",
            "           [0.2594, 0.2396, 0.2745,  ..., 0.1846, 0.1924, 0.2534],\n",
            "           [0.2321, 0.2238, 0.1636,  ..., 0.1417, 0.1890, 0.2338],\n",
            "           ...,\n",
            "           [0.2527, 0.2043, 0.1762,  ..., 0.2237, 0.2396, 0.2824],\n",
            "           [0.1567, 0.2266, 0.1794,  ..., 0.1581, 0.2386, 0.2895],\n",
            "           [0.2131, 0.2658, 0.2935,  ..., 0.3470, 0.3309, 0.2501]]],\n",
            "\n",
            "\n",
            "         [[[0.2221, 0.3505, 0.3987,  ..., 0.4801, 0.5277, 0.2335],\n",
            "           [0.3225, 0.3127, 0.3415,  ..., 0.2587, 0.2835, 0.2977],\n",
            "           [0.2387, 0.2199, 0.3620,  ..., 0.3645, 0.4078, 0.1861],\n",
            "           ...,\n",
            "           [0.3549, 0.3963, 0.1934,  ..., 0.2234, 0.3318, 0.1688],\n",
            "           [0.3897, 0.4891, 0.3972,  ..., 0.3329, 0.4349, 0.2103],\n",
            "           [0.3530, 0.2533, 0.5254,  ..., 0.3077, 0.3423, 0.3422]],\n",
            "\n",
            "          [[0.2584, 0.4938, 0.5475,  ..., 0.5355, 0.3885, 0.3276],\n",
            "           [0.3077, 0.2903, 0.2605,  ..., 0.2540, 0.3127, 0.2126],\n",
            "           [0.2239, 0.3399, 0.3155,  ..., 0.2086, 0.5015, 0.2495],\n",
            "           ...,\n",
            "           [0.1460, 0.4722, 0.2593,  ..., 0.2591, 0.2821, 0.1503],\n",
            "           [0.2559, 0.5553, 0.6362,  ..., 0.5002, 0.5859, 0.1894],\n",
            "           [0.3219, 0.2871, 0.2208,  ..., 0.2795, 0.3683, 0.1882]],\n",
            "\n",
            "          [[0.3600, 0.4637, 0.6171,  ..., 0.4854, 0.4293, 0.2728],\n",
            "           [0.2989, 0.3604, 0.3041,  ..., 0.3155, 0.3649, 0.2548],\n",
            "           [0.2990, 0.1927, 0.2102,  ..., 0.2895, 0.3777, 0.2272],\n",
            "           ...,\n",
            "           [0.2458, 0.3570, 0.1908,  ..., 0.3006, 0.2476, 0.2291],\n",
            "           [0.2716, 0.4938, 0.4590,  ..., 0.3161, 0.3647, 0.2321],\n",
            "           [0.2059, 0.2575, 0.2917,  ..., 0.2201, 0.3199, 0.2029]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0.3437, 0.4418, 0.3334,  ..., 0.3669, 0.5068, 0.2475],\n",
            "           [0.3032, 0.1327, 0.2404,  ..., 0.2512, 0.2500, 0.1753],\n",
            "           [0.2425, 0.3015, 0.2168,  ..., 0.3270, 0.2689, 0.2156],\n",
            "           ...,\n",
            "           [0.2790, 0.2120, 0.2229,  ..., 0.2744, 0.3211, 0.1292],\n",
            "           [0.3366, 0.4955, 0.3453,  ..., 0.3579, 0.4478, 0.1746],\n",
            "           [0.1835, 0.3728, 0.2048,  ..., 0.3095, 0.1883, 0.2141]],\n",
            "\n",
            "          [[0.3641, 0.5524, 0.2940,  ..., 0.3584, 0.5905, 0.2889],\n",
            "           [0.3202, 0.2166, 0.1356,  ..., 0.1791, 0.3248, 0.1756],\n",
            "           [0.1671, 0.2136, 0.1019,  ..., 0.2110, 0.1813, 0.2365],\n",
            "           ...,\n",
            "           [0.1346, 0.2545, 0.2356,  ..., 0.1145, 0.3950, 0.1606],\n",
            "           [0.2129, 0.2824, 0.2671,  ..., 0.1874, 0.2544, 0.2773],\n",
            "           [0.1668, 0.2658, 0.1281,  ..., 0.2006, 0.2316, 0.1894]],\n",
            "\n",
            "          [[0.5067, 0.4638, 0.6221,  ..., 0.6053, 0.4380, 0.3755],\n",
            "           [0.3870, 0.2377, 0.3453,  ..., 0.3219, 0.2486, 0.2250],\n",
            "           [0.2584, 0.3476, 0.3372,  ..., 0.5113, 0.2723, 0.3072],\n",
            "           ...,\n",
            "           [0.2704, 0.4257, 0.5117,  ..., 0.3844, 0.2326, 0.2431],\n",
            "           [0.2717, 0.2959, 0.3679,  ..., 0.4589, 0.2683, 0.2806],\n",
            "           [0.1607, 0.3161, 0.1670,  ..., 0.2204, 0.1931, 0.1831]]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pouMPGESf3DW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_images = []\n",
        "for i in range(2):\n",
        "  newx = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_training_images.append(newx)"
      ],
      "metadata": {
        "id": "0ohSRRMqgJPg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_training_images))\n",
        "print(randomized_training_images[0].shape)\n",
        "# print(randomized_training_images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEM8Z-YVgK4L",
        "outputId": "52d6c204-f159-409e-8bfd-3035c29b90bf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_training_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "je2iTekggR-n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_training_segmentations))\n",
        "print(randomized_training_segmentations[0].shape)\n",
        "# print(randomized_training_segmentations[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQI7ldbUgTss",
        "outputId": "190ec3c8-a11f-4ed0-9154-0fa9fb0bd41d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_training_data = list(zip(randomized_training_images, randomized_training_segmentations))"
      ],
      "metadata": {
        "id": "0Id-C28UgVDx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(dataset=randomized_training_data, batch_size=2, shuffle=True) # batch size should be 2"
      ],
      "metadata": {
        "id": "0kXxYbR2gWRx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1mBIdOfgXms",
        "outputId": "b5714f5d-d6c4-4f96-ac1b-208550ceb63e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_images = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_validation_images.append(newy)"
      ],
      "metadata": {
        "id": "jH0ms_qigaKE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_validation_images))\n",
        "print(randomized_validation_images[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDOlu_IigcUt",
        "outputId": "8260a218-2336-466d-ce4c-6f082ee78fb3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_validation_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "S9ruiz4Tgevx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_validation_segmentations))\n",
        "print(randomized_validation_segmentations[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOYt_EuigfYa",
        "outputId": "cf4a2391-479e-4f2a-a004-a21127138f6a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_validation_data = list(zip(randomized_validation_images, randomized_validation_segmentations))"
      ],
      "metadata": {
        "id": "zFPZkDakgz_p"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validationloader = torch.utils.data.DataLoader(dataset=randomized_validation_data, batch_size=2, shuffle=True) # batch size should be 2"
      ],
      "metadata": {
        "id": "6Wb-FS1Lg1jM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(validationloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fchjms5Gg2au",
        "outputId": "2133d738-e101-4ed1-8154-c143d3876127"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_images = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_testing_images.append(newy)"
      ],
      "metadata": {
        "id": "527yVWmyggOd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_testing_images))\n",
        "print(randomized_testing_images[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDHqy02vhBWt",
        "outputId": "23077967-ff7f-431f-f3a4-87a09c0d79d7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_segmentations = []\n",
        "for i in range(2):\n",
        "  newy = torch.rand(size=(1, 4, 128, 128, 128), dtype=torch.float32)\n",
        "  randomized_testing_segmentations.append(newy)"
      ],
      "metadata": {
        "id": "y4Zu_IyFhC92"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(randomized_testing_segmentations))\n",
        "print(randomized_testing_segmentations[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1SbByMohEmA",
        "outputId": "abbdf26f-1ca0-448a-d47f-f11b5aa1c64a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "torch.Size([1, 4, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_testing_data = list(zip(randomized_testing_segmentations, randomized_testing_segmentations))"
      ],
      "metadata": {
        "id": "O0KeJ9qyhSIB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testingloader = torch.utils.data.DataLoader(dataset=randomized_testing_data, batch_size=2, shuffle=True) # batch size should be 2"
      ],
      "metadata": {
        "id": "Y-6FP0OmhTWX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training\n",
        "The sum of cross-entropy and dice loss \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9Rja_lBDtooP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d1b861bf-cf93-4661-f2cb-529dc799e1cf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTraining\\nThe sum of cross-entropy and dice loss \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chose to make this a class because when you call dice loss in criterion, you don't have anything to input, but when u run the prediction through inside the training, then you have params\n",
        "# also because most sources I saw used a class\n",
        "class DiceLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self, true, pred):\n",
        "    # flatten to easily do it pixel by pixel\n",
        "    true = true.view(-1)\n",
        "    pred = pred.view(-1)\n",
        "    numerator = 2*(true*pred).sum()\n",
        "    denominator = true.sum() + pred.sum()\n",
        "    dice_loss = 1 - (numerator) / (denominator)\n",
        "    return dice_loss"
      ],
      "metadata": {
        "id": "b8VIZVcJhTYT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim"
      ],
      "metadata": {
        "id": "6JfmRbwXiE-l"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# btw im not doing patches\n",
        "# epochs\n",
        "epochs = 2 # should be 1000\n",
        "# loss\n",
        "criterion1 = DiceLoss()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01, momentum= 0.99)"
      ],
      "metadata": {
        "id": "kay03ku8iGA0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  training_loss = 0\n",
        "  validation_loss = 0\n",
        "  print(\"training time\")\n",
        "  for images, segs in trainloader:\n",
        "    optimizer.zero_grad()\n",
        "    print(len(images), len(segs))\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "    # segs = segs.long() - no\n",
        "    segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    outputs, softmax_outputs = model(images)\n",
        "    print(outputs.shape)\n",
        "    print(softmax_outputs.shape)\n",
        "\n",
        "    # arg_outputs = outputs.argmax(dim=1)\n",
        "    # print(arg_outputs.shape)\n",
        "    # print(arg_outputs)\n",
        "    # print(segs.shape)\n",
        "    print()\n",
        "    diceloss = criterion1(softmax_outputs.float(), segs)\n",
        "    celoss = criterion2(outputs.float(), segs)\n",
        "    loss = diceloss + celoss\n",
        "    print(loss) # loss with random tensors will be really high because none of the tensors are related to each other\n",
        "    # loss can be > 1 - https://ai.stackexchange.com/questions/24685/can-the-sparse-categorical-cross-entropy-be-greater-than-one, https://stats.stackexchange.com/questions/392681/cross-entropy-loss-max-value\n",
        "    loss.backward()\n",
        "    training_loss += loss.item()\n",
        "    print()\n",
        "  print(\"validation time\")\n",
        "  for images, segs in validationloader:\n",
        "    optimizer.zero_grad()\n",
        "    print(len(images), len(segs))\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    images = images.squeeze().clone().detach().requires_grad_(True)\n",
        "    # segs = segs.long() - no\n",
        "    segs = segs.squeeze().clone().detach().requires_grad_(True)\n",
        "    print(images.shape)\n",
        "    print(segs.shape)\n",
        "    outputs, softmax_outputs = model(images)\n",
        "    print(outputs.shape)\n",
        "    print(softmax_outputs.shape)\n",
        "    diceloss = criterion1(softmax_outputs.float(), segs)\n",
        "    celoss = criterion2(outputs.float(), segs)\n",
        "    loss = diceloss + celoss\n",
        "    print(loss) # loss with random tensors will be really high because none of the tensors are related to each other\n",
        "    # loss can be > 1 - https://ai.stackexchange.com/questions/24685/can-the-sparse-categorical-cross-entropy-be-greater-than-one, https://stats.stackexchange.com/questions/392681/cross-entropy-loss-max-value\n",
        "    loss.backward()\n",
        "    validation_loss += loss.item()\n",
        "  training_losses.append(training_loss/len(trainloader))\n",
        "  validation_losses.append(validation_loss/len(validationloader))\n",
        "  print(\"Epoch: {}/{}... Training Loss: {}... Validation Loss: {}...\".format(i+1,epochs, training_losses[-1], validation_losses[-1]))\n",
        "  if validation_loss < min(validation_losses):\n",
        "    print(\"Validation loss has decreased...saving model\")\n",
        "    torch.save(model.state_dict(), \"fcn.pth\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqJhvR3HpfQC",
        "outputId": "34e6985c-1f1b-485d-842c-ace4d9579d96"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "\n",
            "tensor(3.5496, grad_fn=<AddBackward0>)\n",
            "\n",
            "validation time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "tensor(3.5498, grad_fn=<AddBackward0>)\n",
            "Epoch: 1/2... Training Loss: 3.5496268272399902... Validation Loss: 3.5497632026672363...\n",
            "\n",
            "training time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "\n",
            "tensor(3.5496, grad_fn=<AddBackward0>)\n",
            "\n",
            "validation time\n",
            "2 2\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 1, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 48, 64, 64, 64])\n",
            "torch.Size([2, 96, 32, 32, 32])\n",
            "torch.Size([2, 192, 16, 16, 16])\n",
            "torch.Size([2, 320, 8, 8, 8])\n",
            "torch.Size([2, 320, 4, 4, 4])\n",
            "torch.Size([2, 192, 8, 8, 8])\n",
            "torch.Size([2, 96, 16, 16, 16])\n",
            "torch.Size([2, 48, 32, 32, 32])\n",
            "torch.Size([2, 24, 64, 64, 64])\n",
            "torch.Size([2, 24, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "torch.Size([2, 4, 128, 128, 128])\n",
            "tensor(3.5498, grad_fn=<AddBackward0>)\n",
            "Epoch: 2/2... Training Loss: 3.5496268272399902... Validation Loss: 3.5497632026672363...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_losses)\n",
        "print(validation_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de7ESYI8rt6b",
        "outputId": "454fa434-8317-417f-cc22-a9a438fa77aa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.5496268272399902, 3.5496268272399902]\n",
            "[3.5497632026672363, 3.5497632026672363]\n"
          ]
        }
      ]
    }
  ]
}